<?xml version="1.0" encoding="UTF-8" ?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"> <channel><title>langroid</title><description>Langroid LLM App Development Framework</description><link>https://langroid.github.io/langroid/</link><atom:link href="https://langroid.github.io/langroid/feed_rss_updated.xml" rel="self" type="application/rss+xml" /><docs>https://github.com/langroid/langroid</docs><language>en-None</language> <pubDate>Tue, 31 Oct 2023 08:04:13 -0000</pubDate> <lastBuildDate>Tue, 31 Oct 2023 08:04:13 -0000</lastBuildDate> <ttl>1440</ttl> <generator>MkDocs RSS plugin - v1.8.0</generator> <image> <url>https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Feed-icon.svg/128px-Feed-icon.svg.png</url> <title>langroid</title><link>https://langroid.github.io/langroid/</link> </image> <item> <title>Language Models: Completion and Chat-Completion</title> <author>pchalasani</author> <category>chat</category> <category>langroid</category> <category>llm</category> <category>local-llm</category> <description>&lt;p&gt;Transformer-based language models are fundamentally next-token predictors, so naturally all LLM APIs today at least provide a completion endpoint. If an LLM is a next-token predictor, how could it possibly be used to generate a response to a question or instruction, or to engage in a conversation with a human user? This is where the idea of &#34;chat-completion&#34; comes in.This post is a refresher on the distinction between completion and chat-completion,and some interesting details on how chat-completion is implemented in practice.&lt;/p&gt;</description><link>https://langroid.github.io/langroid/blog/2023/09/19/language-models-completion-and-chat-completion/</link> <pubDate>Mon, 30 Oct 2023 13:25:09 +0000</pubDate><source url="https://langroid.github.io/langroid/feed_rss_updated.xml">langroid</source><guid isPermaLink="true">https://langroid.github.io/langroid/blog/2023/09/19/language-models-completion-and-chat-completion/</guid> </item> <item> <title>Langroid: Harness LLMs with Multi-Agent Programming</title> <author>pchalasani</author> <category>langroid</category> <category>llm</category> <description>&lt;h1&gt;Langroid: Harness LLMs with Multi-Agent Programming&lt;/h1&gt;&lt;h2&gt;The LLM Opportunity&lt;/h2&gt;&lt;p&gt;Given the remarkable abilities of recent Large Language Models (LLMs), thereis an unprecedented opportunity to build intelligent applications powered bythis transformative technology. The top question for any enterprise is: howbest to harness the power of LLMs for complex applications? For technical andpractical reasons, building LLM-powered applications is not as simple asthrowing a task at an LLM-system and expecting it to do it.&lt;/p&gt;</description><link>https://langroid.github.io/langroid/blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/</link> <pubDate>Mon, 30 Oct 2023 13:25:09 +0000</pubDate><source url="https://langroid.github.io/langroid/feed_rss_updated.xml">langroid</source><guid isPermaLink="true">https://langroid.github.io/langroid/blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/</guid> </item> <item> <title>Using Langroid with Local LLMs</title> <author>pchalasani</author> <category>langroid</category> <category>llm</category> <category>local-llm</category> <description>&lt;h2&gt;Why local models?&lt;/h2&gt;&lt;p&gt;There are commercial, remotely served models that currently appear to beat all open/localmodels. So why care about local models? Local models are exciting for a number of reasons:&lt;/p&gt;</description><link>https://langroid.github.io/langroid/blog/2023/09/14/using-langroid-with-local-llms/</link> <pubDate>Mon, 30 Oct 2023 13:25:09 +0000</pubDate><source url="https://langroid.github.io/langroid/feed_rss_updated.xml">langroid</source><guid isPermaLink="true">https://langroid.github.io/langroid/blog/2023/09/14/using-langroid-with-local-llms/</guid> </item> </channel></rss>