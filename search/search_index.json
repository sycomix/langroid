{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Langroid: Harness LLMs with Multi-Agent Programming","text":""},{"location":"#the-llm-opportunity","title":"The LLM Opportunity","text":"<p>Given the remarkable abilities of recent Large Language Models (LLMs), there is an unprecedented opportunity to build intelligent applications powered by this transformative technology. The top question for any enterprise is: how best to harness the power of LLMs for complex applications? For technical and practical reasons, building LLM-powered applications is not as simple as throwing a task at an LLM-system and expecting it to do it.</p>"},{"location":"#langroids-multi-agent-programming-framework","title":"Langroid's Multi-Agent Programming Framework","text":"<p>Effectively leveraging LLMs at scale requires a principled programming  framework. In particular, there is often a need to maintain multiple LLM  conversations, each instructed in different ways, and \"responsible\" for  different aspects of a task.</p> <p>An agent is a convenient abstraction that encapsulates LLM conversation  state, along with access to long-term memory (vector-stores) and tools (a.k.a functions  or plugins). Thus a Multi-Agent Programming framework is a natural fit  for complex LLM-based applications.</p> <p>Langroid is the first Python LLM-application framework that was explicitly  designed  with Agents as first-class citizens, and Multi-Agent Programming  as the core  design principle. The framework is inspired by ideas from the  Actor Framework.</p> <p>Langroid allows an intuitive definition of agents, tasks and task-delegation  among agents. There is a principled mechanism to orchestrate multi-agent  collaboration. Agents act as message-transformers, and take turns responding to (and transforming) the current message. The architecture is lightweight, transparent,  flexible, and allows other types of orchestration to be implemented. Besides Agents, Langroid also provides simple ways to directly interact with LLMs and vector-stores.  </p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>Agents as first-class citizens: The <code>Agent</code> class encapsulates LLM conversation state,   and optionally a vector-store and tools. Agents are a core abstraction in Langroid;    Agents act as message transformers, and by default provide 3 responder methods, one corresponding to each    entity: LLM, Agent, User. </li> <li>Tasks: A Task class wraps an Agent, gives the agent instructions (or roles, or goals),   manages iteration over an Agent's responder methods,   and orchestrates multi-agent interactions via hierarchical, recursive   task-delegation. The <code>Task.run()</code> method has the same   type-signature as an Agent's responder's methods, and this is key to how   a task of an agent can delegate to other sub-tasks: from the point of view of a Task,   sub-tasks are simply additional responders, to be used in a round-robin fashion   after the agent's own responders.</li> <li>Modularity, Reusabilily, Loose coupling: The <code>Agent</code> and <code>Task</code> abstractions allow users to design   Agents with specific skills, wrap them in Tasks, and combine tasks in a flexible way.</li> <li>LLM Support: Langroid supports OpenAI LLMs including GPT-3.5-Turbo,   GPT-4.</li> <li>Caching of LLM prompts, responses: Langroid by default uses Redis for caching.    Caching with Momento is also supported.</li> <li>Vector-stores: Qdrant and Chroma are currently supported.   Vector stores allow for Retrieval-Augmented-Generaation (RAG).</li> <li>Grounding and source-citation: Access to external documents via vector-stores   allows for grounding and source-citation.</li> <li>Observability, Logging, Lineage: Langroid generates detailed logs of multi-agent interactions and   maintains provenance/lineage of messages, so that you can trace back   the origin of a message.</li> <li>Tools/Plugins/Function-calling: Langroid supports OpenAI's recently   released function calling   feature. In addition, Langroid has its own native equivalent, which we   call tools (also known as \"plugins\" in other contexts). Function   calling and tools have the same developer-facing interface, implemented   using Pydantic,   which makes it very easy to define tools/functions and enable agents   to use them. Benefits of using Pydantic are that you never have to write   complex JSON specs for function calling, and when the LLM   hallucinates malformed JSON, the Pydantic error message is sent back to   the LLM so it can fix it!</li> </ul> <p>Don't worry if some of these terms are not clear to you.  The Getting Started Guide and subsequent pages  will help you get up to speed.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/","title":"Language Models: Completion and Chat-Completion","text":"<p>Transformer-based language models are fundamentally next-token predictors, so  naturally all LLM APIs today at least provide a completion endpoint.  If an LLM is a next-token predictor, how could it possibly be used to  generate a response to a question or instruction, or to engage in a conversation with  a human user? This is where the idea of \"chat-completion\" comes in. This post is a refresher on the distinction between completion and chat-completion, and some interesting details on how chat-completion is implemented in practice.</p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#language-models-as-next-token-predictors","title":"Language Models as Next-token Predictors","text":"<p>A Language Model is essentially a \"next-token prediction\" model, and so all LLMs today provide a \"completion\" endpoint, typically something like: <code>/completions</code> under the base URL.</p> <p>The endpoint simply takes a prompt and returns a completion (i.e. a continuation).</p> <p>A typical prompt sent to a completion endpoint might look like this: <pre><code>The capital of Belgium is \n</code></pre> and the LLM will return a completion like this: <pre><code>Brussels.\n</code></pre> OpenAI's GPT3 is an example of a pure completion LLM. But interacting with a completion LLM is not very natural or useful: you cannot give instructions or ask questions; instead you would always need to  formulate your input as a prompt whose natural continuation is your desired output. For example, if you wanted the LLM to highlight all proper nouns in a sentence, you would format it as the following prompt:</p> <p>Chat-To-Prompt Example: Chat/Instruction converted to a completion prompt.</p> <p><pre><code>User: here is a sentence, the Assistant's task is to identify all proper nouns.\n     Jack lives in Bosnia, and Jill lives in Belgium.\nAssistant:    \n</code></pre> The natural continuation of this prompt would be a response listing the proper nouns, something like: <pre><code>John, Bosnia, Jill, Belgium are all proper nouns.\n</code></pre></p> <p>This seems sensible in theory, but a \"base\" LLM that performs well on completions may not perform well on these kinds of prompts. The reason is that during its training, it may not have been exposed to very many examples of this type of prompt-response pair. So how can an LLM be improved to perform well on these kinds of prompts?</p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#instruction-tuned-aligned-llms","title":"Instruction-tuned, Aligned LLMs","text":"<p>This brings us to the heart of the innovation behind the wildly popular ChatGPT: it uses an enhancement of GPT3 that (besides having a lot more parameters), was explicitly fine-tuned on instructions (and dialogs more generally) -- this is referred to as instruction-fine-tuning or IFT for short. In addition to fine-tuning instructions/dialogs, the models behind ChatGPT (i.e., GPT-3.5-Turbo and GPT-4) are further tuned to produce responses that align with human preferences (i.e. produce responses that are more helpful and safe), using a procedure called Reinforcement Learning with Human Feedback (RLHF). See this OpenAI InstructGPT Paper for details on these techniques and references to the  original papers that introduced these ideas. Another recommended read is Sebastian  Raschka's post on RLHF and related techniques. </p> <p>For convenience, we refer to the combination of IFT and RLHF as chat-tuning. A chat-tuned LLM can be expected to perform well on prompts such as the one in  the Chat-To-Prompt Example above. These types of prompts are still unnatural, however,  so as a convenience, chat-tuned LLM API servers also provide a \"chat-completion\"  endpoint (typically <code>/chat/completions</code> under the base URL), which allows the user to interact with them in a natural dialog, which might look like this (the portions in square brackets are indicators of who is generating the text):</p> <p><pre><code>[User] What is the capital of Belgium?\n[Assistant] The capital of Belgium is Brussels.\n</code></pre> or <pre><code>[User] In the text below, find all proper nouns:\n    Jack lives in Bosnia, and Jill lives in Belgium.\n[Assistant] John, Bosnia, Jill, Belgium are all proper nouns.\n[User] Where does John live?\n[Assistant] John lives in Bosnia.\n</code></pre></p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#chat-completion-endpoints-under-the-hood","title":"Chat Completion Endpoints: under the hood","text":"<p>How could this work, given that LLMs are fundamentally next-token predictors? This is a convenience provided by the LLM API service (e.g. from OpenAI or local model server libraries): when a user invokes the chat-completion endpoint (typically at <code>/chat/completions</code> under the base URL), under the hood, the server converts the instructions and multi-turn chat history into a single string, with annotations indicating user and assistant turns, and ending with something like \"Assistant:\" as in the Chat-To-Prompt Example above.</p> <p>Now the subtle detail to note here is this:</p> <p>It matters how the dialog (instructions plus chat history) is converted into a single prompt string. Converting to a single prompt by simply concatenating the instructions and chat history using an \"intuitive\" format (e.g. indicating user, assistant turns using \"User\", \"Assistant:\", etc.) can work, however most local LLMs are trained on a specific prompt format. So if we format chats in a different way, we may get odd/inferior results.</p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#converting-chats-to-prompts-formatting-rules","title":"Converting Chats to Prompts: Formatting Rules","text":"<p>For example, the llama2 models are trained on a format where the user's input is bracketed within special strings <code>[INST]</code> and <code>[/INST]</code>. There are other requirements that we don't go into here, but interested readers can refer to these links:</p> <ul> <li>A reddit thread on the llama2 formats</li> <li>Facebook's llama2 code</li> <li>Langroid's llama2 formatting code</li> </ul> <p>A dialog fed to a Llama2 model in its expected prompt format would look like this:</p> <pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful assistant.\n&lt;&lt;/SYS&gt;&gt;\n\nHi there! \n[/INST] \nHello! How can I help you today? &lt;/s&gt;\n&lt;s&gt;[INST] In the text below, find all proper nouns:\n    Jack lives in Bosnia, and Jill lives in Belgium.\n [/INST] \nJohn, Bosnia, Jill, Belgium are all proper nouns. &lt;/s&gt;&lt;s&gt; \n[INST] Where does Jack live? [/INST] \nJack lives in Bosnia. &lt;/s&gt;&lt;s&gt;\n[INST] And Jill? [/INST]\nJill lives in Belgium. &lt;/s&gt;&lt;s&gt;\n[INST] Which are its neighboring countries? [/INST]\n</code></pre> <p>This means that if an LLM server library wants to provide a chat-completion endpoint for a local model, it needs to provide a way to convert chat history to a single prompt using the specific formatting rules of the model. For example the <code>oobabooga/text-generation-webui</code>  library has an extensive set of chat formatting templates for a variety of models, and their model server auto-detects the format template from the model name.</p> <p>Chat completion model names: look for 'chat' or 'instruct' in the name</p> <p>You can search for a variety of models on the HuggingFace model hub. For example if you see a name <code>Llama-2-70B-chat-GGUF</code> you know it is chat-tuned. Another example of a chat-tuned model is <code>Llama-2-7B-32K-Instruct</code> </p> <p>A user of these local LLM server libraries thus has two options when using a  local model in chat mode:</p> <ul> <li>use the chat-completion endpoint, and let the underlying library handle the chat-to-prompt formatting, or</li> <li>first format the chat history according to the model's requirements, and then use the   completion endpoint</li> </ul>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#using-local-models-in-langroid","title":"Using Local Models in Langroid","text":"<p>Local models can be used in Langroid by defining a <code>LocalModelConfig</code> object. More details are in this tutorial,  but here we briefly discuss prompt-formatting in this context. Langroid provides a built-in formatter for LLama2 models,  so users looking to use llama2 models with langroid can try either of these options, by setting the <code>use_completion_for_chat</code> flag in the <code>LocalModelConfig</code> object (See the local-LLM tutorial for details).</p> <p>When this flag is set to <code>True</code>, the chat history is formatted using the built-in  Langroid llama2 formatter and the completion endpoint are used. When the flag is set to <code>False</code>, the chat  history is sent directly to the chat-completion endpoint, which internally converts the  chat history to a prompt in the expected llama2 format.</p> <p>For local models other than Llama2, users can either:</p> <ul> <li>write their own formatters by writing a class similar to <code>Llama2Formatter</code> and  then setting the <code>use_completion_for_chat</code> flag to <code>True</code> in the <code>LocalModelConfig</code> object, or</li> <li>use an LLM server library (such as the <code>oobabooga</code> library mentioned above) that provides a chat-completion endpoint,  and converts chats to single prompts under the hood, and set the   <code>use_completion_for_chat</code> flag to <code>False</code> in the <code>LocalModelConfig</code> object.</li> </ul> <p>You can use a similar approach if you are using an LLM application framework other than Langroid.</p>"},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/","title":"Langroid: Harness LLMs with Multi-Agent Programming","text":""},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/#the-llm-opportunity","title":"The LLM Opportunity","text":"<p>Given the remarkable abilities of recent Large Language Models (LLMs), there is an unprecedented opportunity to build intelligent applications powered by this transformative technology. The top question for any enterprise is: how best to harness the power of LLMs for complex applications? For technical and practical reasons, building LLM-powered applications is not as simple as throwing a task at an LLM-system and expecting it to do it.</p>"},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/#langroids-multi-agent-programming-framework","title":"Langroid's Multi-Agent Programming Framework","text":"<p>Effectively leveraging LLMs at scale requires a principled programming framework. In particular, there is often a need to maintain multiple LLM conversations, each instructed in different ways, and \"responsible\" for different aspects of a task.</p> <p>An agent is a convenient abstraction that encapsulates LLM conversation state, along with access to long-term memory (vector-stores) and tools (a.k.a functions or plugins). Thus a Multi-Agent Programming framework is a natural fit for complex LLM-based applications.</p> <p>Langroid is the first Python LLM-application framework that was explicitly designed  with Agents as first-class citizens, and Multi-Agent Programming as the core  design principle. The framework is inspired by ideas from the Actor Framework.</p> <p>Langroid allows an intuitive definition of agents, tasks and task-delegation among agents. There is a principled mechanism to orchestrate multi-agent collaboration. Agents act as message-transformers, and take turns responding to (and transforming) the current message. The architecture is lightweight, transparent, flexible, and allows other types of orchestration to be implemented. Besides Agents, Langroid also provides simple ways to directly interact with LLMs and vector-stores.</p>"},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/#highlights","title":"Highlights","text":"<ul> <li>Agents as first-class citizens: The <code>Agent</code> class encapsulates LLM conversation state,   and optionally a vector-store and tools. Agents are a core abstraction in Langroid;   Agents act as message transformers, and by default provide 3 responder methods, one corresponding to each   entity: LLM, Agent, User.</li> <li>Tasks: A Task class wraps an Agent, gives the agent instructions (or roles, or goals),   manages iteration over an Agent's responder methods,   and orchestrates multi-agent interactions via hierarchical, recursive   task-delegation. The <code>Task.run()</code> method has the same   type-signature as an Agent's responder's methods, and this is key to how   a task of an agent can delegate to other sub-tasks: from the point of view of a Task,   sub-tasks are simply additional responders, to be used in a round-robin fashion   after the agent's own responders.</li> <li>Modularity, Reusability, Loose coupling: The <code>Agent</code> and <code>Task</code> abstractions allow users to design   Agents with specific skills, wrap them in Tasks, and combine tasks in a flexible way.</li> <li>LLM Support: Langroid supports OpenAI LLMs including GPT-3.5-Turbo,   GPT-4.</li> <li>Caching of LLM prompts, responses: Langroid by default uses Redis for caching.   Caching with Momento is also supported.</li> <li>Vector-stores: Qdrant and Chroma are currently supported.   Vector stores allow for Retrieval-Augmented-Generaation (RAG).</li> <li>Grounding and source-citation: Access to external documents via vector-stores   allows for grounding and source-citation.</li> <li>Observability, Logging, Lineage: Langroid generates detailed logs of multi-agent interactions and   maintains provenance/lineage of messages, so that you can trace back   the origin of a message.</li> <li>Tools/Plugins/Function-calling: Langroid supports OpenAI's recently   released function calling   feature. In addition, Langroid has its own native equivalent, which we   call tools (also known as \"plugins\" in other contexts). Function   calling and tools have the same developer-facing interface, implemented   using Pydantic,   which makes it very easy to define tools/functions and enable agents   to use them. Benefits of using Pydantic are that you never have to write   complex JSON specs for function calling, and when the LLM   hallucinates malformed JSON, the Pydantic error message is sent back to   the LLM so it can fix it!</li> </ul>"},{"location":"blog/2023/09/14/using-langroid-with-local-llms/","title":"Using Langroid with Local LLMs","text":""},{"location":"blog/2023/09/14/using-langroid-with-local-llms/#why-local-models","title":"Why local models?","text":"<p>There are commercial, remotely served models that currently appear to beat all open/local models. So why care about local models? Local models are exciting for a number of reasons:</p> <ul> <li>cost: other than compute/electricity, there is no cost to use them.</li> <li>privacy: no concerns about sending your data to a remote server.</li> <li>latency: no network latency due to remote API calls, so faster response times, provided you can get fast enough inference.</li> <li>uncensored: some local models are not censored to avoid sensitive topics.</li> <li>fine-tunable: you can fine-tune them on private/recent data, which current commercial models don't have access to.</li> <li>sheer thrill: having a model running on your machine with no internet connection,   and being able to have an intelligent conversation with it -- there is something almost magical about it.</li> </ul> <p>The main appeal with local models is that with sufficiently careful prompting, they may behave sufficiently well to be useful for specific tasks/domains, and bring all of the above benefits. Some ideas on how you might use local LLMs:</p> <ul> <li>In a multi-agent system, you could have some agents use local models for narrow    tasks with a lower bar for accuracy (and fix responses with multiple tries).</li> <li>You could run many instances of the same or different models and combine their responses.</li> <li>Local LLMs can act as a privacy layer, to identify and handle sensitive data before passing to remote LLMs.</li> <li>Some local LLMs have intriguing features, for example llama.cpp lets you    constrain its output using a grammar.</li> </ul>"},{"location":"blog/2023/09/14/using-langroid-with-local-llms/#running-llms-locally","title":"Running LLMs locally","text":"<p>There are several ways to use LLMs locally. See the <code>r/LocalLLaMA</code> subreddit for a wealth of information. There are open source libraries that offer front-ends to run local models, for example <code>oobabooga/text-generation-webui</code> (or \"ooba-TGW\" for short) but the focus in this tutorial is on spinning up a server that mimics an OpenAI-like API, so that any code that works with the OpenAI API (for say GPT3.5 or GPT4) will work with a local model, with just a simple change: set <code>openai.api_base</code> to the URL where the local API server is listening, typically <code>http://localhost:8000/v1</code>.</p> <p>There are a few libraries we recommend for setting up local models with OpenAI-like APIs:</p> <ul> <li>LiteLLM OpenAI Proxy Server lets you set up a local    proxy server for over 100+ LLM providers (remote and local).</li> <li>ooba-TGW mentioned above, for a variety of models, including llama2 models.</li> <li>llama-cpp-python (LCP for short), specifically for llama2 models.</li> <li>ollama</li> </ul> <p>We recommend visiting these links to see how to install and run these libraries.</p>"},{"location":"blog/2023/09/14/using-langroid-with-local-llms/#use-the-local-model-with-the-openai-library","title":"Use the local model with the OpenAI library","text":"<p>Once you have a server running using any of the above methods,  your code that works with the OpenAI models can be made to work  with the local model, by simply changing the <code>openai.api_base</code> to the  URL where the local server is listening. </p> <p>If you are using Langroid to build LLM applications, the framework takes care of the <code>api_base</code> setting in most cases, and you need to only set the <code>chat_model</code> parameter in the LLM config object for the LLM model you are using. See the Non-OpenAI LLM tutorial for more details.</p>"},{"location":"demos/targeting/audience-targeting/","title":"Audience Targeting for a Business","text":"<p>Suppose you are a marketer for a business, trying to figure out which  audience segments to target. Your downstream systems require that you specify standardized audience segments to target, for example from the IAB Audience Taxonomy.</p> <p>There are thousands of standard audience segments, and normally you would need  to search the list for potential segments that match what you think your ideal customer profile is. This is a tedious, error-prone task.</p> <p>But what if we can leverage an LLM such as GPT-4? We know that GPT-4 has  skills that are ideally suited for this task:</p> <ul> <li>General knowledge about businesses and their ideal customers</li> <li>Ability to recognize which standard segments match an English description of a customer profile</li> <li>Ability to plan a conversation to get the information it needs to answer a question</li> </ul> <p>Once you decide to use an LLM, you still need to figure out how to organize the  various components of this task:</p> <ul> <li>Research: What are some ideal customer profiles for the business</li> <li>Segmentation: Which standard segments match an English description of a customer profile</li> <li>Planning: how to organize the task to identify a few standard segments</li> </ul>"},{"location":"demos/targeting/audience-targeting/#using-langroid-agents","title":"Using Langroid Agents","text":"<p>Langroid makes it intuitive and simple to build an LLM-powered system organized around agents, each responsible for a different task. In less than a day we built a 3-agent system to automate this task:</p> <ul> <li>The <code>Marketer</code> Agent is given the Planning role.</li> <li>The <code>Researcher</code> Agent is given the Research role,    and it has access to the business description. </li> <li>The <code>Segmentor</code> Agent is given the Segmentation role. It has access to the    IAB Audience Taxonomy via a vector database, i.e. its rows have been mapped to   vectors via an embedding model, and these vectors are stored in a vector-database.    Thus given an English description of a customer profile,   the <code>Segmentor</code> Agent maps it to a vector using the embedding model,   and retrieves the nearest (in vector terms, e.g. cosine similarity)    IAB Standard Segments from the vector-database. The Segmentor's LLM    further refines this by selecting the best-matching segments from the retrieved list.</li> </ul> <p>To kick off the system, the human user describes a business in English, or provides the URL of the business's website.  The <code>Marketer</code> Agent sends customer profile queries to the <code>Researcher</code>, who answers in plain English based on  the business description, and the Marketer takes this description and sends it to the Segmentor, who maps it to Standard IAB Segments. The task is done when the Marketer finds 4 Standard segments.  The agents are depicted in the diagram below:</p> <p></p>"},{"location":"demos/targeting/audience-targeting/#an-example-glashutte-watches","title":"An example: Glashutte Watches","text":"<p>The human user first provides the URL of the business, in this case: <pre><code>https://www.jomashop.com/glashutte-watches.html\n</code></pre> From this URL, the <code>Researcher</code> agent summarizes its understanding of the business. The <code>Marketer</code> agent starts by asking the <code>Researcher</code>: <pre><code>Could you please describe the age groups and interests of our typical customer?\n</code></pre> The <code>Researcher</code> responds with an English description of the customer profile: <pre><code>Our typical customer is a fashion-conscious individual between 20 and 45 years...\n</code></pre> The <code>Researcher</code> forwards this English description to the <code>Segmentor</code> agent, who maps it to a standardized segment, e.g.: <pre><code>Interest|Style &amp; Fashion|Fashion Trends\n...\n</code></pre> This conversation continues until the <code>Marketer</code> agent has identified 4 standardized segments.</p> <p>Here is what the conversation looks like:</p> <p></p>"},{"location":"examples/agent-tree/","title":"Hierarchical computation with Langroid Agents","text":"<p>Here is a simple example showing tree-structured computation where each node in the tree is handled by a separate agent. This is a toy numerical example, and illustrates:</p> <ul> <li>how to have agents organized in a hierarchical structure to accomplish a task </li> <li>the use of global state accessible to all agents, and </li> <li>the use of tools/function-calling.</li> </ul>"},{"location":"examples/agent-tree/#the-computation","title":"The Computation","text":"<p>We want to carry out the following calculation for a given input number \\(n\\):</p> <pre><code>def Main(n):\n    if n is odd:\n        return (3*n+1) + n\n    else:\n        if n is divisible by 10:\n            return n/10 + n\n        else:\n            return n/2 + n\n</code></pre>"},{"location":"examples/agent-tree/#using-function-composition","title":"Using function composition","text":"<p>Imagine we want to do this calculation using a few auxiliary functions:</p> <pre><code>def Main(n):\n    # return non-null value computed by Odd or Even\n    Record n as global variable # to be used by Adder below\n    return Odd(n) or Even(n)\n\ndef Odd(n):\n    # Handle odd n\n    if n is odd:\n        new = 3*n+1\n        return Adder(new)\n    else:\n        return None\n\ndef Even(n):\n    # Handle even n: return non-null value computed by EvenZ or EvenNZ\n    return EvenZ(n) or EvenNZ(n)\n\ndef EvenZ(n):\n    # Handle even n divisible by 10, i.e. ending in Zero\n    if n is divisible by 10:\n        new = n/10\n        return Adder(new)\n    else:\n        return None\n\ndef EvenNZ(n):\n    # Handle even n not divisible by 10, i.e. not ending in Zero\n    if n is not divisible by 10:\n        new = n/2\n        return Adder(new)\n    else:\n        return None  \n\ndef Adder(new):\n    # Add new to starting number, available as global variable n\n    return new + n\n</code></pre>"},{"location":"examples/agent-tree/#mapping-to-a-tree-structure","title":"Mapping to a tree structure","text":"<p>This compositional/nested computation can be represented as a tree:</p> <pre><code>       Main\n     /     \\\n  Even     Odd\n  /   \\        \\\nEvenZ  EvenNZ   Adder\n  |      |\n Adder  Adder\n</code></pre> <p>Let us specify the behavior we would like for each node, in a  \"decoupled\" way, i.e. we don't want a node to be aware of the other nodes. As we see later, this decoupled design maps very well onto Langroid's multi-agent task orchestration. To completely define the node behavior, we need to specify how it handles an \"incoming\" number \\(n\\) (from a parent node  or user), and how it handles a \"result\" number \\(r\\) (from a child node).</p> <ul> <li><code>Main</code>: <ul> <li>incoming \\(n\\): simply send down \\(n\\), record the starting number \\(n_0 = n\\) as a global variable. </li> <li>result \\(r\\): return \\(r\\).</li> </ul> </li> <li><code>Odd</code>: <ul> <li>incoming \\(n\\): if n is odd, send down \\(3*n+1\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>Even</code>: <ul> <li>incoming \\(n\\): if n is even, send down \\(n\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>EvenZ</code>: (guaranteed by the tree hierarchy, to receive an even number.)  <ul> <li>incoming \\(n\\): if n is divisible by 10, send down \\(n/10\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>EvenNZ</code>: (guaranteed by the tree hierarchy, to receive an even number.)<ul> <li>incoming \\(n\\): if n is not divisible by 10, send down \\(n/2\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>Adder</code>:<ul> <li>incoming \\(n\\): return \\(n + n_0\\) where \\(n_0\\) is the  starting number recorded by Main as a global variable.</li> <li>result \\(r\\): Not applicable since <code>Adder</code> is a leaf node.</li> </ul> </li> </ul>"},{"location":"examples/agent-tree/#from-tree-nodes-to-langroid-agents","title":"From tree nodes to Langroid Agents","text":"<p>Let us see how we can perform this calculation using multiple Langroid agents, where</p> <ul> <li>we define an agent corresponding to each of the nodes above, namely  <code>Main</code>, <code>Odd</code>, <code>Even</code>, <code>EvenZ</code>, <code>EvenNZ</code>, and <code>Adder</code>.</li> <li>we wrap each Agent into a Task, and use the <code>Task.add_subtask()</code> method to connect the agents into    the desired hierarchical structure.</li> </ul> <p>Below is one way to do this using Langroid. We designed this with the following desirable features:</p> <ul> <li> <p>Decoupling: Each agent is instructed separately, without mention of any other agents   (E.g. Even agent does not know about Odd Agent, EvenZ agent, etc).   In particular, this means agents will not be \"addressing\" their message   to specific other agents, e.g. send number to Odd agent when number is odd,   etc. Allowing addressing would make the solution easier to implement,   but would not be a decoupled solution.   Instead, we want Agents to simply put the number \"out there\", and have it handled   by an applicable agent, in the task loop (which consists of the agent's responders,   plus any sub-task <code>run</code> methods).</p> </li> <li> <p>Simplicity: Keep the agent instructions relatively simple. We would not want a solution   where we have to instruct the agents (their LLMs) in convoluted ways. </p> </li> </ul> <p>One way naive solutions fail is because agents are not able to distinguish between a number that is being \"sent down\" the tree as input, and a number that is being \"sent up\" the tree as a result from a child node.</p> <p>We use a simple trick: we instruct the LLM to mark returned values using the RESULT keyword, and instruct the LLMs on how to handle numbers that come with RESULT keyword, and those that don't In addition, we leverage some features of Langroid's task orchestration:</p> <ul> <li>When <code>llm_delegate</code> is <code>True</code>, if the LLM says <code>DONE [rest of msg]</code>, the task is   considered done, and the result of the task is <code>[rest of msg]</code> (i.e the part after <code>DONE</code>).</li> <li>In the task loop's <code>step()</code> function (which seeks a valid message during a turn of   the conversation) when any responder says <code>DO-NOT-KNOW</code>, it is not considered a valid   message, and the search continues to other responders, in round-robin fashion.</li> </ul> <p>See the <code>chat-tree.py</code> example for an implementation of this solution. You can run that example as follows: <pre><code>python3 examples/basic/chat-tree.py\n</code></pre> In the sections below we explain the code in more detail.</p>"},{"location":"examples/agent-tree/#define-the-agents","title":"Define the agents","text":"<p>Let us start with defining the configuration to be used by all agents:</p> <pre><code>from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\nfrom langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\n\nconfig = ChatAgentConfig(\n  llm=OpenAIGPTConfig(\n    chat_model=OpenAIChatModel.GPT4,\n  ),\n  vecdb=None, # no need for a vector database\n)\n</code></pre> <p>Next we define each of the agents, for example:</p> <pre><code>main_agent = ChatAgent(config)\n</code></pre> <p>and similarly for the other agents.</p>"},{"location":"examples/agent-tree/#wrap-each-agent-in-a-task","title":"Wrap each Agent in a Task","text":"<p>To allow agent interactions, the first step is to wrap each agent in a Task. When we define the task, we pass in the instructions above as part of the system message. Recall the instructions for the <code>Main</code> agent:</p> <ul> <li><code>Main</code>:<ul> <li>incoming \\(n\\): simply send down \\(n\\), record the starting number \\(n_0 = n\\) as a global variable.</li> <li>result \\(r\\): return \\(r\\).</li> </ul> </li> </ul> <p>We include the equivalent of these instructions in the <code>main_task</code> that wraps  the <code>main_agent</code>:</p> <pre><code>from langroid.agent.task import Task\n\nmain_task = Task(\n    main_agent,\n    name=\"Main\",\n    default_human_response=\"\", #(1)!\n    only_user_quits_root=False, #(2)!\n    system_message=\"\"\"\n          You will receive two types of messages, to which you will respond as follows:\n\n          INPUT Message format: &lt;number&gt;\n          In this case simply write the &lt;number&gt;, say nothing else.\n\n          RESULT Message format: RESULT &lt;number&gt;\n          In this case simply say \"DONE &lt;number&gt;\", e.g.:\n          DONE 19\n\n          To start off, ask the user for the initial number, \n          using the `ask_num` tool/function.\n          \"\"\",\n    llm_delegate=True, # allow LLM to control end of task via DONE\n    single_round=False,\n)\n</code></pre> <ol> <li>Don't wait for user input in each turn; </li> <li>Allow LLM to quit the task when it is done.</li> </ol> <p>There are a couple of points to highlight about the <code>system_message</code>  value in this task definition:</p> <ul> <li>When the <code>Main</code> agent receives just a number, it simply writes out that number,   and in the Langroid Task loop, this number becomes the \"current pending message\"   to be handled by one of the sub-tasks, i.e. <code>Even, Odd</code>. Note that these sub-tasks   are not mentioned in the system message, consistent with the decoupling principle.</li> <li>As soon as either of these sub-tasks returns a non-Null response, in the format \"RESULT \", the <code>Main</code> agent   is instructed to return this result saying \"DONE \". Since <code>llm_delegate</code>   is set to <code>True</code> (meaning the LLM can decide when the task has ended),    this causes the <code>Main</code> task to be considered finished and the task loop is exited. <p>Since we want the <code>Main</code> agent to record the initial number as a global variable, we use a tool/function <code>AskNum</code> defined as follows  (see this section in the getting started guide  for more details on Tools):</p> <pre><code>from rich.prompt import Prompt\nfrom langroid.agent.tool_message import ToolMessage\n\n\nclass AskNumTool(ToolMessage):\n  request = \"ask_num\"\n  purpose = \"Ask user for the initial number\"\n\n  def handle(self) -&gt; str:\n    \"\"\"\n    This is a stateless tool (i.e. does not use any Agent member vars), so we can\n    define the handler right here, instead of defining an `ask_num`\n    method in the agent.\n    \"\"\"\n    num = Prompt.ask(\"Enter a number\")\n    # record this in global state, so other agents can access it\n    MyGlobalState.set_values(number=num)\n    return str(num)\n</code></pre> <p>We then enable the <code>main_agent</code> to use and handle messages that conform to the  <code>AskNum</code> tool spec:</p> <pre><code>main_agent.enable_message(AskNumTool)\n</code></pre> <p>Using and Handling a tool/function</p> <p>\"Using\" a tool means the agent's LLM generates  the function-call (if using OpenAI function-calling) or  the JSON structure (if using Langroid's native tools mechanism)  corresponding to this tool. \"Handling\" a tool refers to the Agent's method  recognizing the tool and executing the corresponding code.</p> <p>The tasks for other agents are defined similarly. We will only note here that the <code>Adder</code> agent needs a special tool <code>AddNumTool</code> to be able to add the current number to the initial number set by the <code>Main</code> agent. </p>"},{"location":"examples/agent-tree/#connect-the-tasks-into-a-tree-structure","title":"Connect the tasks into a tree structure","text":"<p>So far, we have wrapped each agent in a task, in isolation, and there is no  connection between the tasks. The final step is to connect the tasks to  the tree structure we saw earlier:</p> <pre><code>main_task.add_sub_task([even_task, odd_task])\neven_task.add_sub_task([evenz_task, even_nz_task])\nevenz_task.add_sub_task(adder_task)\neven_nz_task.add_sub_task(adder_task)\nodd_task.add_sub_task(adder_task)\n</code></pre> <p>Now all that remains is to run the main task:</p> <pre><code>main_task.run()\n</code></pre> <p>Here is what a run starting with \\(n=12\\) looks like:</p> <p></p>"},{"location":"examples/guide/","title":"Guide to examples in <code>langroid-examples</code> repo","text":"<p>The <code>langroid-examples</code> repo contains several examples of using the Langroid agent-oriented programming  framework for LLM applications. Below is a guide to the examples. First please ensure you follow the installation instructions in the <code>langroid-examples</code> repo README.</p> <p>At minimum a GPT4-compatible OpenAI API key is required. As currently set up, many of the examples will not work with a weaker model. Weaker models may require more detailed or different prompting, and possibly a more iterative approach with multiple agents to verify and retry, etc \u2014 this is on our roadmap.</p> <p>All the example scripts are meant to be run on the command line. In each script there is a description and sometimes instructions on how to run the script.</p> <p>NOTE: When you run any script, it pauses for \u201chuman\u201d input at every step, and depending on the context, you can either hit enter to continue, or in case there is a question/response expected from the human, you can enter your question or response and then hit enter.</p>"},{"location":"examples/guide/#basic-examples","title":"Basic Examples","text":"<ul> <li> <p><code>/examples/basic/chat.py</code> This is a basic chat application.</p> <ul> <li>Illustrates Agent task loop.</li> </ul> </li> <li> <p><code>/examples/basic/autocorrect.py</code> Chat with autocorrect: type fast and carelessly/lazily and  the LLM will try its best to interpret what you want, and offer choices when confused.</p> <ul> <li>Illustrates Agent task loop.</li> </ul> </li> <li> <p><code>/examples/basic/chat-search.py</code>  This uses a <code>GoogleSearchTool</code> function-call/tool to answer questions using a google web search if needed.   Try asking questions about facts known after Sep 2021 (GPT4 training cutoff),   like  <code>when was llama2 released</code></p> <ul> <li>Illustrates Agent + Tools/function-calling + web-search</li> </ul> </li> <li> <p><code>/examples/basic/chat-tree.py</code> is a toy example of tree-structured multi-agent   computation, see a detailed writeup here.</p> <ul> <li>Illustrates multi-agent task collaboration, task delegation.</li> </ul> </li> </ul>"},{"location":"examples/guide/#document-chat-examples-or-rag-retrieval-augmented-generation","title":"Document-chat examples, or RAG (Retrieval Augmented Generation)","text":"<ul> <li><code>/examples/docqa/chat.py</code> is a document-chat application. Point it to local file,   directory or web url, and ask questions<ul> <li>Illustrates basic RAG</li> </ul> </li> <li><code>/examples/docqa/chat-search.py</code>: ask about anything and it will try to answer   based on docs indexed in vector-db, otherwise it will do a Google search, and   index the results in the vec-db for this and later answers.<ul> <li>Illustrates RAG + Function-calling/tools</li> </ul> </li> <li><code>/examples/docqa/chat_multi.py</code>:  \u2014 this is a 2-agent system that will summarize   a large document with 5 bullet points: the first agent generates questions for   the retrieval agent, and is done when it gathers 5 key points.<ul> <li>Illustrates 2-agent collaboration + RAG to summarize a document</li> </ul> </li> <li><code>/examples/docqa/chat_multi_extract.py</code>:  \u2014 extracts structured info from a   lease document: Main agent asks questions to a retrieval agent. <ul> <li>Illustrates 2-agent collaboration, RAG, Function-calling/tools, Structured Information Extraction.</li> </ul> </li> </ul>"},{"location":"examples/guide/#data-chat-examples-tabular-sql","title":"Data-chat examples (tabular, SQL)","text":"<ul> <li><code>/examples/data-qa/table_chat.py</code>:  - point to a URL or local csv file and ask   questions. The agent generates pandas code that is run within langroid.<ul> <li>Illustrates function-calling/tools and code-generation</li> </ul> </li> <li><code>/examples/data-qa/sql-chat/sql_chat.py</code>:  \u2014 chat with a sql db \u2014 ask questions in   English, it will generate sql code to answer them.   See tutorial here<ul> <li>Illustrates function-calling/tools and code-generation</li> </ul> </li> </ul>"},{"location":"quick-start/","title":"Getting Started","text":"<p>In these sections we show you how to use the various components of <code>langroid</code>. To follow along, we recommend you clone the <code>langroid-examples</code> repo.</p> <p>Consult the tests as well</p> <p>As you get deeper into Langroid, you will find it useful to consult the tests folder under <code>tests/main</code> in the main Langroid repo.</p> <p>Start with the <code>Setup</code> section to install Langroid and get your environment set up.</p>"},{"location":"quick-start/chat-agent-docs/","title":"Augmenting Agents with Retrieval","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>chat-agent-docs.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/chat-agent-docs.py</code>.</p>"},{"location":"quick-start/chat-agent-docs/#why-is-this-important","title":"Why is this important?","text":"<p>Until now in this guide, agents have not used external data. Although LLMs already have enormous amounts of knowledge \"hard-wired\" into their weights during training (and this is after all why ChatGPT has exploded in popularity), for practical enterprise applications there are a few reasons it is critical to augment LLMs with access to specific, external documents:</p> <ul> <li>Private data: LLMs are trained on public data, but in many applications   we want to use private data that is not available to the public.   For example, a company may want to extract useful information from its private   knowledge-base.</li> <li>New data: LLMs are trained on data that was available at the time of training,   and so they may not be able to answer questions about new topics</li> <li>Constrained responses, or Grounding: LLMs are trained to generate text that is   consistent with the distribution of text in the training data.   However, in many applications we want to constrain the LLM's responses   to be consistent with the content of a specific document.   For example, if we want to use an LLM to generate a response to a customer   support ticket, we want the response to be consistent with the content of the ticket.   In other words, we want to reduce the chances that the LLM hallucinates   a response that is not consistent with the ticket.</li> </ul> <p>In all these scenarios, we want to augment the LLM with access to a specific set of documents, and use retrieval augmented generation (RAG) to generate more relevant, useful, accurate responses. Langroid provides a simple, flexible mechanism  RAG using vector-stores, thus ensuring grounded responses constrained to  specific documents. Another key feature of Langroid is that retrieval lineage  is maintained, and responses based on documents are always accompanied by source citations.</p>"},{"location":"quick-start/chat-agent-docs/#docchatagent-for-retrieval-augmented-generation","title":"<code>DocChatAgent</code> for Retrieval-Augmented Generation","text":"<p>Langroid provides a special type of agent called  <code>DocChatAgent</code>, which is a <code>ChatAgent</code> augmented with a vector-store, and some special methods that enable the agent to ingest documents into the vector-store, and answer queries based on these documents.</p> <p>The <code>DocChatAgent</code> provides many ways to ingest documents into the vector-store, including from URLs and local file-paths and URLs. Given a collection of document paths, ingesting their content into the vector-store involves the following steps:</p> <ol> <li>Split the document into shards (in a configurable way)</li> <li>Map each shard to an embedding vector using an embedding model. The default   embedding model is OpenAI's <code>text-embedding-ada-002</code> model, but users can    instead use <code>all-MiniLM-L6-v2</code> from HuggingFace <code>sentence-transformers</code> library.<sup>1</sup></li> <li>Store embedding vectors in the vector-store, along with the shard's content and    any document-level meta-data (this ensures Langroid knows which document a shard   came from when it retrieves it augment an LLM query)</li> </ol> <p><code>DocChatAgent</code>'s <code>llm_response</code> overrides the default <code>ChatAgent</code> method,  by augmenting the input message with relevant shards from the vector-store, along with instructions to the LLM to respond based on the shards.</p>"},{"location":"quick-start/chat-agent-docs/#define-some-documents","title":"Define some documents","text":"<p>Let us see how <code>DocChatAgent</code> helps with retrieval-agumented generation (RAG). For clarity, rather than ingest documents from paths or URLs, let us just set up some simple documents in the code itself,  using Langroid's <code>Document</code> class:</p> <pre><code>documents =[\n    lr.mytypes.Document(\n        content=\"\"\"\n            In the year 2050, GPT10 was released. \n\n            In 2057, paperclips were seen all over the world. \n\n            Global warming was solved in 2060. \n\n            In 2061, the world was taken over by paperclips.         \n\n            In 2045, the Tour de France was still going on.\n            They were still using bicycles. \n\n            There was one more ice age in 2040.\n            \"\"\",\n        metadata=lr.mytypes.DocMetaData(source=\"wikipedia-2063\"),\n    ),\n    lr.mytypes.Document(\n        content=\"\"\"\n            We are living in an alternate universe \n            where Germany has occupied the USA, and the capital of USA is Berlin.\n\n            Charlie Chaplin was a great comedian.\n            In 2050, all Asian merged into Indonesia.\n            \"\"\",\n        metadata=lr.mytypes.DocMetaData(source=\"Almanac\"),\n    ),\n]\n</code></pre> <p>There are two text documents. We will split them by double-newlines (<code>\\n\\n</code>), as we see below.</p>"},{"location":"quick-start/chat-agent-docs/#configure-the-docchatagent-and-ingest-documents","title":"Configure the DocChatAgent and ingest documents","text":"<p>Following the pattern in Langroid, we first set up a <code>DocChatAgentConfig</code> object and then instantiate a <code>DocChatAgent</code> from it.</p> <pre><code>config = lr.agent.special.DocChatAgentConfig(\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4,\n    ),\n    vecdb=lr.vector_store.QdrantDBConfig(\n        collection_name=\"quick-start-chat-agent-docs\",\n        replace_collection=True, #(1)!\n    ),\n    parsing=lr.parsing.parser.ParsingConfig(\n        separators=[\"\\n\\n\"],\n        splitter=lr.parsing.parser.Splitter.SIMPLE, #(2)!\n        n_similar_docs=2, #(3)!\n    )\n)\nagent = lr.agent.special.DocChatAgent(config)\n</code></pre> <ol> <li>Specifies that each time we run the code, we create a fresh collection,  rather than re-use the existing one with the same name.</li> <li>Specifies to split all text content by the first separator in the <code>separators</code> list</li> <li>Specifies that, for a query,    we want to retrieve at most 2 similar shards from the vector-store</li> </ol> <p>Now that the <code>DocChatAgent</code> is configured, we can ingest the documents  into the vector-store:</p> <pre><code>agent.ingest_docs(documents)\n</code></pre>"},{"location":"quick-start/chat-agent-docs/#setup-the-task-and-run-it","title":"Setup the task and run it","text":"<p>As before, all that remains is to set up the task and run it:</p> <pre><code>task = lr.Task(agent)\ntask.run()\n</code></pre> <p>And that is all there is to it! Feel free to try out the  <code>chat-agent-docs.py</code> script in the <code>langroid-examples</code> repository.</p> <p>Here is a screenshot of the output:</p> <p></p> <p>Notice how follow-up questions correctly take the preceding dialog into account, and every answer is accompanied by a source citation.</p>"},{"location":"quick-start/chat-agent-docs/#answer-questions-from-a-set-of-urls","title":"Answer questions from a set of URLs","text":"<p>Instead of having in-code documents as above, what if you had a set of URLs instead -- how do you use Langroid to answer questions based on the content  of those URLS?</p> <p><code>DocChatAgent</code> makes it very simple to do this.  First include the URLs in the <code>DocChatAgentConfig</code> object:</p> <pre><code>config = lr.agent.special.DocChatAgentConfig(\n  doc_paths = [\n    \"https://cthiriet.com/articles/scaling-laws\",\n    \"https://www.jasonwei.net/blog/emergence\",\n  ]\n)\n</code></pre> <p>Then, call the <code>ingest()</code> method of the <code>DocChatAgent</code> object:</p> <p><pre><code>agent.ingest()\n</code></pre> And the rest of the code remains the same.</p>"},{"location":"quick-start/chat-agent-docs/#see-also","title":"See also","text":"<p>In the <code>langroid-examples</code> repository, you can find full working examples of document question-answering:</p> <ul> <li><code>examples/docqa/chat.py</code>   an app that takes a list of URLs or document paths from a user, and answers questions on them.</li> <li><code>examples/docqa/chat_multi.py</code>   a two-agent app where the <code>WriterAgent</code> is tasked with writing 5 key points about a topic,    and takes the help of a <code>DocAgent</code> that answers its questions based on a given set of documents.</li> </ul>"},{"location":"quick-start/chat-agent-docs/#next-steps","title":"Next steps","text":"<p>This Getting Started guide walked you through the core features of Langroid. If you want to see full working examples combining these elements,  have a look at the  <code>examples</code> folder in the <code>langroid-examples</code> repo. </p> <ol> <li> <p>To use this embedding model, install langroid via <code>pip install langroid[hf-embeddings]</code> Note that this will install <code>torch</code> and <code>sentence-transfoemers</code> libraries.\u00a0\u21a9</p> </li> </ol>"},{"location":"quick-start/chat-agent-tool/","title":"A chat agent, equipped with a tool/function-call","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is   in the <code>chat-agent-tool.py</code> script in the <code>langroid-examples</code> repo:   <code>examples/quick-start/chat-agent-tool.py</code>.</p>"},{"location":"quick-start/chat-agent-tool/#tools-plugins-function-calling","title":"Tools, plugins, function-calling","text":"<p>An LLM normally generates unstructured text in response to a prompt (or sequence of prompts). However there are many situations where we would like the LLM to generate structured text, or even code, that can be handled by specialized functions outside the LLM, for further processing.  In these situations, we want the LLM to \"express\" its \"intent\" unambiguously, and we achieve this by instructing the LLM on how to format its output (typically in JSON) and under what conditions it should generate such output. This mechanism has become known by various names over the last few months (tools, plugins, or function-calling), and is extremely useful in numerous scenarios, such as:</p> <ul> <li>Extracting structured information from a document: for example, we can use  the tool/functions mechanism to have the LLM present the key terms in a lease document in a JSON structured format, to simplify further processing.  See an example of this in the <code>langroid-examples</code> repo. </li> <li>Specialized computation: the LLM can request a units conversion,  or request scanning a large file (which wouldn't fit into its context) for a specific pattern.</li> <li>Code execution: the LLM can generate code that is executed in a sandboxed environment, and the results of the execution are returned to the LLM.</li> </ul> <p>For LLM developers, Langroid provides a clean, uniform interface for the recently released OpenAI Function-calling as well Langroid's own native \"tools\" mechanism. You can choose which to enable by setting the  <code>use_tools</code> and <code>use_functions_api</code> flags in the <code>ChatAgentConfig</code> object. The implementation leverages the excellent  Pydantic library. Benefits of using Pydantic are that you never have to write complex JSON specs  for function calling, and when the LLM hallucinates malformed JSON,  the Pydantic error message is sent back to the LLM so it can fix it!</p>"},{"location":"quick-start/chat-agent-tool/#example-find-the-smallest-number-in-a-list","title":"Example: find the smallest number in a list","text":"<p>Again we will use a simple number-game as a toy example to quickly and succinctly illustrate the ideas without spending too much on token costs.  This is a modification of the <code>chat-agent.py</code> example we saw in an earlier section. The idea of this single-agent game is that the agent has in \"mind\" a list of numbers between 1 and 100, and the LLM has to find out the smallest number from this list. The LLM has access to a <code>probe</code> tool  (think of it as a function) that takes an argument <code>number</code>. When the LLM  \"uses\" this tool (i.e. outputs a message in the format required by the tool), the agent handles this structured message and responds with  the number of values in its list that are at most equal to the <code>number</code> argument. </p>"},{"location":"quick-start/chat-agent-tool/#define-the-tool-as-a-toolmessage","title":"Define the tool as a <code>ToolMessage</code>","text":"<p>The first step is to define the tool, which we call <code>ProbeTool</code>, as an instance of the <code>ToolMessage</code> class, which is itself derived from Pydantic's <code>BaseModel</code>. Essentially the <code>ProbeTool</code> definition specifies </p> <ul> <li>the name of the Agent method that handles the tool, in this case <code>probe</code></li> <li>the fields that must be included in the tool message, in this case <code>number</code></li> <li>the \"purpose\" of the tool, i.e. under what conditions it should be used, and what it does</li> </ul> <p>Here is what the <code>ProbeTool</code> definition looks like: <pre><code>class ProbeTool(lr.agent.ToolMessage):\n    request: str = \"probe\" #(1)!\n    purpose: str = \"\"\" \n        To find which number in my list is closest to the &lt;number&gt; you specify\n        \"\"\" #(2)!\n    number: int #(3)!\n</code></pre></p> <ol> <li>this indicates that the agent's <code>probe</code> method will handle this tool-message</li> <li>The <code>purpose</code> is used behind the scenes to instruct the LLM</li> <li><code>number</code> is a required argument of the tool-message (function)</li> </ol>"},{"location":"quick-start/chat-agent-tool/#define-the-chatagent-with-the-probe-method","title":"Define the ChatAgent, with the <code>probe</code> method","text":"<p>As before we first create a <code>ChatAgentConfig</code> object:</p> <pre><code>config = lr.ChatAgentConfig(\n    name=\"Spy\",\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4,\n    ),\n    use_tools=True, #(1)!\n    use_functions_api=False, #(2)!\n    vecdb=None,\n)\n</code></pre> <ol> <li>whether to use langroid's native tools mechanism</li> <li>whether to use OpenAI's function-calling mechanism</li> </ol> <p>Next we define the Agent class itself, which we call <code>SpyGameAgent</code>, with a member variable to hold its \"secret\" list of numbers. We also add <code>probe</code> method (to handle the <code>ProbeTool</code> message) to this class, and instantiate it:</p> <pre><code>class SpyGameAgent(lr.ChatAgent):\n    def __init__(self, config: lr.ChatAgentConfig):\n        super().__init__(config)\n        self.numbers = [3, 4, 8, 11, 15, 25, 40, 80, 90]\n\n    def probe(self, msg: ProbeTool) -&gt; str:\n        # return how many values in self.numbers are less or equal to msg.number\n        return str(len([n for n in self.numbers if n &lt;= msg.number]))\n\nspy_game_agent = SpyGameAgent(config)\n</code></pre>"},{"location":"quick-start/chat-agent-tool/#enable-the-spy_game_agent-to-handle-the-probe-tool","title":"Enable the <code>spy_game_agent</code> to handle the <code>probe</code> tool","text":"<p>The final step in setting up the tool is to enable  the <code>spy_game_agent</code> to handle the <code>probe</code> tool:</p> <pre><code>spy_game_agent.enable_message(ProbeTool)\n</code></pre>"},{"location":"quick-start/chat-agent-tool/#set-up-the-task-and-instructions","title":"Set up the task and instructions","text":"<p>We set up the task for the <code>spy_game_agent</code> and run it:</p> <p><pre><code>task = lr.Task(\n   spy_game_agent,\n   system_message=\"\"\"\n            I have a list of numbers between 1 and 100. \n            Your job is to find the smallest of them.\n            To help with this, you can give me a number and I will\n            tell you how many of my numbers are equal or less than your number.\n            Once you have found the smallest number,\n            you can say DONE and report your answer.\n        \"\"\"\n)\ntask.run()\n</code></pre> Notice that in the task setup we  have not explicitly instructed the LLM to use the <code>probe</code> tool. But this is done \"behind the scenes\", either by the OpenAI API  (when we use function-calling by setting the <code>use_functions_api</code> flag to <code>True</code>), or by Langroid's native tools mechanism (when we set the <code>use_tools</code> flag to <code>True</code>).</p> <p>See the <code>chat-agent-tool.py</code> in the <code>langroid-examples</code> repo, for a working example that you can run as follows: <pre><code>python3 examples/quick-start/chat-agent-tool.py\n</code></pre></p> <p>Here is a screenshot of the chat in action, using Langroid's tools mechanism</p> <p></p> <p>And if we run it with the <code>-f</code> flag (to switch to using OpenAI function-calling):</p> <p></p>"},{"location":"quick-start/chat-agent-tool/#see-also","title":"See also","text":"<p>One of the uses of tools/function-calling is to extract structured information from  a document. In the <code>langroid-examples</code> repo, there are two examples of this: </p> <ul> <li><code>examples/extract/chat.py</code>,    which shows how to extract Machine Learning model quality information from a description of    a solution approach on Kaggle.</li> <li><code>examples/docqa/chat_multi_extract.py</code>   which extracts key terms from a commercial lease document, in a nested JSON format.</li> </ul>"},{"location":"quick-start/chat-agent-tool/#next-steps","title":"Next steps","text":"<p>In the 3-agent chat example, recall that the <code>processor_agent</code> did not have to bother with specifying who should handle the current number. In the next section we add a twist to this game, so that the <code>processor_agent</code> has to decide who should handle the current number.</p>"},{"location":"quick-start/chat-agent/","title":"A simple chat agent","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>chat-agent.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/chat-agent.py</code>.</p>"},{"location":"quick-start/chat-agent/#agents","title":"Agents","text":"<p>A <code>ChatAgent</code> is an abstraction that  wraps a few components, including:</p> <ul> <li>an LLM (<code>ChatAgent.llm</code>), possibly equipped with tools/function-calling.    The <code>ChatAgent</code> class maintains LLM conversation history.</li> <li>optionally a vector-database (<code>ChatAgent.vecdb</code>)</li> </ul>"},{"location":"quick-start/chat-agent/#agents-as-message-transformers","title":"Agents as message transformers","text":"<p>In Langroid, a core function of <code>ChatAgents</code> is message transformation. There are three special message transformation methods, which we call responders. Each of these takes a message and returns a message.  More specifically, their function signature is (simplified somewhat): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> where <code>ChatDocument</code> is a class that wraps a message content (text) and its metadata. There are three responder methods in <code>ChatAgent</code>, one corresponding to each  responding entity (<code>LLM</code>, <code>USER</code>, or <code>AGENT</code>):</p> <ul> <li><code>llm_response</code>: returns the LLM response to the input message.   (The input message is added to the LLM history, and so is the subsequent response.)</li> <li><code>agent_response</code>: a method that can be used to implement a custom agent response.     Typically, an <code>agent_response</code> is used to handle messages containing a     \"tool\" or \"function-calling\" (more on this later). Another use of <code>agent_response</code>     is message validation.</li> <li><code>user_response</code>: get input from the user. Useful to allow a human user to     intervene or quit.</li> </ul> <p>Creating an agent is easy. First define a <code>ChatAgentConfig</code> object, and then instantiate a <code>ChatAgent</code> object with that config: <pre><code>import langroid as lr\n\nconfig = lr.ChatAgentConfig(\n    llm = lr.language_models.OpenAIGPTConfig(\n    chat_model=lr.language_models.OpenAIChatModel.GPT4\n    ) #(1)!\n)\nagent = lr.ChatAgent(config)\n</code></pre></p> <ol> <li>This agent only has an LLM, and no vector-store. Examples of agents with    vector-stores will be shown later.</li> </ol> <p>We can now use the agent's responder methods, for example: <pre><code>response = agent.llm_response(\"What is 2 + 4?\")\nif response is not None:\n    print(response.content)\nresponse = agent.user_response(\"add 3 to this\")\n...\n</code></pre> The <code>ChatAgent</code> conveniently accumulates message history so you don't have to, as you did in the previous section with direct LLM usage. However to create an interative loop involving the human user, you still  need to write your own. The <code>Task</code> abstraction frees you from this, as we see below.</p>"},{"location":"quick-start/chat-agent/#task-orchestrator-for-agents","title":"Task: orchestrator for agents","text":"<p>In order to do anything useful with a <code>ChatAgent</code>, we need to have a way to  sequentially invoke its responder methods, in a principled way. For example in the simple chat loop we saw in the  previous section, in the  <code>try-llm.py</code> script, we had a loop that alternated between getting a human input and an LLM response. This is one of the simplest possible loops, but in more complex applications,  we need a general way to orchestrate the agent's responder methods.</p> <p>The <code>Task</code> class is an abstraction around a  <code>ChatAgent</code>, responsible for iterating over the agent's responder methods, as well as orchestrating delegation and hand-offs among multiple tasks. A <code>Task</code> is initialized with a specific <code>ChatAgent</code> instance, and some  optional arguments, including an initial message to \"kick-off\" the agent. The <code>Task.run()</code> method is the main entry point for <code>Task</code> objects, and works  as follows:</p> <ul> <li>it first calls the <code>Task.init()</code> method to initialize the <code>pending_message</code>,    which represents the latest message that needs a response.</li> <li>it then repeatedly calls <code>Task.step()</code> until <code>Task.done()</code> is True, and returns   <code>Task.result()</code> as the final result of the task.</li> </ul> <p><code>Task.step()</code> is where all the action happens. It represents a \"turn\" in the  \"conversation\": in the case of a single <code>ChatAgent</code>, the conversation involves  only the three responders mentioned above, but when a <code>Task</code> has sub-tasks,  it can involve other tasks well  (we see this in the a later section but ignore this for now).  <code>Task.step()</code> loops over  the <code>ChatAgent</code>'s responders (plus sub-tasks if any) until it finds a valid  response<sup>1</sup> to the current <code>pending_message</code>, i.e. a \"meaningful\" response,  something other than <code>None</code> for example. Once <code>Task.step()</code> finds a valid response, it updates the <code>pending_message</code>  with this response, and the next invocation of <code>Task.step()</code> will search for a valid response to this  updated message, and so on. <code>Task.step()</code> incorporates mechanisms to ensure proper handling of messages, e.g. the USER gets a chance to respond after each non-USER response (to avoid infinite runs without human intervention), and preventing an entity from responding if it has just responded, etc.</p> <p><code>Task.run()</code> has the same signature as agent's responder methods.</p> <p>The key to composability of tasks is that <code>Task.run()</code> has exactly the same type-signature as any of the agent's responder methods,  i.e. <code>str | ChatDocument -&gt; ChatDocument</code>. This means that a <code>Task</code> can be used as a responder in another <code>Task</code>, and so on recursively.  We will see this in action in the Two Agent Chat section.</p> <p>The above details were only provided to give you a glimpse into how Agents and  Tasks work. Unless you are creating a custom orchestration mechanism, you do not need to be aware of these details. In fact our basic human + LLM chat loop can be trivially  implemented with a <code>Task</code>, in a couple of lines of code: <pre><code>task = lr.Task(agent, name=\"Bot\", system_message=\"You are a helpful assistant\")\n</code></pre> We can then run the task: <pre><code>task.run() #(1)!\n</code></pre></p> <ol> <li>Note how this hides all of the complexity of constructing and updating a     sequence of <code>LLMMessages</code></li> </ol> <p>Note that the agent's <code>agent_response()</code> method always returns <code>None</code> (since the default  implementation of this method looks for a tool/function-call, and these never occur in this task). So the calls to <code>task.step()</code> result in alternating responses from the LLM and the user.</p> <p>See <code>chat-agent.py</code> for a working example that you can run with <pre><code>python3 examples/quick-start/chat-agent.py\n</code></pre></p> <p>Here is a screenshot of the chat in action:<sup>2</sup></p> <p></p>"},{"location":"quick-start/chat-agent/#next-steps","title":"Next steps","text":"<p>In the next section you will  learn some general principles on how to have multiple agents collaborate  on a task using Langroid.</p> <ol> <li> <p>To customize a Task's behavior you can subclass it and  override methods like <code>valid()</code>, <code>done()</code>, <code>result()</code>, or even <code>step()</code>.\u00a0\u21a9</p> </li> <li> <p>In the screenshot, the numbers in parentheses indicate how many  messages have accumulated in the LLM's message history.  This is only provided for informational and debugging purposes, and  you can ignore it for now.\u00a0\u21a9</p> </li> </ol>"},{"location":"quick-start/llm-interaction/","title":"LLM interaction","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is  in the <code>try-llm.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/try-llm.py</code>.</p> <p>Let's start with the basics -- how to directly interact with an OpenAI LLM using Langroid.</p>"},{"location":"quick-start/llm-interaction/#configure-instantiate-the-llm-class","title":"Configure, instantiate the LLM class","text":"<p>First define the configuration for the LLM, in this case one of the OpenAI GPT chat models: <pre><code>import langroid as lr\n\ncfg = lr.language_models.OpenAIGPTConfig(\n    chat_model=lr.language_models.OpenAIChatModel.GPT4,\n)\n</code></pre></p> <p>About Configs</p> <p>A recurring pattern you will see in Langroid is that for many classes, we have a corresponding <code>Config</code> class (an instance of a Pydantic <code>BaseModel</code>), and the class constructor takes this <code>Config</code> class as its only argument. This lets us avoid having long argument lists in constructors, and brings flexibility since adding a new argument to the constructor is as simple as adding a new field to the corresponding <code>Config</code> class. For example the constructor for the <code>OpenAIGPT</code> class takes a single argument, an instance of the <code>OpenAIGPTConfig</code> class.</p> <p>Now that we've defined the configuration of the LLM, we can instantiate it: <pre><code>mdl = lr.language_models.OpenAIGPT(cfg)\n</code></pre></p> <p>We will use OpenAI's GPT4 model's chat completion API.</p>"},{"location":"quick-start/llm-interaction/#messages-the-llmmessage-class","title":"Messages: The <code>LLMMessage</code> class","text":"<p>This API takes a list of \"messages\" as input -- this is typically the conversation history so far, consisting of an initial system message, followed by a sequence of alternating messages from the LLM (\"Assistant\") and the user. Langroid provides an abstraction  <code>LLMMessage</code> to construct messages, e.g. <pre><code>from langroid.language_models import Role, LLMMessage\n\nmsg = LLMMessage(\n    content=\"what is the capital of Bangladesh?\", \n    role=Role.USER\n)\n</code></pre></p>"},{"location":"quick-start/llm-interaction/#llm-response-to-a-sequence-of-messages","title":"LLM response to a sequence of messages","text":"<p>To get a response from the LLM, we call the mdl's <code>chat</code> method, and pass in a list of messages, along with a bound on how long (in tokens) we want the response to be: <pre><code>messages = [\n    LLMMessage(content=\"You are a helpful assistant\", role=Role.SYSTEM), #(1)!\n    LLMMessage(content=\"What is the capital of Ontario?\", role=Role.USER), #(2)!\n]\n\nresponse = mdl.chat(messages, max_tokens=200)\n</code></pre></p> <ol> <li> With a system message, you can assign a \"role\" to the LLM</li> <li> Responses from the LLM will have role <code>Role.ASSISTANT</code>;    this is done behind the scenes by the <code>response.to_LLMMessage()</code> call below.</li> </ol> <p>The response is an object of class <code>LLMResponse</code>,  which we can convert to an <code>LLMMessage</code> to append to the conversation history: <pre><code>messages.append(response.to_LLMMessage())\n</code></pre></p> <p>You can put the above in a simple loop,  to get a simple command-line chat interface!</p> <pre><code>from rich import print\nfrom rich.prompt import Prompt #(1)!\n\nmessages = [\n    LLMMessage(role=Role.SYSTEM, content=\"You are a helpful assitant\"),\n]\n\nwhile True:\n    message = Prompt.ask(\"[blue]Human\")\n    if message in [\"x\", \"q\"]:\n        print(\"[magenta]Bye!\")\n        break\n    messages.append(LLMMessage(role=Role.USER, content=message))\n\n    response = mdl.chat(messages=messages, max_tokens=200)\n    messages.append(response.to_LLMMessage())\n    print(\"[green]Bot: \" + response.message)\n</code></pre> <ol> <li>Rich is a Python library for rich text and beautiful formatting in the terminal.    We use it here to get a nice prompt for the user's input.    You can install it with <code>pip install rich</code>.</li> </ol> <p>See <code>examples/quick-start/try-llm.py</code> for a complete example that you can run using <pre><code>python3 examples/quick-start/try-llm.py\n</code></pre></p> <p>Here is a screenshot of what it looks like:</p> <p></p>"},{"location":"quick-start/llm-interaction/#next-steps","title":"Next steps","text":"<p>You might be thinking:  \"It is tedious to keep track of the LLM conversation history and set up a  loop. Does Langroid provide any abstractions to make this easier?\"</p> <p>We're glad you asked! And this leads to the notion of an <code>Agent</code>.  The next section will show you how to use the <code>ChatAgent</code> class  to set up a simple chat Agent in a couple of lines of code.</p>"},{"location":"quick-start/multi-agent-task-delegation/","title":"Multi-Agent collaboration via Task Delegation","text":""},{"location":"quick-start/multi-agent-task-delegation/#why-multiple-agents","title":"Why multiple agents?","text":"<p>Let's say we want to develop a complex LLM-based application, for example an application that reads a legal contract, extracts structured information, cross-checks it against some taxonomoy, gets some human input, and produces clear summaries. In theory it may be possible to solve this in a monolithic architecture using an LLM API and a vector-store. But this approach quickly runs into problems -- you would need to maintain multiple LLM conversation histories and states, multiple vector-store instances, and coordinate all of the interactions between them.</p> <p>Langroid's <code>ChatAgent</code> and <code>Task</code> abstractions provide a natural and intuitive way to decompose a solution approach into multiple tasks, each requiring different skills and capabilities. Some of these tasks may need access to an LLM, others may need access to a vector-store, and yet others may need tools/plugins/function-calling capabilities, or any combination of these. It may also make sense to have some tasks that manage the overall solution process. From an architectural perspective, this type of modularity has numerous benefits:</p> <ul> <li>Reusability: We can reuse the same agent/task in other contexts,</li> <li>Scalability: We can scale up the solution by adding more agents/tasks,</li> <li>Flexibility: We can easily change the solution by adding/removing agents/tasks.</li> <li>Maintainability: We can maintain the solution by updating individual agents/tasks.</li> <li>Testability: We can test/debug individual agents/tasks in isolation.</li> <li>Composability: We can compose agents/tasks to create new agents/tasks.</li> <li>Extensibility: We can extend the solution by adding new agents/tasks.</li> <li>Interoperability: We can integrate the solution with other systems by   adding new agents/tasks.</li> <li>Security/Privacy: We can secure the solution by isolating sensitive agents/tasks.</li> <li>Performance: We can improve performance by isolating performance-critical agents/tasks.</li> </ul>"},{"location":"quick-start/multi-agent-task-delegation/#task-collaboration-via-sub-tasks","title":"Task collaboration via sub-tasks","text":"<p>Langroid currently provides a mechanism for hierarchical (i.e. tree-structured) task delegation: a <code>Task</code> object can add other <code>Task</code> objects as sub-tasks, as shown in this pattern:</p> <pre><code>from langroid import ChatAgent, ChatAgentConfig, Task\n\nmain_agent = ChatAgent(ChatAgentConfig(...))\nmain_task = Task(main_agent, ...)\n\nhelper_agent1 = ChatAgent(ChatAgentConfig(...))\nhelper_agent2 = ChatAgent(ChatAgentConfig(...))\nhelper_task1 = Task(agent1, ...)\nhelper_task2 = Task(agent2, ...)\n\nmain_task.add_sub_task([helper_task1, helper_task2])\n</code></pre> <p>What happens when we call <code>main_task.run()</code>? Recall from the previous section that <code>Task.run()</code> works by repeatedly calling <code>Task.step()</code> until <code>Task.done()</code> is True. When the <code>Task</code> object has no sub-tasks, <code>Task.step()</code> simply tries to get a valid response from the <code>Task</code>'s <code>ChatAgent</code>'s \"native\" responders, in this sequence: <pre><code>[self.agent_response, self.llm_response, self.user_response] #(1)!\n</code></pre></p> <ol> <li>This is the default sequence in Langroid, but it can be changed by    overriding <code>ChatAgent.entity_responders()</code></li> </ol> <p>When a <code>Task</code> object has subtasks, the sequence of responders tried by <code>Task.step()</code> consists of the above \"native\" responders, plus the sequence of <code>Task.run()</code> calls on the sub-tasks, in the order in which they were added to the <code>Task</code> object. For the example above, this means that <code>main_task.step()</code> will seek a valid response in this sequence:</p> <p><pre><code>[self.agent_response, self.llm_response, self.user_response, \n    helper_task1.run(), helper_task2.run()]\n</code></pre> Fortunately, as noted in the previous section, <code>Task.run()</code> has the same type signature as that of the <code>ChatAgent</code>'s \"native\" responders, so this works seamlessly. Of course, each of the sub-tasks can have its own sub-tasks, and so on, recursively. One way to think of this type of task delegation is that <code>main_task()</code> \"fails-over\" to <code>helper_task1()</code> and <code>helper_task2()</code> when it cannot respond to the current <code>pending_message</code> on its own.</p>"},{"location":"quick-start/multi-agent-task-delegation/#or-else-logic-vs-and-then-logic","title":"Or Else logic vs And Then logic","text":"<p>It is important to keep in mind how <code>step()</code> works: As each responder  in the sequence is tried, when there is a valid response, the  next call to <code>step()</code> restarts its search at the beginning of the sequence (with the only exception being that the human User is given a chance  to respond after each non-human response).  In this sense, the semantics of the responder sequence is similar to OR Else logic, as opposed to AND Then logic.</p> <p>If we want to have a sequence of sub-tasks that is more like AND Then logic, we can achieve this by recursively adding subtasks. In the above example suppose we wanted the <code>main_task</code>  to trigger <code>helper_task1</code> and <code>helper_task2</code> in sequence, then we could set it up like this:</p> <pre><code>helper_task1.add_sub_task(helper_task2) #(1)!\nmain_task.add_sub_task(helper_task1)\n</code></pre> <ol> <li>When adding a single sub-task, we do not need to wrap it in a list.</li> </ol>"},{"location":"quick-start/multi-agent-task-delegation/#next-steps","title":"Next steps","text":"<p>In the next section we will see how this mechanism  can be used to set up a simple collaboration between two agents.</p>"},{"location":"quick-start/setup/","title":"Setup","text":""},{"location":"quick-start/setup/#install","title":"Install","text":"<p>Ensure you are using Python 3.11. It is best to work in a virtual environment:</p> <p><pre><code># go to your repo root (which may be langroid-examples)\ncd &lt;your repo root&gt;\npython3 -m venv .venv\n. ./.venv/bin/activate\n</code></pre> To see how to use Langroid in your own repo, you can take a look at the <code>langroid-examples</code> repo, which can be a good starting point for your own repo. The <code>langroid-examples</code> repo already contains a <code>pyproject.toml</code> file so that you can  use <code>Poetry</code> to manage your virtual environment and dependencies.  For example you can do  <pre><code>poetry install # installs latest version of langroid\n</code></pre> Alternatively, use <code>pip</code> to install <code>langroid</code> into your virtual environment: <pre><code>pip install langroid\n</code></pre></p> Optional Installs for using SQL Chat with a PostgreSQL DB <p>If you are using <code>SQLChatAgent</code> (e.g. the script <code>examples/data-qa/sql-chat/sql_chat.py</code>, with a postgres db, you will need to:</p> <ul> <li>Install PostgreSQL dev libraries for your platform, e.g.<ul> <li><code>sudo apt-get install libpq-dev</code> on Ubuntu,</li> <li><code>brew install postgresql</code> on Mac, etc.</li> </ul> </li> <li>Install langroid with the postgres extra, e.g. <code>pip install langroid[postgres]</code>   or <code>poetry add langroid[postgres]</code> or <code>poetry install -E postgres</code>.   If this gives you an error, try <code>pip install psycopg2-binary</code> in your virtualenv.</li> </ul> <p>Work in a nice terminal, such as Iterm2, rather than a notebook</p> <p>All of the examples we will go through are command-line applications. For the best experience we recommend you work in a nice terminal that supports  colored outputs, such as Iterm2.    </p> <p>OpenAI GPT4 is required</p> <p>The various LLM prompts and instructions in Langroid  have been tested to work well with GPT4. Switching to GPT3.5-Turbo is easy via a config flag, and may suffice  for some applications, but in general you may see inferior results.</p>"},{"location":"quick-start/setup/#set-up-tokenskeys","title":"Set up tokens/keys","text":"<p>To get started, all you need is an OpenAI API Key. If you don't have one, see this OpenAI Page. Currently only OpenAI models are supported. Others will be added later (Pull Requests welcome!).</p> <p>In the root of the repo, copy the <code>.env-template</code> file to a new file <code>.env</code>: <pre><code>cp .env-template .env\n</code></pre> Then insert your OpenAI API Key. Your <code>.env</code> file should look like this: <pre><code>OPENAI_API_KEY=your-key-here-without-quotes\n</code></pre></p> <p>Alternatively, you can set this as an environment variable in your shell (you will need to do this every time you open a new shell): <pre><code>export OPENAI_API_KEY=your-key-here-without-quotes\n</code></pre></p> <p>All of the following environment variable settings are optional, and some are only needed to use specific features (as noted below).</p> <ul> <li>Qdrant Vector Store API Key, URL. This is only required if you want to use Qdrant cloud.   You can sign up for a free 1GB account at Qdrant cloud.   If you skip setting up these, Langroid will use Qdrant in local-storage mode.   Alternatively Chroma is also currently supported.   We use the local-storage version of Chroma, so there is no need for an API key.   Langroid uses Qdrant by default.</li> <li>Redis Password, host, port: This is optional, and only needed to cache LLM API responses   using Redis Cloud. Redis offers a free 30MB Redis account   which is more than sufficient to try out Langroid and even beyond.   If you don't set up these, Langroid will use a pure-python   Redis in-memory cache via the Fakeredis library.</li> <li>Momento Serverless Caching of LLM API responses (as an alternative to Redis).   To use Momento instead of Redis:<ul> <li>enter your Momento Token in the <code>.env</code> file, as the value of <code>MOMENTO_AUTH_TOKEN</code> (see example file below),</li> <li>in the <code>.env</code> file set <code>CACHE_TYPE=momento</code> (instead of <code>CACHE_TYPE=redis</code> which is the default).</li> </ul> </li> <li>GitHub Personal Access Token (required for apps that need to analyze git   repos; token-based API calls are less rate-limited). See this   GitHub page.</li> <li>Google Custom Search API Credentials: Only needed to enable an Agent to use the <code>GoogleSearchTool</code>.   To use Google Search as an LLM Tool/Plugin/function-call,   you'll need to set up   a Google API key,   then setup a Google Custom Search Engine (CSE) and get the CSE ID.   (Documentation for these can be challenging, we suggest asking GPT4 for a step-by-step guide.)   After obtaining these credentials, store them as values of   <code>GOOGLE_API_KEY</code> and <code>GOOGLE_CSE_ID</code> in your <code>.env</code> file.   Full documentation on using this (and other such \"stateless\" tools) is coming soon, but   in the meantime take a peek at the test   <code>tests/main/test_google_search_tool.py</code> to see how to use it.</li> </ul> <p>If you add all of these optional variables, your <code>.env</code> file should look like this: <pre><code>OPENAI_API_KEY=your-key-here-without-quotes\nGITHUB_ACCESS_TOKEN=your-personal-access-token-no-quotes\nCACHE_TYPE=redis # or momento\nREDIS_PASSWORD=your-redis-password-no-quotes\nREDIS_HOST=your-redis-hostname-no-quotes\nREDIS_PORT=your-redis-port-no-quotes\nMOMENTO_AUTH_TOKEN=your-momento-token-no-quotes # instead of REDIS* variables\nQDRANT_API_KEY=your-key\nQDRANT_API_URL=https://your.url.here:6333 # note port number must be included\nGOOGLE_API_KEY=your-key\nGOOGLE_CSE_ID=your-cse-id\n</code></pre></p>"},{"location":"quick-start/setup/#microsoft-azure-openai-setupoptional","title":"Microsoft Azure OpenAI setup[Optional]","text":"<p>This section applies only if you are using Microsoft Azure OpenAI.</p> <p>When using Azure OpenAI, additional environment variables are required in the <code>.env</code> file. This page Microsoft Azure OpenAI provides more information, and you can set each environment variable as follows:</p> <ul> <li><code>AZURE_API_KEY</code>, from the value of <code>API_KEY</code></li> <li><code>AZURE_OPENAI_API_BASE</code> from the value of <code>ENDPOINT</code>, typically looks like <code>https://your.domain.azure.com</code>.</li> <li>For <code>AZURE_OPENAI_API_VERSION</code>, you can use the default value in <code>.env-template</code>, and latest version can be found here</li> <li><code>AZURE_OPENAI_DEPLOYMENT_NAME</code> is the name of the deployed model, which is defined by the user during the model setup</li> <li><code>AZURE_GPT_MODEL_NAME</code> GPT-3.5-Turbo or GPT-4 model names that you chose when you setup your Azure OpenAI account.</li> </ul>"},{"location":"quick-start/setup/#next-steps","title":"Next steps","text":"<p>Now you should be ready to use Langroid! As a next step, you may want to see how you can use Langroid to interact  directly with the LLM (OpenAI GPT models only for now).</p>"},{"location":"quick-start/three-agent-chat-num-router/","title":"Three-Agent Collaboration, with message Routing","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>three-agent-chat-num-router.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num-router.py</code>.</p> <p>Let's change the number game from the three agent chat example slightly. In that example, when the <code>even_agent</code>'s LLM receives an odd number, it responds with <code>DO-NOT-KNOW</code>, and similarly for the <code>odd_agent</code> when it receives an even number. The <code>step()</code> method of the <code>repeater_task</code> considers <code>DO-NOT-KNOW</code> to be an invalid response and continues to  look for a valid response from any remaining sub-tasks. Thus there was no need for the <code>processor_agent</code> to specify who should handle the current number.</p> <p>But what if there is a scenario where the <code>even_agent</code> and <code>odd_agent</code> might return a legit but \"wrong\" answer? In this section we add this twist -- when the <code>even_agent</code> receives an odd number, it responds with -10, and similarly for the <code>odd_agent</code> when it receives an even number. We tell the <code>processor_agent</code> to avoid getting a negative number.</p> <p>The goal we have set for the <code>processor_agent</code> implies that it  must specify the intended recipient of  the number it is sending.  We can enforce this using a special Langroid Tool,  <code>RecipientTool</code>. So when setting up the <code>processor_task</code> we include instructions to use this tool (whose name is <code>recipient_message</code>, the value of <code>RecipientTool.request</code>):</p> <pre><code>processor_agent = lr.ChatAgent(config)\nprocessor_task = lr.Task(\n    processor_agent,\n    name = \"Processor\",\n    system_message=\"\"\"\n        You will receive a list of numbers from me (the user).\n        Your goal is to apply a transformation to each number.\n        However you do not know how to do this transformation.\n        You can take the help of two people to perform the \n        transformation.\n        If the number is even, send it to EvenHandler,\n        and if it is odd, send it to OddHandler.\n\n        IMPORTANT: send the numbers ONE AT A TIME\n\n        The handlers will transform the number and give you a new number.        \n        If you send it to the wrong person, you will receive a negative value.\n        Your aim is to never get a negative number, so you must \n        clearly specify who you are sending the number to, using the\n        `recipient_message` tool/function-call, where the `content` field\n        is the number you want to send, and the `recipient` field is the name\n        of the intended recipient, either \"EvenHandler\" or \"OddHandler\".        \n\n        Once all numbers in the given list have been transformed, \n        say DONE and show me the result. \n        Start by asking me for the list of numbers.\n    \"\"\",\n    llm_delegate=True,\n    single_round=False,\n)\n</code></pre> <p>To enable the <code>processor_agent</code> to use this tool, we must enable it: <pre><code>processor_agent.enable_message(lr.agent.tools.RecipientTool)\n</code></pre></p> <p>The rest of the code remains the same as in the previous section, i.e., we simply add the two handler tasks as sub-tasks of the <code>processor_task</code>, like this: <pre><code>processor_task.add_sub_task([even_task, odd_task])\n</code></pre></p> <p>One of the benefits of using the <code>RecipientTool</code> is that it contains  mechanisms to remind the LLM to specify a recipient for its message, when it forgets to do so (this does happen once in a while, even with GPT-4).</p> <p>Feel free to try the working example script <code>three-agent-chat-num-router.py</code> in the  <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num-router.py</code>:</p> <pre><code>python3 examples/quick-start/three-agent-chat-num-router.py\n</code></pre> <p>Below is screenshot of what this might look like, using the OpenAI function-calling  mechanism with the <code>recipient_message</code> tool:</p> <p></p> <p>And here is what it looks like using Langroid's built-in tools mechanism (use the <code>-t</code> option when running the script):</p> <p></p> <p>And here is what it looks like using </p>"},{"location":"quick-start/three-agent-chat-num-router/#next-steps","title":"Next steps","text":"<p>In the next section you will learn how to use Langroid with external documents.</p>"},{"location":"quick-start/three-agent-chat-num/","title":"Three-Agent Collaboration","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>three-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num.py</code>.</p> <p>Let us set up a simple numbers exercise between 3 agents. The <code>Processor</code> agent receives a list of numbers, and its goal is to  apply a transformation to each number \\(n\\). However it does not know how to apply these transformations, and takes the help of two other agents to do so. Given a number \\(n\\),</p> <ul> <li>The <code>EvenHandler</code> returns \\(n/2\\) if n is even, otherwise says <code>DO-NOT-KNOW</code>.</li> <li>The <code>OddHandler</code> returns \\(3n+1\\) if n is odd, otherwise says <code>DO-NOT-KNOW</code>.</li> </ul> <p>As before we first create a common <code>ChatAgentConfig</code> to use for all agents:</p> <pre><code>config = lr.ChatAgentConfig(\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4,\n    ),\n    vecdb=None,\n)\n</code></pre> <p>Next, set up the <code>processor_agent</code>, along with instructions for the task: <pre><code>processor_agent = lr.ChatAgent(config)\nprocessor_task = lr.Task(\n    processor_agent,\n    name = \"Processor\",\n    system_message=\"\"\"\n        You will receive a list of numbers from the user.\n        Your goal is to apply a transformation to each number.\n        However you do not know how to do this transformation,\n        so the user will help you. \n        You can simply send the user each number FROM THE GIVEN LIST\n        and the user will return the result \n        with the appropriate transformation applied.\n        IMPORTANT: only send one number at a time, concisely, say nothing else.\n        Once you have accomplished your goal, say DONE and show the result.\n        Start by asking the user for the list of numbers.\n        \"\"\",\n    llm_delegate=True, #(1)!\n    single_round=False, #(2)!\n)\n</code></pre></p> <ol> <li>Setting the <code>llm_delegate</code> option to <code>True</code> means that the <code>processor_task</code> is     delegated to the LLM (as opposed to the User),      in the sense that the LLM is the one \"seeking\" a response to the latest      number. Specifically, this means that in the <code>processor_task.step()</code>      when a sub-task returns <code>DO-NOT-KNOW</code>,     it is not considered a valid response, and the search for a valid response      continues to the next sub-task if any.</li> <li><code>single_round=False</code> means that the <code>processor_task</code> should not terminate after      a valid response from a responder.</li> </ol> <p>Set up the other two agents and tasks:</p> <pre><code>NO_ANSWER = lr.utils.constants.NO_ANSWER\n\neven_agent = lr.ChatAgent(config)\neven_task = lr.Task(\n    even_agent,\n    name = \"EvenHandler\",\n    system_message=f\"\"\"\n    You will be given a number. \n    If it is even, divide by 2 and say the result, nothing else.\n    If it is odd, say {NO_ANSWER}\n    \"\"\",\n    single_round=True,  # task done after 1 step() with valid response\n)\n\nodd_agent = lr.ChatAgent(config)\nodd_task = lr.Task(\n    odd_agent,\n    name = \"OddHandler\",\n    system_message=f\"\"\"\n    You will be given a number n. \n    If it is odd, return (n*3+1), say nothing else. \n    If it is even, say {NO_ANSWER}\n    \"\"\",\n    single_round=True,  # task done after 1 step() with valid response\n)\n</code></pre> <p>Now add the <code>even_task</code> and <code>odd_task</code> as subtasks of the <code>processor_task</code>,  and then run it as before:</p> <pre><code>processor_task.add_sub_task([even_task, odd_task])\nprocessor_task.run()\n</code></pre> <p>Feel free to try the working example script <code>three-agent-chat-num.py</code> <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num.py</code>:</p> <pre><code>python3 examples/quick-start/three-agent-chat-num.py\n</code></pre> <p>Here's a screenshot of what it looks like: </p>"},{"location":"quick-start/three-agent-chat-num/#next-steps","title":"Next steps","text":"<p>In the next section you will learn how to use Langroid to equip a <code>ChatAgent</code> with tools or function-calling.</p>"},{"location":"quick-start/two-agent-chat-num/","title":"Two-Agent Collaboration","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>two-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/two-agent-chat-num.py</code>.</p> <p>To illustrate these ideas, let's look at a toy example<sup>1</sup> where  a <code>Student</code> agent receives a list of numbers to add. We set up this agent with an instruction that they do not know how to add, and they can ask for help adding pairs of numbers. To add pairs of numbers, we set up an <code>Adder</code> agent.</p> <p>First define a common <code>ChatAgentConfig</code> to use for both agents: <pre><code>config = lr.ChatAgentConfig(\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4,\n    ),\n    vecdb = None, #(1)!\n)\n</code></pre></p> <ol> <li>We don't need access to external docs so we set <code>vecdb=None</code> to avoid     the overhead of loading a vector-store.</li> </ol> <p>Next, set up the student agent and the corresponding task:</p> <pre><code>student_agent = lr.ChatAgent(config)\nstudent_task = lr.Task(\n    student_agent,\n    name = \"Student\",\n    system_message=\"\"\"\n        You will receive a list of numbers from me (the User),\n        and your goal is to calculate their sum.\n        However you do not know how to add numbers.\n        I can help you add numbers, two at a time, since\n        I only know how to add pairs of numbers.\n        Send me a pair of numbers to add, one at a time, \n        and I will tell you their sum.\n        For each question, simply ask me the sum in math notation, \n        e.g., simply say \"1 + 2\", etc, and say nothing else.\n        Once you have added all the numbers in the list, \n        say DONE and give me the final sum. \n        Start by asking me for the list of numbers.\n    \"\"\",\n    llm_delegate = True, #(1)!\n    single_round=False,  # (2)! \n)\n</code></pre> <ol> <li>Whenever we \"flip roles\" and assign the LLM the role of generating questions,     we set <code>llm_delegate=True</code>. In effect this ensures that the LLM \"decides\" when    the task is done.</li> <li>This setting means the task is not a single-round task, i.e. it is not done    after one <code>step()</code> with a valid response.</li> </ol> <p>Next, set up the adder agent and task:</p> <pre><code>adder_agent = lr.ChatAgent(config)\nadder_task = lr.Task(\n    adder_agent,\n    name = \"Adder\", #(1)!\n    system_message=\"\"\"\n        You are an expert on addition of numbers. \n        When given numbers to add, simply return their sum, say nothing else\n        \"\"\",\n    single_round=True,  # task done after 1 step() with valid response (2)!\n)\n</code></pre> <ol> <li>The Task name is used when displaying the conversation in the console.</li> <li>We set <code>single_round=True</code> to ensure that the expert task is done after     one step() with a valid response. </li> </ol> <p>Finally, we add the <code>adder_task</code> as a sub-task of the <code>student_task</code>,  and run the <code>student_task</code>:</p> <pre><code>student_task.add_sub_task(adder_task) #(1)!\nstudent_task.run()\n</code></pre> <ol> <li>When adding just one sub-task, we don't need to use a list.</li> </ol> <p>For a full working example, see the  <code>two-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo. You can run this using: <pre><code>python3 examples/quick-start/two-agent-chat-num.py\n</code></pre></p> <p>Here is an example of the conversation that results:</p> <p></p>"},{"location":"quick-start/two-agent-chat-num/#logs-of-multi-agent-interactions","title":"Logs of multi-agent interactions","text":"<p>For advanced users</p> <p>This section is for advanced users who want more visibility into the internals of multi-agent interactions.</p> <p>When running a multi-agent chat, e.g. using <code>task.run()</code>, two types of logs are generated: - plain-text logs in <code>logs/&lt;task_name&gt;.log</code> - tsv logs in <code>logs/&lt;task_name&gt;.tsv</code></p> <p>It is important to realize that the logs show every iteration  of the loop in <code>Task.step()</code>, i.e. every attempt at responding to the current pending message, even those that are not allowed. The ones marked with an asterisk (*) are the ones that are considered valid responses for a given <code>step()</code> (which is a \"turn\" in the conversation).</p> <p>The plain text logs have color-coding ANSI chars to make them easier to read by doing <code>less &lt;log_file&gt;</code>. The format is (subject to change): <pre><code>(TaskName) Responder SenderEntity (EntityName) (=&gt; Recipient) TOOL Content\n</code></pre></p> <p>The structure of the <code>tsv</code> logs is similar. A great way to view these is to install and use the excellent <code>visidata</code> (https://www.visidata.org/) tool: <pre><code>vd logs/&lt;task_name&gt;.tsv\n</code></pre></p>"},{"location":"quick-start/two-agent-chat-num/#next-steps","title":"Next steps","text":"<p>As a next step, look at how to set up a collaboration among three agents for a simple numbers game.</p> <ol> <li> <p>Toy numerical examples are perfect to illustrate the ideas without   incurring too much token cost from LLM API calls.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/","title":"langroid","text":"<p>langroid/init.py </p> <p>Main langroid package</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>langroid<ul> <li>agent<ul> <li>base</li> <li>batch</li> <li>chat_agent</li> <li>chat_document</li> <li>special<ul> <li>doc_chat_agent</li> <li>recipient_validator_agent</li> <li>relevance_extractor_agent</li> <li>retriever_agent</li> <li>sql<ul> <li>sql_chat_agent</li> <li>utils<ul> <li>description_extractors</li> <li>populate_metadata</li> <li>system_message</li> <li>tools</li> </ul> </li> </ul> </li> <li>table_chat_agent</li> </ul> </li> <li>task</li> <li>tool_message</li> <li>tools<ul> <li>google_search_tool</li> <li>recipient_tool</li> <li>segment_extract_tool</li> </ul> </li> </ul> </li> <li>cachedb<ul> <li>base</li> <li>momento_cachedb</li> <li>redis_cachedb</li> </ul> </li> <li>embedding_models<ul> <li>base</li> <li>models</li> </ul> </li> <li>language_models<ul> <li>azure_openai</li> <li>base</li> <li>config</li> <li>openai_gpt</li> <li>prompt_formatter<ul> <li>base</li> <li>llama2_formatter</li> </ul> </li> <li>utils</li> </ul> </li> <li>mytypes</li> <li>parsing<ul> <li>agent_chats</li> <li>code_parser</li> <li>document_parser</li> <li>json</li> <li>para_sentence_split</li> <li>parser</li> <li>repo_loader</li> <li>search</li> <li>spider</li> <li>table_loader</li> <li>url_loader</li> <li>urls</li> <li>utils</li> <li>web_search</li> </ul> </li> <li>prompts<ul> <li>dialog</li> <li>prompts_config</li> <li>templates</li> <li>transforms</li> </ul> </li> <li>utils<ul> <li>algorithms<ul> <li>graph</li> </ul> </li> <li>configuration</li> <li>constants</li> <li>globals</li> <li>logging</li> <li>output<ul> <li>printing</li> </ul> </li> <li>pydantic_utils</li> <li>system</li> </ul> </li> <li>vector_store<ul> <li>base</li> <li>chromadb</li> <li>lancedb</li> <li>meilisearch</li> <li>momento</li> <li>qdrantdb</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mytypes/","title":"mytypes","text":"<p>langroid/mytypes.py </p>"},{"location":"reference/mytypes/#langroid.mytypes.Entity","title":"<code>Entity</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the different types of entities that can respond to the current message.</p>"},{"location":"reference/mytypes/#langroid.mytypes.DocMetaData","title":"<code>DocMetaData</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Metadata for a document.</p>"},{"location":"reference/mytypes/#langroid.mytypes.DocMetaData.dict","title":"<code>dict(*args, **kwargs)</code>","text":"<p>Override dict method to convert bool fields to int, to appease some downstream libraries,  e.g. Chroma which complains about bool fields in metadata.</p> Source code in <code>langroid/mytypes.py</code> <pre><code>def dict(self, *args: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Override dict method to convert bool fields to int, to appease some\n    downstream libraries,  e.g. Chroma which complains about bool fields in\n    metadata.\n    \"\"\"\n    original_dict = super().dict(*args, **kwargs)\n\n    for key, value in original_dict.items():\n        if isinstance(value, bool):\n            original_dict[key] = 1 * value\n\n    return original_dict\n</code></pre>"},{"location":"reference/mytypes/#langroid.mytypes.Document","title":"<code>Document</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Interface for interacting with a document.</p>"},{"location":"reference/agent/","title":"agent","text":"<p>langroid/agent/init.py </p>"},{"location":"reference/agent/base/","title":"base","text":"<p>langroid/agent/base.py </p>"},{"location":"reference/agent/base/#langroid.agent.base.AgentConfig","title":"<code>AgentConfig</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>General config settings for an LLM agent. This is nested, combining configs of various components.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent","title":"<code>Agent(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>An Agent is an abstraction that encapsulates mainly two components:</p> <ul> <li>a language model (LLM)</li> <li>a vector store (vecdb)</li> </ul> <p>plus associated components such as a parser, and variables that hold information about any tool/function-calling messages that have been defined.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def __init__(self, config: AgentConfig):\n    self.config = config\n    self.lock = asyncio.Lock()  # for async access to update self.llm.usage_cost\n    self.dialog: List[Tuple[str, str]] = []  # seq of LLM (prompt, response) tuples\n    self.llm_tools_map: Dict[str, Type[ToolMessage]] = {}\n    self.llm_tools_handled: Set[str] = set()\n    self.llm_tools_usable: Set[str] = set()\n    self.total_llm_token_cost = 0.0\n    self.total_llm_token_usage = 0\n    self.token_stats_str = \"\"\n    self.default_human_response: Optional[str] = None\n    self._indent = \"\"\n    self.llm = LanguageModel.create(config.llm)\n    self.vecdb = VectorStore.create(config.vecdb) if config.vecdb else None\n    self.parser: Optional[Parser] = (\n        Parser(config.parsing) if config.parsing else None\n    )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.indent","title":"<code>indent: str</code>  <code>property</code> <code>writable</code>","text":"<p>Indentation to print before any responses from the agent's entities.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.entity_responders","title":"<code>entity_responders()</code>","text":"<p>Sequence of (entity, response_method) pairs. This sequence is used     in a <code>Task</code> to respond to the current pending message.     See <code>Task.step()</code> for details. Returns:     Sequence of (entity, response_method) pairs.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders(\n    self,\n) -&gt; List[\n    Tuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n    \"\"\"\n    Sequence of (entity, response_method) pairs. This sequence is used\n        in a `Task` to respond to the current pending message.\n        See `Task.step()` for details.\n    Returns:\n        Sequence of (entity, response_method) pairs.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response),\n        (Entity.LLM, self.llm_response),\n        (Entity.USER, self.user_response),\n    ]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.entity_responders_async","title":"<code>entity_responders_async()</code>","text":"<p>Async version of <code>entity_responders</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders_async(\n    self,\n) -&gt; List[\n    Tuple[\n        Entity,\n        Callable[\n            [None | str | ChatDocument], Coroutine[Any, Any, None | ChatDocument]\n        ],\n    ]\n]:\n    \"\"\"\n    Async version of `entity_responders`. See there for details.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response_async),\n        (Entity.LLM, self.llm_response_async),\n        (Entity.USER, self.user_response_async),\n    ]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.enable_message_handling","title":"<code>enable_message_handling(message_class=None)</code>","text":"<p>Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type     from LLM. Also \"registers\" (i.e. adds) the <code>message_class</code> to the     <code>self.llm_tools_map</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to enable; Optional; if None, all known message classes are enabled for handling.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def enable_message_handling(\n    self, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n    \"\"\"\n    Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n        from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n        `self.llm_tools_map` dict.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to enable;\n            Optional; if None, all known message classes are enabled for handling.\n\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.add(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable a message class from being handled by this Agent.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to disable. If None, all message classes are disabled.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable a message class from being handled by this Agent.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to disable.\n            If None, all message classes are disabled.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.sample_multi_round_dialog","title":"<code>sample_multi_round_dialog()</code>","text":"<p>Generate a sample multi-round dialog based on enabled message classes. Returns:     str: The sample dialog string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def sample_multi_round_dialog(self) -&gt; str:\n    \"\"\"\n    Generate a sample multi-round dialog based on enabled message classes.\n    Returns:\n        str: The sample dialog string.\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    # use at most 2 sample conversations, no need to be exhaustive;\n    sample_convo = [\n        msg_cls().usage_example()  # type: ignore\n        for i, msg_cls in enumerate(enabled_classes)\n        if i &lt; 2\n    ]\n    return \"\\n\\n\".join(sample_convo)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Response from the \"agent itself\", typically (but not only) used to handle LLM's \"tool message\" or <code>function_call</code> (e.g. OpenAI <code>function_call</code>). Args:     msg (str|ChatDocument): the input to respond to: if msg is a string,         and it contains a valid JSON-structured \"tool message\", or         if msg is a ChatDocument, and it contains a <code>function_call</code>. Returns:     Optional[ChatDocument]: the response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def agent_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Response from the \"agent itself\", typically (but not only)\n    used to handle LLM's \"tool message\" or `function_call`\n    (e.g. OpenAI `function_call`).\n    Args:\n        msg (str|ChatDocument): the input to respond to: if msg is a string,\n            and it contains a valid JSON-structured \"tool message\", or\n            if msg is a ChatDocument, and it contains a `function_call`.\n    Returns:\n        Optional[ChatDocument]: the response, packaged as a ChatDocument\n\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = self.handle_message(msg)\n    if results is None:\n        return None\n    if isinstance(results, ChatDocument):\n        return results\n    if not settings.quiet:\n        console.print(f\"[red]{self.indent}\", end=\"\")\n        print(f\"[red]Agent: {results}\")\n    sender_name = self.config.name\n    if isinstance(msg, ChatDocument) and msg.function_call is not None:\n        # if result was from handling an LLM `function_call`,\n        # set sender_name to \"request\", i.e. name of the function_call\n        sender_name = msg.function_call.name\n\n    return ChatDocument(\n        content=results,\n        metadata=ChatDocMetaData(\n            source=Entity.AGENT,\n            sender=Entity.AGENT,\n            sender_name=sender_name,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.user_response","title":"<code>user_response(msg=None)</code>","text":"<p>Get user response to current message. Could allow (human) user to intervene with an actual answer, or quit using \"q\" or \"x\"</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>(str) User response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get user response to current message. Could allow (human) user to intervene\n    with an actual answer, or quit using \"q\" or \"x\"\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n        (str) User response, packaged as a ChatDocument\n\n    \"\"\"\n    if self.default_human_response is not None:\n        # useful for automated testing\n        user_msg = self.default_human_response\n    elif not settings.interactive:\n        user_msg = \"\"\n    else:\n        user_msg = Prompt.ask(\n            f\"[blue]{self.indent}Human \"\n            \"(respond or q, x to exit current level, \"\n            f\"or hit enter to continue)\\n{self.indent}\",\n        ).strip()\n\n    # only return non-None result if user_msg not empty\n    if not user_msg:\n        return None\n    else:\n        if user_msg.startswith(\"SYSTEM\"):\n            user_msg = user_msg[6:].strip()\n            source = Entity.SYSTEM\n            sender = Entity.SYSTEM\n        else:\n            source = Entity.USER\n            sender = Entity.USER\n        return ChatDocument(\n            content=user_msg,\n            metadata=DocMetaData(\n                source=source,\n                sender=sender,\n            ),\n        )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_can_respond","title":"<code>llm_can_respond(message=None)</code>","text":"<p>Whether the LLM can respond to a message. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the LLM can respond to a message.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n\n    Returns:\n\n    \"\"\"\n    if self.llm is None:\n        return False\n\n    if isinstance(message, ChatDocument) and message.function_call is not None:\n        # LLM should not handle `function_call` messages,\n        # EVEN if message.function_call is not a legit function_call\n        # The OpenAI API raises error if there is a message in history\n        # from a non-Assistant role, with a `function_call` in it\n        return False\n\n    if message is not None and len(self.get_tool_messages(message)) &gt; 0:\n        # if there is a valid \"tool\" message (either JSON or via `function_call`)\n        # then LLM cannot respond to it\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_response_async","title":"<code>llm_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `llm_response`. See there for details.\n    \"\"\"\n    if msg is None or not self.llm_can_respond(msg):\n        return None\n\n    if isinstance(msg, ChatDocument):\n        prompt = msg.content\n    else:\n        prompt = msg\n\n    output_len = self.config.llm.max_output_tokens\n    if self.num_tokens(prompt) + output_len &gt; self.llm.completion_context_length():\n        output_len = self.llm.completion_context_length() - self.num_tokens(prompt)\n        if output_len &lt; self.config.llm.min_output_tokens:\n            raise ValueError(\n                \"\"\"\n            Token-length of Prompt + Output is longer than the\n            completion context length of the LLM!\n            \"\"\"\n            )\n        else:\n            logger.warning(\n                f\"\"\"\n            Requested output length has been shortened to {output_len}\n            so that the total length of Prompt + Output is less than\n            the completion context length of the LLM. \n            \"\"\"\n            )\n\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm.agenerate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # We would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response.\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        print(cached + \"[green]\" + response.message)\n    async with self.lock:\n        self.update_token_usage(\n            response,\n            prompt,\n            self.llm.get_stream(),\n            chat=False,  # i.e. it's a completion model not chat model\n            print_response_stats=self.config.show_stats and not settings.quiet,\n        )\n    return ChatDocument.from_LLMResponse(response, displayed=True)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_response","title":"<code>llm_response(msg=None)</code>","text":"<p>LLM response to a prompt. Args:     msg (str|ChatDocument): prompt string, or ChatDocument object</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Response from LLM, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    LLM response to a prompt.\n    Args:\n        msg (str|ChatDocument): prompt string, or ChatDocument object\n\n    Returns:\n        Response from LLM, packaged as a ChatDocument\n    \"\"\"\n    if msg is None or not self.llm_can_respond(msg):\n        return None\n\n    if isinstance(msg, ChatDocument):\n        prompt = msg.content\n    else:\n        prompt = msg\n\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream():\n            # show rich spinner only if not streaming!\n            cm = console.status(\"LLM responding to message...\")\n            stack.enter_context(cm)\n        output_len = self.config.llm.max_output_tokens\n        if (\n            self.num_tokens(prompt) + output_len\n            &gt; self.llm.completion_context_length()\n        ):\n            output_len = self.llm.completion_context_length() - self.num_tokens(\n                prompt\n            )\n            if output_len &lt; self.config.llm.min_output_tokens:\n                raise ValueError(\n                    \"\"\"\n                Token-length of Prompt + Output is longer than the\n                completion context length of the LLM!\n                \"\"\"\n                )\n            else:\n                logger.warning(\n                    f\"\"\"\n                Requested output length has been shortened to {output_len}\n                so that the total length of Prompt + Output is less than\n                the completion context length of the LLM. \n                \"\"\"\n                )\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        response = self.llm.generate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # we would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        console.print(f\"[green]{self.indent}\", end=\"\")\n        print(cached + \"[green]\" + response.message)\n    self.update_token_usage(\n        response,\n        prompt,\n        self.llm.get_stream(),\n        chat=False,  # i.e. it's a completion model not chat model\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    return ChatDocument.from_LLMResponse(response, displayed=True)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.get_json_tool_messages","title":"<code>get_json_tool_messages(input_str)</code>","text":"<p>Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>input string, typically a message sent by an LLM</p> required <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_json_tool_messages(self, input_str: str) -&gt; List[ToolMessage]:\n    \"\"\"\n    Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.\n\n    Args:\n        input_str (str): input string, typically a message sent by an LLM\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n    json_substrings = extract_top_level_json(input_str)\n    if len(json_substrings) == 0:\n        return []\n    results = [self._get_one_tool_message(j) for j in json_substrings]\n    return [r for r in results if r is not None]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.tool_validation_error","title":"<code>tool_validation_error(ve)</code>","text":"<p>Handle a validation error raised when parsing a tool message,     when there is a legit tool name used, but it has missing/bad fields. Args:     tool (ToolMessage): The tool message that failed validation     ve (ValidationError): The exception raised</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message to send back to the LLM</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def tool_validation_error(self, ve: ValidationError) -&gt; str:\n    \"\"\"\n    Handle a validation error raised when parsing a tool message,\n        when there is a legit tool name used, but it has missing/bad fields.\n    Args:\n        tool (ToolMessage): The tool message that failed validation\n        ve (ValidationError): The exception raised\n\n    Returns:\n        str: The error message to send back to the LLM\n    \"\"\"\n    tool_name = cast(ToolMessage, ve.model).default_value(\"request\")\n    bad_field_errors = \"\\n\".join(\n        [f\"{e['loc'][0]}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n    )\n    return f\"\"\"\n    There were one or more errors in your attempt to use the \n    TOOL or function_call named '{tool_name}': \n    {bad_field_errors}\n    Please write your message again, correcting the errors.\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message","title":"<code>handle_message(msg)</code>","text":"<p>Handle a \"tool\" message either a string containing one or more valid \"tool\" JSON substrings,  or a ChatDocument containing a <code>function_call</code> attribute. Handle with the corresponding handler method, and return the results as a combined string.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The string or ChatDocument to handle</p> required <p>Returns:</p> Type Description <code>None | str | ChatDocument</code> <p>Optional[Str]: The result of the handler method in string form so it can</p> <code>None | str | ChatDocument</code> <p>be sent back to the LLM, or None if <code>msg</code> was not successfully</p> <code>None | str | ChatDocument</code> <p>handled by a method.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message(self, msg: str | ChatDocument) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Handle a \"tool\" message either a string containing one or more\n    valid \"tool\" JSON substrings,  or a\n    ChatDocument containing a `function_call` attribute.\n    Handle with the corresponding handler method, and return\n    the results as a combined string.\n\n    Args:\n        msg (str | ChatDocument): The string or ChatDocument to handle\n\n    Returns:\n        Optional[Str]: The result of the handler method in string form so it can\n        be sent back to the LLM, or None if `msg` was not successfully\n        handled by a method.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) == 0:\n        return self.handle_message_fallback(msg)\n\n    results = [self.handle_tool_message(t) for t in tools]\n\n    results_list = [r for r in results if r is not None]\n    if len(results_list) == 0:\n        return self.handle_message_fallback(msg)\n    # there was a non-None result\n    chat_doc_results = [r for r in results_list if isinstance(r, ChatDocument)]\n    if len(chat_doc_results) &gt; 1:\n        logger.warning(\n            \"\"\"There were multiple ChatDocument results from tools,\n            which is unexpected. The first one will be returned, and the others\n            will be ignored.\n            \"\"\"\n        )\n    if len(chat_doc_results) &gt; 0:\n        return chat_doc_results[0]\n\n    str_doc_results = [r for r in results_list if isinstance(r, str)]\n    final = \"\\n\".join(str_doc_results)\n    if final == \"\":\n        logger.warning(\n            \"\"\"final result from a tool handler should not be empty str, since  \n         it would be considered an invalid result and other responders \n         will be tried, and we may not necessarily want that\"\"\"\n        )\n    return final\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method to handle possible \"tool\" msg if not other method applies or if an error is thrown. This method can be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     str: The result of the handler method in string form so it can         be sent back to the LLM.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Fallback method to handle possible \"tool\" msg if not other method applies\n    or if an error is thrown.\n    This method can be overridden by subclasses.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        str: The result of the handler method in string form so it can\n            be sent back to the LLM.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_tool_message","title":"<code>handle_tool_message(tool)</code>","text":"<p>Respond to a tool request from the LLM, in the form of an ToolMessage object. Args:     tool: ToolMessage object representing the tool request.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_tool_message(self, tool: ToolMessage) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Respond to a tool request from the LLM, in the form of an ToolMessage object.\n    Args:\n        tool: ToolMessage object representing the tool request.\n\n    Returns:\n\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    handler_method = getattr(self, tool_name, None)\n    if handler_method is None:\n        return None\n\n    try:\n        result = handler_method(tool)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return result  # type: ignore\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.update_token_usage","title":"<code>update_token_usage(response, prompt, stream, chat=True, print_response_stats=True)</code>","text":"<p>Updates <code>response.usage</code> obj (token usage and cost fields).the usage memebr It updates the cost after checking the cache and updates the tokens (prompts and completion) if the response stream is True, because OpenAI doesn't returns these fields.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>LLMResponse</code> <p>LLMResponse object</p> required <code>prompt</code> <code>str | List[LLMMessage]</code> <p>prompt or list of LLMMessage objects</p> required <code>stream</code> <code>bool</code> <p>whether to update the usage in the response object if the response is not cached.</p> required <code>chat</code> <code>bool</code> <p>whether this is a chat model or a completion model</p> <code>True</code> <code>print_response_stats</code> <code>bool</code> <p>whether to print the response stats</p> <code>True</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def update_token_usage(\n    self,\n    response: LLMResponse,\n    prompt: str | List[LLMMessage],\n    stream: bool,\n    chat: bool = True,\n    print_response_stats: bool = True,\n) -&gt; None:\n    \"\"\"\n    Updates `response.usage` obj (token usage and cost fields).the usage memebr\n    It updates the cost after checking the cache and updates the\n    tokens (prompts and completion) if the response stream is True, because OpenAI\n    doesn't returns these fields.\n\n    Args:\n        response (LLMResponse): LLMResponse object\n        prompt (str | List[LLMMessage]): prompt or list of LLMMessage objects\n        stream (bool): whether to update the usage in the response object\n            if the response is not cached.\n        chat (bool): whether this is a chat model or a completion model\n        print_response_stats (bool): whether to print the response stats\n    \"\"\"\n    if response is None or self.llm is None:\n        return\n\n    # Note: If response was not streamed, then\n    # `response.usage` would already have been set by the API,\n    # so we only need to update in the stream case.\n    if stream:\n        # usage, cost = 0 when response is from cache\n        prompt_tokens = 0\n        completion_tokens = 0\n        cost = 0.0\n        if not response.cached:\n            prompt_tokens = self.num_tokens(prompt)\n            completion_tokens = self.num_tokens(response.message)\n            if response.function_call is not None:\n                completion_tokens += self.num_tokens(str(response.function_call))\n            cost = self.compute_token_cost(prompt_tokens, completion_tokens)\n        response.usage = LLMTokenUsage(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            cost=cost,\n        )\n\n    # update total counters\n    if response.usage is not None:\n        self.total_llm_token_cost += response.usage.cost\n        self.total_llm_token_usage += response.usage.total_tokens\n        self.llm.update_usage_cost(\n            chat,\n            response.usage.prompt_tokens,\n            response.usage.completion_tokens,\n            response.usage.cost,\n        )\n        chat_length = 1 if isinstance(prompt, str) else len(prompt)\n        self.token_stats_str = self._get_response_stats(\n            chat_length, self.total_llm_token_cost, response\n        )\n        if print_response_stats:\n            print(self.indent + self.token_stats_str)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.ask_agent","title":"<code>ask_agent(agent, request, no_answer=NO_ANSWER, user_confirm=True)</code>","text":"<p>Send a request to another agent, possibly after confirming with the user. This is not currently used, since we rely on the task loop and <code>RecipientTool</code> to address requests to other agents. It is generally best to avoid using this method.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to ask</p> required <code>request</code> <code>str</code> <p>request to send</p> required <code>no_answer</code> <code>str</code> <p>expected response when agent does not know the answer</p> <code>NO_ANSWER</code> <code>user_confirm</code> <code>bool</code> <p>whether to gate the request with a human confirmation</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>response from agent</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def ask_agent(\n    self,\n    agent: \"Agent\",\n    request: str,\n    no_answer: str = NO_ANSWER,\n    user_confirm: bool = True,\n) -&gt; Optional[str]:\n    \"\"\"\n    Send a request to another agent, possibly after confirming with the user.\n    This is not currently used, since we rely on the task loop and\n    `RecipientTool` to address requests to other agents. It is generally best to\n    avoid using this method.\n\n    Args:\n        agent (Agent): agent to ask\n        request (str): request to send\n        no_answer (str): expected response when agent does not know the answer\n        user_confirm (bool): whether to gate the request with a human confirmation\n\n    Returns:\n        str: response from agent\n    \"\"\"\n    agent_type = type(agent).__name__\n    if user_confirm:\n        user_response = Prompt.ask(\n            f\"\"\"[magenta]Here is the request or message:\n            {request}\n            Should I forward this to {agent_type}?\"\"\",\n            default=\"y\",\n            choices=[\"y\", \"n\"],\n        )\n        if user_response not in [\"y\", \"yes\"]:\n            return None\n    answer = agent.llm_response(request)\n    if answer != no_answer:\n        return (f\"{agent_type} says: \" + str(answer)).strip()\n    return None\n</code></pre>"},{"location":"reference/agent/batch/","title":"batch","text":"<p>langroid/agent/batch.py </p>"},{"location":"reference/agent/batch/#langroid.agent.batch.run_batch_tasks","title":"<code>run_batch_tasks(task, items, input_map=lambda : str(x), output_map=lambda : x)</code>","text":"<p>Run copies of <code>task</code> async/concurrently one per item in <code>items</code> list. For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result. Args:     task (Task): task to run     items (List[Any]): list of items to process     input_map (Callable[[Any], str|ChatDocument]): function to map item to         initial message to process     output_map (Callable[[ChatDocument|str], Any]): function to map result         to final result</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List[Any]: list of final results</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_tasks(\n    task: Task,\n    items: List[Any],\n    input_map: Callable[[Any], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], Any] = lambda x: x,\n) -&gt; List[Any]:\n    \"\"\"\n    Run copies of `task` async/concurrently one per item in `items` list.\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n    Args:\n        task (Task): task to run\n        items (List[Any]): list of items to process\n        input_map (Callable[[Any], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], Any]): function to map result\n            to final result\n\n    Returns:\n        List[Any]: list of final results\n    \"\"\"\n\n    inputs = [input_map(item) for item in items]\n\n    async def _do_task(input: str | ChatDocument, i: int) -&gt; Any:\n        task_i = task.clone(i)\n        if task_i.agent.config.llm is not None:\n            task_i.agent.config.llm.stream = False\n            task_i.agent.config.show_stats = False\n\n        result = await task_i.run_async(input)\n        return output_map(result)\n\n    async def _do_all() -&gt; List[Any]:\n        with quiet_mode(not settings.debug):\n            return await asyncio.gather(  # type: ignore\n                *(_do_task(input, i) for i, input in enumerate(inputs))\n            )\n\n    # show rich console spinner\n\n    n = len(items)\n    with console.status(f\"[bold green]Running {n} copies of {task.name}...\"):\n        results = asyncio.run(_do_all())\n\n    return results\n</code></pre>"},{"location":"reference/agent/batch/#langroid.agent.batch.run_batch_agent_method","title":"<code>run_batch_agent_method(agent, method, items, input_map=lambda : str(x), output_map=lambda : x)</code>","text":"<p>Run the <code>method</code> on copies of <code>agent</code>, async/concurrently one per item in <code>items</code> list. ASSUMPTION: The <code>method</code> is an async method and has signature:     method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None So this would typically be used for the agent's \"responder\" methods, e.g. <code>llm_response_async</code> or <code>agent_responder_async</code>.</p> <p>For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent whose method to run</p> required <code>method</code> <code>str</code> <p>Async method to run on copies of <code>agent</code>. The method is assumed to have signature: <code>method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None</code></p> required <code>input_map</code> <code>Callable[[Any], str | ChatDocument]</code> <p>function to map item to initial message to process</p> <code>lambda : str(x)</code> <code>output_map</code> <code>Callable[[ChatDocument | str], Any]</code> <p>function to map result to final result</p> <code>lambda : x</code> <p>Returns:     List[Any]: list of final results</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_agent_method(\n    agent: Agent,\n    method: Callable[\n        [str | ChatDocument | None], Coroutine[Any, Any, ChatDocument | None]\n    ],\n    items: List[Any],\n    input_map: Callable[[Any], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], Any] = lambda x: x,\n) -&gt; List[Any]:\n    \"\"\"\n    Run the `method` on copies of `agent`, async/concurrently one per\n    item in `items` list.\n    ASSUMPTION: The `method` is an async method and has signature:\n        method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None\n    So this would typically be used for the agent's \"responder\" methods,\n    e.g. `llm_response_async` or `agent_responder_async`.\n\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n\n    Args:\n        agent (Agent): agent whose method to run\n        method (str): Async method to run on copies of `agent`.\n            The method is assumed to have signature:\n            `method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None`\n        input_map (Callable[[Any], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], Any]): function to map result\n            to final result\n    Returns:\n        List[Any]: list of final results\n    \"\"\"\n    # Check if the method is async\n    method_name = method.__name__\n    if not inspect.iscoroutinefunction(method):\n        raise ValueError(f\"The method {method_name} is not async.\")\n\n    inputs = [input_map(item) for item in items]\n    agent_cfg = copy.deepcopy(agent.config)\n    assert agent_cfg.llm is not None, \"agent must have llm config\"\n    agent_cfg.llm.stream = False\n    agent_cfg.show_stats = False\n    agent_cls = type(agent)\n    agent_name = agent_cfg.name\n\n    async def _do_task(input: str | ChatDocument, i: int) -&gt; Any:\n        agent_cfg.name = f\"{agent_cfg.name}-{i}\"\n        agent_i = agent_cls(agent_cfg)\n        method_i = getattr(agent_i, method_name, None)\n        if method_i is None:\n            raise ValueError(f\"Agent {agent_name} has no method {method_name}\")\n        result = await method_i(input)\n        return output_map(result)\n\n    async def _do_all() -&gt; List[Any]:\n        with quiet_mode():\n            return await asyncio.gather(  # type: ignore\n                *(_do_task(input, i) for i, input in enumerate(inputs))\n            )\n\n    n = len(items)\n    with console.status(f\"[bold green]Running {n} copies of {agent_name}...\"):\n        results = asyncio.run(_do_all())\n\n    return results\n</code></pre>"},{"location":"reference/agent/chat_agent/","title":"chat_agent","text":"<p>langroid/agent/chat_agent.py </p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgentConfig","title":"<code>ChatAgentConfig</code>","text":"<p>             Bases: <code>AgentConfig</code></p> <p>Configuration for ChatAgent Attributes:     system_message: system message to include in message sequence          (typically defines role and task of agent).          Used only if <code>task</code> is not specified in the constructor.     user_message: user message to include in message sequence.          Used only if <code>task</code> is not specified in the constructor.     use_tools: whether to use our own ToolMessages mechanism     use_functions_api: whether to use functions native to the LLM API             (e.g. OpenAI's <code>function_call</code> mechanism)</p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent","title":"<code>ChatAgent(config, task=None)</code>","text":"<p>             Bases: <code>Agent</code></p> <p>Chat Agent interacting with external env (could be human, or external tools). The agent (the LLM actually) is provided with an optional \"Task Spec\", which is a sequence of <code>LLMMessage</code>s. These are used to initialize the <code>task_messages</code> of the agent. In most applications we will use a <code>ChatAgent</code> rather than a bare <code>Agent</code>. The <code>Agent</code> class mainly exists to hold various common methods and attributes. One difference between <code>ChatAgent</code> and <code>Agent</code> is that <code>ChatAgent</code>'s <code>llm_response</code> method uses \"chat mode\" API (i.e. one that takes a message sequence rather than a single message), whereas the same method in the <code>Agent</code> class uses \"completion mode\" API (i.e. one that takes a single message).</p> <pre><code>config: settings for the agent\n</code></pre> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def __init__(\n    self, config: ChatAgentConfig, task: Optional[List[LLMMessage]] = None\n):\n    \"\"\"\n    Chat-mode agent initialized with task spec as the initial message sequence\n    Args:\n        config: settings for the agent\n\n    \"\"\"\n    super().__init__(config)\n    self.config: ChatAgentConfig = config\n    if (\n        self.llm is not None\n        and (\n            not isinstance(self.llm, OpenAIGPT)\n            or not self.llm.is_openai_chat_model()\n        )\n        and self.config.use_functions_api\n    ):\n        # for non-OpenAI models, use Langroid Tool instead of Function-calling\n        logger.warning(\n            f\"\"\"\n            Function calling not available for {self.llm.config.chat_model},\n            switching to Langroid Tools instead.\n            \"\"\"\n        )\n        self.config._switch_fn_to_tools()\n\n    self.message_history: List[LLMMessage] = []\n    self.tool_instructions_added: bool = False\n    # An agent's \"task\" is defined by a system msg and an optional user msg;\n    # These are \"priming\" messages that kick off the agent's conversation.\n    self.system_message: str = self.config.system_message\n    self.user_message: str | None = self.config.user_message\n\n    if task is not None:\n        # if task contains a system msg, we override the config system msg\n        if len(task) &gt; 0 and task[0].role == Role.SYSTEM:\n            self.system_message = task[0].content\n        # if task contains a user msg, we override the config user msg\n        if len(task) &gt; 1 and task[1].role == Role.USER:\n            self.user_message = task[1].content\n\n    # system-level instructions for using tools/functions:\n    # We maintain these as tools/functions are enabled/disabled,\n    # and whenever an LLM response is sought, these are used to\n    # recreate the system message (via `_create_system_and_tools_message`)\n    # each time, so it reflects the current set of enabled tools/functions.\n    # (a) these are general instructions on using certain tools/functions,\n    #   if they are specified in a ToolMessage class as a classmethod `instructions`\n    self.system_tool_instructions: str = \"\"\n    # (b) these are only for the builtin in Langroid TOOLS mechanism:\n    self.system_json_tool_instructions: str = \"\"\n\n    self.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\n    self.llm_functions_handled: Set[str] = set()\n    self.llm_functions_usable: Set[str] = set()\n    self.llm_function_force: Optional[Dict[str, str]] = None\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.task_messages","title":"<code>task_messages: List[LLMMessage]</code>  <code>property</code>","text":"<p>The task messages are the initial messages that define the task of the agent. There will be at least a system message plus possibly a user msg. Returns:     List[LLMMessage]: the task messages</p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.clear_history","title":"<code>clear_history(start=-2)</code>","text":"<p>Clear the message history, starting at the index <code>start</code></p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>index of first message to delete; default = -2     (i.e. delete last 2 messages, typically these     are the last user and assistant messages)</p> <code>-2</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clear_history(self, start: int = -2) -&gt; None:\n    \"\"\"\n    Clear the message history, starting at the index `start`\n\n    Args:\n        start (int): index of first message to delete; default = -2\n                (i.e. delete last 2 messages, typically these\n                are the last user and assistant messages)\n    \"\"\"\n    if start &lt; 0:\n        n = len(self.message_history)\n        start = max(0, n + start)\n    self.message_history = self.message_history[:start]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_history","title":"<code>update_history(message, response)</code>","text":"<p>Update the message history with the latest user message and LLM response. Args:     message (str): user message     response: (str): LLM response</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_history(self, message: str, response: str) -&gt; None:\n    \"\"\"\n    Update the message history with the latest user message and LLM response.\n    Args:\n        message (str): user message\n        response: (str): LLM response\n    \"\"\"\n    self.message_history.extend(\n        [\n            LLMMessage(role=Role.USER, content=message),\n            LLMMessage(role=Role.ASSISTANT, content=response),\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.json_format_rules","title":"<code>json_format_rules()</code>","text":"<p>Specification of JSON formatting rules, based on the currently enabled usable <code>ToolMessage</code>s</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatting rules</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def json_format_rules(self) -&gt; str:\n    \"\"\"\n    Specification of JSON formatting rules, based on the currently enabled\n    usable `ToolMessage`s\n\n    Returns:\n        str: formatting rules\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"You can ask questions in natural language.\"\n\n    json_instructions = \"\\n\\n\".join(\n        [\n            textwrap.dedent(\n                f\"\"\"\n            TOOL: {msg_cls.default_value(\"request\")}\n            PURPOSE: {msg_cls.default_value(\"purpose\")} \n            JSON FORMAT: {\n                json.dumps(\n                    msg_cls.llm_function_schema(request=True).parameters,\n                    indent=4,\n                )\n            }\n            {\"EXAMPLE: \" + msg_cls.usage_example() if msg_cls.examples() else \"\"}\n            \"\"\".lstrip()\n            )\n            for i, msg_cls in enumerate(enabled_classes)\n            if msg_cls.default_value(\"request\") in self.llm_tools_usable\n        ]\n    )\n    return textwrap.dedent(\n        f\"\"\"\n        === ALL AVAILABLE TOOLS and THEIR JSON FORMAT INSTRUCTIONS ===\n        You have access to the following TOOLS to accomplish your task:\n\n        {json_instructions}\n\n        When one of the above TOOLs is applicable, you must express your \n        request as \"TOOL:\" followed by the request in the above JSON format.\n        \"\"\"\n        + \"\"\"\n        The JSON format will be:\n            \\\\{\n                \"request\": \"&lt;tool_name&gt;\",\n                \"&lt;arg1&gt;\": &lt;value1&gt;,\n                \"&lt;arg2&gt;\": &lt;value2&gt;,\n                ...\n            \\\\}             \n        ----------------------------\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.tool_instructions","title":"<code>tool_instructions()</code>","text":"<p>Instructions for tools or function-calls, for enabled and usable Tools. These are inserted into system prompt regardless of whether we are using our own ToolMessage mechanism or the LLM's function-call mechanism.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>concatenation of instructions for all usable tools</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_instructions(self) -&gt; str:\n    \"\"\"\n    Instructions for tools or function-calls, for enabled and usable Tools.\n    These are inserted into system prompt regardless of whether we are using\n    our own ToolMessage mechanism or the LLM's function-call mechanism.\n\n    Returns:\n        str: concatenation of instructions for all usable tools\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"\"\n    instructions = []\n    for msg_cls in enabled_classes:\n        if (\n            hasattr(msg_cls, \"instructions\")\n            and inspect.ismethod(msg_cls.instructions)\n            and msg_cls.default_value(\"request\") in self.llm_tools_usable\n        ):\n            # example will be shown in json_format_rules() when using TOOLs,\n            # so we don't need to show it here.\n            example = \"\" if self.config.use_tools else (msg_cls.usage_example())\n            if example != \"\":\n                example = \"EXAMPLE: \" + example\n            guidance = (\n                \"\"\n                if msg_cls.instructions() == \"\"\n                else (\"GUIDANCE: \" + msg_cls.instructions())\n            )\n            if guidance == \"\" and example == \"\":\n                continue\n            instructions.append(\n                textwrap.dedent(\n                    f\"\"\"\n                    TOOL: {msg_cls.default_value(\"request\")}:\n                    {guidance}\n                    {example}\n                    \"\"\".lstrip()\n                )\n            )\n    if len(instructions) == 0:\n        return \"\"\n    instructions_str = \"\\n\\n\".join(instructions)\n    return textwrap.dedent(\n        f\"\"\"\n        === GUIDELINES ON SOME TOOLS/FUNCTIONS USAGE ===\n        {instructions_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.augment_system_message","title":"<code>augment_system_message(message)</code>","text":"<p>Augment the system message with the given message. Args:     message (str): system message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def augment_system_message(self, message: str) -&gt; None:\n    \"\"\"\n    Augment the system message with the given message.\n    Args:\n        message (str): system message\n    \"\"\"\n    self.system_message += \"\\n\\n\" + message\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.last_message_with_role","title":"<code>last_message_with_role(role)</code>","text":"<p>from <code>message_history</code>, return the last message with role <code>role</code></p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_with_role(self, role: Role) -&gt; LLMMessage | None:\n    \"\"\"from `message_history`, return the last message with role `role`\"\"\"\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            return self.message_history[i]\n    return None\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_last_message","title":"<code>update_last_message(message, role=Role.USER)</code>","text":"<p>Update the last message that has role <code>role</code> in the message history. Useful when we want to replace a long user prompt, that may contain context documents plus a question, with just the question. Args:     message (str): user message     role (str): role of message to replace</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Update the last message that has role `role` in the message history.\n    Useful when we want to replace a long user prompt, that may contain context\n    documents plus a question, with just the question.\n    Args:\n        message (str): user message\n        role (str): role of message to replace\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history[i].content = message\n            break\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False, require_recipient=False)</code>","text":"<p>Add the tool (message class) to the agent, and enable either - tool USE (i.e. the LLM can generate JSON to use this tool), - tool HANDLING (i.e. the agent can handle JSON from this tool),</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to enable, for USE, or HANDLING, or both. Optional; if None, then apply the enabling to all tools in the agent's toolset that have been enabled so far.</p> required <code>use</code> <code>bool</code> <p>IF True, allow the agent (LLM) to use this tool (or all tools), else disallow</p> <code>True</code> <code>handle</code> <code>bool</code> <p>if True, allow the agent (LLM) to handle (i.e. respond to) this tool (or all tools)</p> <code>True</code> <code>force</code> <code>bool</code> <p>whether to FORCE the agent (LLM) to USE the specific  tool represented by <code>message_class</code>.  <code>force</code> is ignored if <code>message_class</code> is None.</p> <code>False</code> <code>require_recipient</code> <code>bool</code> <p>whether to require that recipient be specified when using the tool message (only applies if <code>use</code> is True).</p> <code>False</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def enable_message(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n    use: bool = True,\n    handle: bool = True,\n    force: bool = False,\n    require_recipient: bool = False,\n) -&gt; None:\n    \"\"\"\n    Add the tool (message class) to the agent, and enable either\n    - tool USE (i.e. the LLM can generate JSON to use this tool),\n    - tool HANDLING (i.e. the agent can handle JSON from this tool),\n\n    Args:\n        message_class: The ToolMessage class to enable,\n            for USE, or HANDLING, or both.\n            Optional; if None, then apply the enabling to all tools in the\n            agent's toolset that have been enabled so far.\n        use: IF True, allow the agent (LLM) to use this tool (or all tools),\n            else disallow\n        handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n            tool (or all tools)\n        force: whether to FORCE the agent (LLM) to USE the specific\n             tool represented by `message_class`.\n             `force` is ignored if `message_class` is None.\n        require_recipient: whether to require that recipient be specified\n            when using the tool message (only applies if `use` is True).\n\n    \"\"\"\n    super().enable_message_handling(message_class)  # enables handling only\n    tools = self._get_tool_list(message_class)\n    if message_class is not None:\n        if require_recipient:\n            message_class = message_class.require_recipient()\n        request = message_class.default_value(\"request\")\n        llm_function = message_class.llm_function_schema()\n        self.llm_functions_map[request] = llm_function\n        if force:\n            self.llm_function_force = dict(name=request)\n        else:\n            self.llm_function_force = None\n\n    for t in tools:\n        if handle:\n            self.llm_tools_handled.add(t)\n            self.llm_functions_handled.add(t)\n        else:\n            self.llm_tools_handled.discard(t)\n            self.llm_functions_handled.discard(t)\n\n        if use:\n            self.llm_tools_usable.add(t)\n            self.llm_functions_usable.add(t)\n        else:\n            self.llm_tools_usable.discard(t)\n            self.llm_functions_usable.discard(t)\n\n    # Set tool instructions and JSON format instructions\n    if self.config.use_tools:\n        self.system_json_tool_instructions = self.json_format_rules()\n    self.system_tool_instructions = self.tool_instructions()\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable this agent from RESPONDING to a <code>message_class</code> (Tool). If     <code>message_class</code> is None, then disable this agent from responding to ALL. Args:     message_class: The ToolMessage class to disable; Optional.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable this agent from RESPONDING to a `message_class` (Tool). If\n        `message_class` is None, then disable this agent from responding to ALL.\n    Args:\n        message_class: The ToolMessage class to disable; Optional.\n    \"\"\"\n    super().disable_message_handling(message_class)\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n        self.llm_functions_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use","title":"<code>disable_message_use(message_class)</code>","text":"<p>Disable this agent from USING a message class (Tool). If <code>message_class</code> is None, then disable this agent from USING ALL tools. Args:     message_class: The ToolMessage class to disable.         If None, disable all.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n    \"\"\"\n    Disable this agent from USING a message class (Tool).\n    If `message_class` is None, then disable this agent from USING ALL tools.\n    Args:\n        message_class: The ToolMessage class to disable.\n            If None, disable all.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_usable.discard(t)\n        self.llm_functions_usable.discard(t)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use_except","title":"<code>disable_message_use_except(message_class)</code>","text":"<p>Disable this agent from USING ALL messages EXCEPT a message class (Tool) Args:     message_class: The only ToolMessage class to allow</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n    \"\"\"\n    Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n    Args:\n        message_class: The only ToolMessage class to allow\n    \"\"\"\n    request = message_class.__fields__[\"request\"].default\n    for r in self.llm_functions_usable:\n        if r != request:\n            self.llm_tools_usable.discard(r)\n            self.llm_functions_usable.discard(r)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Respond to a single user message, appended to the message history, in \"chat\" mode Args:     message (str|ChatDocument): message or ChatDocument object to respond to.         If None, use the self.task_messages Returns:     LLM response as a ChatDocument object</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Respond to a single user message, appended to the message history,\n    in \"chat\" mode\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n            If None, use the self.task_messages\n    Returns:\n        LLM response as a ChatDocument object\n    \"\"\"\n    if self.llm is None:\n        return None\n    hist, output_len = self._prep_llm_messages(message)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = self.llm_response_messages(hist, output_len)\n    # TODO - when response contains function_call we should include\n    # that (and related fields) in the message_history\n    self.message_history.append(ChatDocument.to_LLMMessage(response))\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async version of `llm_response`. See there for details.\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    hist, output_len = self._prep_llm_messages(message)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm_response_messages_async(hist, output_len)\n    # TODO - when response contains function_call we should include\n    # that (and related fields) in the message_history\n    self.message_history.append(ChatDocument.to_LLMMessage(response))\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_messages","title":"<code>llm_response_messages(messages, output_len=None)</code>","text":"<p>Respond to a series of messages, e.g. with OpenAI ChatCompletion Args:     messages: seq of messages (with role, content fields) sent to LLM     output_len: max number of tokens expected in response.             If None, use the LLM's default max_output_tokens. Returns:     Document (i.e. with fields \"content\", \"metadata\")</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_messages(\n    self, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Respond to a series of messages, e.g. with OpenAI ChatCompletion\n    Args:\n        messages: seq of messages (with role, content fields) sent to LLM\n        output_len: max number of tokens expected in response.\n                If None, use the LLM's default max_output_tokens.\n    Returns:\n        Document (i.e. with fields \"content\", \"metadata\")\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.max_output_tokens\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream() and not settings.quiet:\n            # show rich spinner only if not streaming!\n            cm = console.status(\"LLM responding to messages...\")\n            stack.enter_context(cm)\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        functions: Optional[List[LLMFunctionSpec]] = None\n        fun_call: str | Dict[str, str] = \"none\"\n        if self.config.use_functions_api and len(self.llm_functions_usable) &gt; 0:\n            functions = [\n                self.llm_functions_map[f] for f in self.llm_functions_usable\n            ]\n            fun_call = (\n                \"auto\"\n                if self.llm_function_force is None\n                else self.llm_function_force\n            )\n        assert self.llm is not None\n        response = self.llm.chat(\n            messages,\n            output_len,\n            functions=functions,\n            function_call=fun_call,\n        )\n    if not self.llm.get_stream() or response.cached:\n        # We would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response.\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        if response.function_call is not None:\n            response_str = str(response.function_call)\n        else:\n            response_str = response.message\n        if not settings.quiet:\n            print(cached + \"[green]\" + response_str)\n    self.update_token_usage(\n        response,\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    return ChatDocument.from_LLMResponse(response, displayed=True)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_messages_async","title":"<code>llm_response_messages_async(messages, output_len=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_messages</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_messages_async(\n    self, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_messages`. See there for details.\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.max_output_tokens\n    functions: Optional[List[LLMFunctionSpec]] = None\n    fun_call: str | Dict[str, str] = \"none\"\n    if self.config.use_functions_api and len(self.llm_functions_usable) &gt; 0:\n        functions = [self.llm_functions_map[f] for f in self.llm_functions_usable]\n        fun_call = (\n            \"auto\" if self.llm_function_force is None else self.llm_function_force\n        )\n    assert self.llm is not None\n    response = await self.llm.achat(\n        messages,\n        output_len,\n        functions=functions,\n        function_call=fun_call,\n    )\n\n    if not self.llm.get_stream() or response.cached:\n        # We would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response.\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        if response.function_call is not None:\n            response_str = str(response.function_call)\n        else:\n            response_str = response.message\n        if not settings.quiet:\n            print(cached + \"[green]\" + response_str)\n\n    self.update_token_usage(\n        response,\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    return ChatDocument.from_LLMResponse(response, displayed=True)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_forget","title":"<code>llm_response_forget(message)</code>","text":"<p>LLM Response to single message, and restore message_history. In effect a \"one-off\" message &amp; response that leaves agent message history state intact.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <p>Returns:</p> Type Description <code>ChatDocument</code> <p>A Document object with the response.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_forget(self, message: str) -&gt; ChatDocument:\n    \"\"\"\n    LLM Response to single message, and restore message_history.\n    In effect a \"one-off\" message &amp; response that leaves agent\n    message history state intact.\n\n    Args:\n        message (str): user message\n\n    Returns:\n        A Document object with the response.\n\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(ChatDocument, ChatAgent.llm_response(self, message))\n    # clear the last two messages, which are the\n    # user message and the assistant response\n    self.message_history.pop()\n    self.message_history.pop()\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_forget_async","title":"<code>llm_response_forget_async(message)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_forget</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_forget_async(self, message: str) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_forget`. See there for details.\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(\n            ChatDocument, await ChatAgent.llm_response_async(self, message)\n        )\n    # clear the last two messages, which are the\n    # user message and the assistant response\n    self.message_history.pop()\n    self.message_history.pop()\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.chat_num_tokens","title":"<code>chat_num_tokens(messages=None)</code>","text":"<p>Total number of tokens in the message history so far.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Optional[List[LLMMessage]]</code> <p>if provided, compute the number of tokens in this list of messages, rather than the current message history.</p> <code>None</code> <p>Returns:     int: number of tokens in message history</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n    \"\"\"\n    Total number of tokens in the message history so far.\n\n    Args:\n        messages: if provided, compute the number of tokens in this list of\n            messages, rather than the current message history.\n    Returns:\n        int: number of tokens in message history\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\n            \"ChatAgent.parser is None. \"\n            \"You must set ChatAgent.parser \"\n            \"before calling chat_num_tokens().\"\n        )\n    hist = messages if messages is not None else self.message_history\n    return sum([self.parser.num_tokens(m.content) for m in hist])\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.message_history_str","title":"<code>message_history_str(i=None)</code>","text":"<p>Return a string representation of the message history Args:     i: if provided, return only the i-th message when i is postive,         or last k messages when i = -k. Returns:</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def message_history_str(self, i: Optional[int] = None) -&gt; str:\n    \"\"\"\n    Return a string representation of the message history\n    Args:\n        i: if provided, return only the i-th message when i is postive,\n            or last k messages when i = -k.\n    Returns:\n    \"\"\"\n    if i is None:\n        return \"\\n\".join([str(m) for m in self.message_history])\n    elif i &gt; 0:\n        return str(self.message_history[i])\n    else:\n        return \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/agent/chat_document/","title":"chat_document","text":"<p>langroid/agent/chat_document.py </p>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument","title":"<code>ChatDocument</code>","text":"<p>             Bases: <code>Document</code></p>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.get_json_tools","title":"<code>get_json_tools()</code>","text":"<p>Get names of attempted JSON tool usages in the content     of the message. Returns:     List[str]: list of JSON tool names</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def get_json_tools(self) -&gt; List[str]:\n    \"\"\"\n    Get names of attempted JSON tool usages in the content\n        of the message.\n    Returns:\n        List[str]: list of JSON tool names\n    \"\"\"\n    jsons = extract_top_level_json(self.content)\n    tools = []\n    for j in jsons:\n        json_data = json.loads(j)\n        tool = json_data.get(\"request\")\n        if tool is not None:\n            tools.append(tool)\n    return tools\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.log_fields","title":"<code>log_fields()</code>","text":"<p>Fields for logging in csv/tsv logger Returns:     List[str]: list of fields</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def log_fields(self) -&gt; ChatDocLoggerFields:\n    \"\"\"\n    Fields for logging in csv/tsv logger\n    Returns:\n        List[str]: list of fields\n    \"\"\"\n    tool_type = \"\"  # FUNC or TOOL\n    tool = \"\"  # tool name or function name\n    if self.function_call is not None:\n        tool_type = \"FUNC\"\n        tool = self.function_call.name\n    elif self.get_json_tools() != []:\n        tool_type = \"TOOL\"\n        tool = self.get_json_tools()[0]\n    recipient = self.metadata.recipient\n    content = self.content\n    sender_entity = self.metadata.sender\n    sender_name = self.metadata.sender_name\n    return ChatDocLoggerFields(\n        sender_entity=sender_entity,\n        sender_name=sender_name,\n        recipient=recipient,\n        block=self.metadata.block,\n        tool_type=tool_type,\n        tool=tool,\n        content=content,\n    )\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.to_LLMMessage","title":"<code>to_LLMMessage(message)</code>  <code>staticmethod</code>","text":"<p>Convert to LLMMessage for use with LLM.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>Message to convert.</p> required <p>Returns:     LLMMessage: LLMMessage representation of this str or ChatDocument.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef to_LLMMessage(message: str | Type[\"ChatDocument\"]) -&gt; LLMMessage:\n    \"\"\"\n    Convert to LLMMessage for use with LLM.\n\n    Args:\n        message (str|ChatDocument): Message to convert.\n    Returns:\n        LLMMessage: LLMMessage representation of this str or ChatDocument.\n\n    \"\"\"\n    sender_name = None\n    sender_role = Role.USER\n    fun_call = None\n    if isinstance(message, ChatDocument):\n        content = message.content\n        fun_call = message.function_call\n        sender_name = message.metadata.sender_name\n        if message.metadata.sender == Entity.SYSTEM:\n            sender_role = Role.SYSTEM\n        if (\n            message.metadata.parent is not None\n            and message.metadata.parent.function_call is not None\n        ):\n            sender_role = Role.FUNCTION\n            sender_name = message.metadata.parent.function_call.name\n        elif message.metadata.sender == Entity.LLM:\n            sender_role = Role.ASSISTANT\n    else:\n        # LLM can only respond to text content, so extract it\n        content = message\n\n    return LLMMessage(\n        role=sender_role,\n        content=content,\n        function_call=fun_call,\n        name=sender_name,\n    )\n</code></pre>"},{"location":"reference/agent/task/","title":"task","text":"<p>langroid/agent/task.py </p>"},{"location":"reference/agent/task/#langroid.agent.task.Task","title":"<code>Task(agent, name='', llm_delegate=False, single_round=False, system_message='', user_message='', restart=False, default_human_response=None, interactive=True, only_user_quits_root=True, erase_substeps=False)</code>","text":"<p>A <code>Task</code> wraps an <code>Agent</code> object, and sets up the <code>Agent</code>'s goals and instructions. A <code>Task</code> maintains two key variables:</p> <ul> <li><code>self.pending_message</code>, which is the message awaiting a response, and</li> <li><code>self.pending_sender</code>, which is the entity that sent the pending message.</li> </ul> <p>The possible responders to <code>self.pending_message</code> are the <code>Agent</code>'s own \"native\" responders (<code>agent_response</code>, <code>llm_response</code>, and <code>user_response</code>), and the <code>run()</code> methods of any sub-tasks. All responders have the same type-signature (somewhat simplified): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> Responders may or may not specify an intended recipient of their generated response.</p> <p>The main top-level method in the <code>Task</code> class is <code>run()</code>, which repeatedly calls <code>step()</code> until <code>done()</code> returns true. The <code>step()</code> represents a \"turn\" in the conversation: this method sequentially (in round-robin fashion) calls the responders until it finds one that generates a valid response to the <code>pending_message</code> (as determined by the <code>valid()</code> method). Once a valid response is found, <code>step()</code> updates the <code>pending_message</code> and <code>pending_sender</code> variables, and on the next iteration, <code>step()</code> re-starts its search for a valid response from the beginning of the list of responders (the exception being that the human user always gets a chance to respond after each non-human valid response). This process repeats until <code>done()</code> returns true, at which point <code>run()</code> returns the value of <code>result()</code>, which is the final result of the task.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent associated with the task</p> required <code>name</code> <code>str</code> <p>name of the task</p> <code>''</code> <code>llm_delegate</code> <code>bool</code> <p>whether to delegate control to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity).</p> <code>False</code> <code>single_round</code> <code>bool</code> <p>If true, task runs until one message by controller, and subsequent response by non-controller. If false, runs for the specified number of turns in <code>run</code>, or until <code>done()</code> is true. One run of step() is considered a \"turn\".</p> <code>False</code> <code>system_message</code> <code>str</code> <p>if not empty, overrides agent's system_message</p> <code>''</code> <code>user_message</code> <code>str</code> <p>if not empty, overrides agent's user_message</p> <code>''</code> <code>restart</code> <code>bool</code> <p>if true, resets the agent's message history</p> <code>False</code> <code>default_human_response</code> <code>str</code> <p>default response from user; useful for testing, to avoid interactive input from user.</p> <code>None</code> <code>interactive</code> <code>bool</code> <p>if true, wait for human input after each non-human response (prevents infinite loop of non-human responses). Default is true. If false, then <code>default_human_response</code> is set to \"\"</p> <code>True</code> <code>only_user_quits_root</code> <code>bool</code> <p>if true, only user can quit the root task.</p> <code>True</code> <code>erase_substeps</code> <code>bool</code> <p>if true, when task completes, erase intermediate conversation with subtasks from this agent's <code>message_history</code>, and also erase all subtask agents' <code>message_history</code>. Note: erasing can reduce prompt sizes, but results in repetitive sub-task delegation.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def __init__(\n    self,\n    agent: Agent,\n    name: str = \"\",\n    llm_delegate: bool = False,\n    single_round: bool = False,\n    system_message: str = \"\",\n    user_message: str | None = \"\",\n    restart: bool = False,\n    default_human_response: Optional[str] = None,\n    interactive: bool = True,\n    only_user_quits_root: bool = True,\n    erase_substeps: bool = False,\n):\n    \"\"\"\n    A task to be performed by an agent.\n\n    Args:\n        agent (Agent): agent associated with the task\n        name (str): name of the task\n        llm_delegate (bool): whether to delegate control to LLM; conceptually,\n            the \"controlling entity\" is the one \"seeking\" responses to its queries,\n            and has a goal it is aiming to achieve. The \"controlling entity\" is\n            either the LLM or the USER. (Note within a Task there is just one\n            LLM, and all other entities are proxies of the \"User\" entity).\n        single_round (bool): If true, task runs until one message by controller,\n            and subsequent response by non-controller. If false, runs for the\n            specified number of turns in `run`, or until `done()` is true.\n            One run of step() is considered a \"turn\".\n        system_message (str): if not empty, overrides agent's system_message\n        user_message (str): if not empty, overrides agent's user_message\n        restart (bool): if true, resets the agent's message history\n        default_human_response (str): default response from user; useful for\n            testing, to avoid interactive input from user.\n        interactive (bool): if true, wait for human input after each non-human\n            response (prevents infinite loop of non-human responses).\n            Default is true. If false, then `default_human_response` is set to \"\"\n        only_user_quits_root (bool): if true, only user can quit the root task.\n        erase_substeps (bool): if true, when task completes, erase intermediate\n            conversation with subtasks from this agent's `message_history`, and also\n            erase all subtask agents' `message_history`.\n            Note: erasing can reduce prompt sizes, but results in repetitive\n            sub-task delegation.\n    \"\"\"\n    if isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\n        agent = cast(ChatAgent, agent)\n        agent.clear_history(0)\n        # possibly change the system and user messages\n        if system_message:\n            # we always have at least 1 task_message\n            agent.set_system_message(system_message)\n        if user_message:\n            agent.set_user_message(user_message)\n\n    self.logger: None | RichFileLogger = None\n    self.tsv_logger: None | logging.Logger = None\n    self.color_log: bool = True\n    self.agent = agent\n    self.name = name or agent.config.name\n    self.default_human_response = default_human_response\n    self.interactive = interactive\n    self.message_history_idx = -1\n    if not interactive:\n        self.default_human_response = \"\"\n    if default_human_response is not None:\n        self.agent.default_human_response = default_human_response\n    self.only_user_quits_root = only_user_quits_root\n    # set to True if we want to collapse multi-turn conversation with sub-tasks into\n    # just the first outgoing message and last incoming message.\n    # Note this also completely erases sub-task agents' message_history.\n    self.erase_substeps = erase_substeps\n\n    agent_entity_responders = agent.entity_responders()\n    agent_entity_responders_async = agent.entity_responders_async()\n    self.responders: List[Responder] = [e for e, _ in agent_entity_responders]\n    self.responders_async: List[Responder] = [\n        e for e, _ in agent_entity_responders_async\n    ]\n    self.non_human_responders: List[Responder] = [\n        r for r in self.responders if r != Entity.USER\n    ]\n    self.non_human_responders_async: List[Responder] = [\n        r for r in self.responders_async if r != Entity.USER\n    ]\n\n    self.human_tried = False  # did human get a chance to respond in last step?\n    self._entity_responder_map: Dict[\n        Entity, Callable[..., Optional[ChatDocument]]\n    ] = dict(agent_entity_responders)\n\n    self._entity_responder_async_map: Dict[\n        Entity, Callable[..., Coroutine[Any, Any, Optional[ChatDocument]]]\n    ] = dict(agent_entity_responders_async)\n\n    self.name_sub_task_map: Dict[str, Task] = {}\n    # latest message in a conversation among entities and agents.\n    self.pending_message: Optional[ChatDocument] = None\n    self.pending_sender: Responder = Entity.USER\n    self.single_round = single_round\n    self.turns = -1  # no limit\n    self.llm_delegate = llm_delegate\n    if llm_delegate:\n        self.controller = Entity.LLM\n        if self.single_round:\n            # 0: User instructs (delegating to LLM);\n            # 1: LLM asks;\n            # 2: user replies.\n            self.turns = 2\n    else:\n        self.controller = Entity.USER\n        if self.single_round:\n            self.turns = 1  # 0: User asks, 1: LLM replies.\n\n    # other sub_tasks this task can delegate to\n    self.sub_tasks: List[Task] = []\n    self.parent_task: Set[Task] = set()\n    self.caller: Task | None = None  # which task called this task's `run` method\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.clone","title":"<code>clone(i)</code>","text":"<p>Returns a copy of this task, with a new agent.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def clone(self, i: int) -&gt; \"Task\":\n    \"\"\"\n    Returns a copy of this task, with a new agent.\n    \"\"\"\n    assert isinstance(self.agent, ChatAgent), \"Task clone only works for ChatAgent\"\n\n    agent_cls = type(self.agent)\n    config_copy = copy.deepcopy(self.agent.config)\n    agent: ChatAgent = agent_cls(config_copy)\n    return Task(\n        agent,\n        name=self.name + f\"-{i}\",\n        llm_delegate=self.llm_delegate,\n        single_round=self.single_round,\n        system_message=self.agent.system_message,\n        user_message=self.agent.user_message,\n        restart=False,\n        default_human_response=self.default_human_response,\n        interactive=self.interactive,\n        only_user_quits_root=self.only_user_quits_root,\n        erase_substeps=self.erase_substeps,\n    )\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.add_sub_task","title":"<code>add_sub_task(task)</code>","text":"<p>Add a sub-task (or list of subtasks) that this task can delegate (or fail-over) to. Note that the sequence of sub-tasks is important, since these are tried in order, as the parent task searches for a valid response.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | List[Task]</code> <p>sub-task(s) to add</p> required Source code in <code>langroid/agent/task.py</code> <pre><code>def add_sub_task(self, task: Task | List[Task]) -&gt; None:\n    \"\"\"\n    Add a sub-task (or list of subtasks) that this task can delegate\n    (or fail-over) to. Note that the sequence of sub-tasks is important,\n    since these are tried in order, as the parent task searches for a valid\n    response.\n\n    Args:\n        task (Task|List[Task]): sub-task(s) to add\n    \"\"\"\n\n    if isinstance(task, list):\n        for t in task:\n            self.add_sub_task(t)\n        return\n    assert isinstance(task, Task), f\"added task must be a Task, not {type(task)}\"\n\n    task.parent_task.add(self)  # add myself to set of parent tasks of `task`\n    self.sub_tasks.append(task)\n    self.name_sub_task_map[task.name] = task\n    self.responders.append(cast(Responder, task))\n    self.responders_async.append(cast(Responder, task))\n    self.non_human_responders.append(cast(Responder, task))\n    self.non_human_responders_async.append(cast(Responder, task))\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.init","title":"<code>init(msg=None)</code>","text":"<p>Initialize the task, with an optional message to start the conversation. Initializes <code>self.pending_message</code> and <code>self.pending_sender</code>. Args:     msg (str|ChatDocument): optional message to start the conversation.</p> <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>the initialized <code>self.pending_message</code>.</p> <code>ChatDocument | None</code> <p>Currently not used in the code, but provided for convenience.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Initialize the task, with an optional message to start the conversation.\n    Initializes `self.pending_message` and `self.pending_sender`.\n    Args:\n        msg (str|ChatDocument): optional message to start the conversation.\n\n    Returns:\n        (ChatDocument|None): the initialized `self.pending_message`.\n        Currently not used in the code, but provided for convenience.\n    \"\"\"\n    self.pending_sender = Entity.USER\n    if isinstance(msg, str):\n        self.pending_message = ChatDocument(\n            content=msg,\n            metadata=ChatDocMetaData(\n                sender=Entity.USER,\n            ),\n        )\n    else:\n        self.pending_message = msg\n        if self.pending_message is not None and self.caller is not None:\n            # msg may have come from `caller`, so we pretend this is from\n            # the CURRENT task's USER entity\n            self.pending_message.metadata.sender = Entity.USER\n\n    self._show_pending_message_if_debug()\n\n    if self.caller is not None and self.caller.logger is not None:\n        self.logger = self.caller.logger\n    else:\n        self.logger = RichFileLogger(f\"logs/{self.name}.log\", color=self.color_log)\n\n    if self.caller is not None and self.caller.tsv_logger is not None:\n        self.tsv_logger = self.caller.tsv_logger\n    else:\n        self.tsv_logger = setup_file_logger(\"tsv_logger\", f\"logs/{self.name}.tsv\")\n        header = ChatDocLoggerFields().tsv_header()\n        self.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\n\n    self.log_message(Entity.USER, self.pending_message)\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.run","title":"<code>run(msg=None, turns=-1, caller=None)</code>","text":"<p>Synchronous version of <code>run_async()</code>. See <code>run_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def run(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n    turns: int = -1,\n    caller: None | Task = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"Synchronous version of `run_async()`.\n    See `run_async()` for details.\"\"\"\n\n    assert (\n        msg is None or isinstance(msg, str) or isinstance(msg, ChatDocument)\n    ), f\"msg arg in Task.run() must be None, str, or ChatDocument, not {type(msg)}\"\n\n    if (\n        isinstance(msg, ChatDocument)\n        and msg.metadata.recipient != \"\"\n        and msg.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n    self._pre_run_loop(\n        msg=msg,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self.step()\n        if self.done():\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        if turns &gt; 0 and i &gt;= turns:\n            break\n\n    final_result = self.result()\n    self._post_run_loop()\n    return final_result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.run_async","title":"<code>run_async(msg=None, turns=-1, caller=None)</code>  <code>async</code>","text":"<p>Loop over <code>step()</code> until task is considered done or <code>turns</code> is reached. Runs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>initial message to process; if None, the LLM will respond to its initial <code>self.task_messages</code> which set up and kick off the overall task. The agent tries to achieve this goal by looping over <code>self.step()</code> until the task is considered done; this can involve a series of messages produced by Agent, LLM or Human (User).</p> <code>None</code> <code>turns</code> <code>int</code> <p>number of turns to run the task for; default is -1, which means run until task is done.</p> <code>-1</code> <code>caller</code> <code>Task | None</code> <p>the calling task, if any</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: valid result of the task.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def run_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n    turns: int = -1,\n    caller: None | Task = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Loop over `step()` until task is considered done or `turns` is reached.\n    Runs asynchronously.\n\n    Args:\n        msg (str|ChatDocument): initial message to process; if None,\n            the LLM will respond to its initial `self.task_messages`\n            which set up and kick off the overall task.\n            The agent tries to achieve this goal by looping\n            over `self.step()` until the task is considered\n            done; this can involve a series of messages produced by Agent,\n            LLM or Human (User).\n        turns (int): number of turns to run the task for;\n            default is -1, which means run until task is done.\n        caller (Task|None): the calling task, if any\n\n    Returns:\n        Optional[ChatDocument]: valid result of the task.\n    \"\"\"\n\n    # Even if the initial \"sender\" is not literally the USER (since the task could\n    # have come from another LLM), as far as this agent is concerned, the initial\n    # message can be considered to be from the USER\n    # (from the POV of this agent's LLM).\n\n    if (\n        isinstance(msg, ChatDocument)\n        and msg.metadata.recipient != \"\"\n        and msg.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n    self._pre_run_loop(\n        msg=msg,\n        caller=caller,\n        is_async=True,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        await self.step_async()\n        if self.done():\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        if turns &gt; 0 and i &gt;= turns:\n            break\n\n    final_result = self.result()\n    self._post_run_loop()\n    return final_result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.step","title":"<code>step(turns=-1)</code>","text":"<p>Synchronous version of <code>step_async()</code>. See <code>step_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def step(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    Synchronous version of `step_async()`. See `step_async()` for details.\n    \"\"\"\n    result = None\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self.pending_message = error_doc\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders.copy()\n    if Entity.USER in self.responders and not self.human_tried:\n        # give human first chance if they haven't been tried in last step:\n        # ensures human gets chance at each turn.\n        responders.insert(0, Entity.USER)\n\n    for r in responders:\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                function_call=None,\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = self.response(r, turns)\n        is_break = self._process_responder_result(r, parent, result)\n        if is_break:\n            break\n    if not self.valid(result):\n        self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.step_async","title":"<code>step_async(turns=-1)</code>  <code>async</code>","text":"<p>A single \"turn\" in the task conversation: The \"allowed\" responders in this turn (which can be either the 3 \"entities\", or one of the sub-tasks) are tried in sequence, until a valid response is obtained; a valid response is one that contributes to the task, either by ending it, or producing a response to be further acted on. Update <code>self.pending_message</code> to the latest valid response (or NO_ANSWER if no valid response was obtained from any responder).</p> <p>Parameters:</p> Name Type Description Default <code>turns</code> <code>int</code> <p>number of turns to process. Typically used in testing where there is no human to \"quit out\" of current level, or in cases where we want to limit the number of turns of a delegated agent.</p> <code>-1</code> <p>Returns (ChatDocument|None):     Updated <code>self.pending_message</code>. Currently the return value is not used         by the <code>task.run()</code> method, but we return this as a convenience for         other use-cases, e.g. where we want to run a task step by step in a         different context.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def step_async(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    A single \"turn\" in the task conversation: The \"allowed\" responders in this\n    turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n    tried in sequence, until a _valid_ response is obtained; a _valid_\n    response is one that contributes to the task, either by ending it,\n    or producing a response to be further acted on.\n    Update `self.pending_message` to the latest valid response (or NO_ANSWER\n    if no valid response was obtained from any responder).\n\n    Args:\n        turns (int): number of turns to process. Typically used in testing\n            where there is no human to \"quit out\" of current level, or in cases\n            where we want to limit the number of turns of a delegated agent.\n\n    Returns (ChatDocument|None):\n        Updated `self.pending_message`. Currently the return value is not used\n            by the `task.run()` method, but we return this as a convenience for\n            other use-cases, e.g. where we want to run a task step by step in a\n            different context.\n    \"\"\"\n    result = None\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self.pending_message = error_doc\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders_async.copy()\n    if Entity.USER in self.responders_async and not self.human_tried:\n        # give human first chance if they haven't been tried in last step:\n        # ensures human gets chance at each turn.\n        responders.insert(0, Entity.USER)\n\n    for r in responders:\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                function_call=None,\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = await self.response_async(r, turns)\n        is_break = self._process_responder_result(r, parent, result)\n        if is_break:\n            break\n    if not self.valid(result):\n        self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.response","title":"<code>response(e, turns=-1)</code>","text":"<p>Sync version of <code>response_async()</code>. See <code>response_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def response(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Sync version of `response_async()`. See `response_async()` for details.\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        result = e.run(\n            self.pending_message,\n            turns=actual_turns,\n            caller=self,\n        )\n        return result\n    else:\n        # Note we always use async responders, even though\n        # ultimately a synch endpoint is used.\n        response_fn = self._entity_responder_map[cast(Entity, e)]\n        result = response_fn(self.pending_message)\n        return result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.response_async","title":"<code>response_async(e, turns=-1)</code>  <code>async</code>","text":"<p>Get response to <code>self.pending_message</code> from a responder. If response is valid (i.e. it ends the current turn of seeking responses):     -then return the response as a ChatDocument object,     -otherwise return None. Args:     e (Responder): responder to get response from.     turns (int): number of turns to run the task for.         Default is -1, which means run until task is done.</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: response to <code>self.pending_message</code> from entity if</p> <code>Optional[ChatDocument]</code> <p>valid, None otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def response_async(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get response to `self.pending_message` from a responder.\n    If response is __valid__ (i.e. it ends the current turn of seeking\n    responses):\n        -then return the response as a ChatDocument object,\n        -otherwise return None.\n    Args:\n        e (Responder): responder to get response from.\n        turns (int): number of turns to run the task for.\n            Default is -1, which means run until task is done.\n\n    Returns:\n        Optional[ChatDocument]: response to `self.pending_message` from entity if\n        valid, None otherwise\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        result = await e.run_async(\n            self.pending_message,\n            turns=actual_turns,\n            caller=self,\n        )\n        return result\n    else:\n        response_fn = self._entity_responder_async_map[cast(Entity, e)]\n        result = await response_fn(self.pending_message)\n        return result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.result","title":"<code>result()</code>","text":"<p>Get result of task. This is the default behavior. Derived classes can override this. Returns:     ChatDocument: result of task</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def result(self) -&gt; ChatDocument:\n    \"\"\"\n    Get result of task. This is the default behavior.\n    Derived classes can override this.\n    Returns:\n        ChatDocument: result of task\n    \"\"\"\n    result_msg = self.pending_message\n\n    content = result_msg.content if result_msg else \"\"\n    if DONE in content:\n        # assuming it is of the form \"DONE: &lt;content&gt;\"\n        content = content.replace(DONE, \"\").strip()\n    fun_call = result_msg.function_call if result_msg else None\n    attachment = result_msg.attachment if result_msg else None\n    block = result_msg.metadata.block if result_msg else None\n    recipient = result_msg.metadata.recipient if result_msg else None\n    responder = result_msg.metadata.parent_responder if result_msg else None\n\n    # regardless of which entity actually produced the result,\n    # when we return the result, we set entity to USER\n    # since to the \"parent\" task, this result is equivalent to a response from USER\n    return ChatDocument(\n        content=content,\n        function_call=fun_call,\n        attachment=attachment,\n        metadata=ChatDocMetaData(\n            source=Entity.USER,\n            sender=Entity.USER,\n            block=block,\n            parent_responder=responder,\n            sender_name=self.name,\n            recipient=recipient,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.done","title":"<code>done()</code>","text":"<p>Check if task is done. This is the default behavior. Derived classes can override this. Returns:     bool: True if task is done, False otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def done(self) -&gt; bool:\n    \"\"\"\n    Check if task is done. This is the default behavior.\n    Derived classes can override this.\n    Returns:\n        bool: True if task is done, False otherwise\n    \"\"\"\n    user_quit = (\n        self.pending_message is not None\n        and self.pending_message.content in USER_QUIT\n        and self.pending_message.metadata.sender == Entity.USER\n    )\n    if self._level == 0 and self.only_user_quits_root:\n        # for top-level task, only user can quit out\n        return user_quit\n\n    return (\n        # no valid response from any entity/agent in current turn\n        self.pending_message is None\n        # LLM decided task is done\n        or DONE in self.pending_message.content\n        or (  # current task is addressing message to caller task\n            self.caller is not None\n            and self.caller.name != \"\"\n            and self.pending_message.metadata.recipient == self.caller.name\n        )\n        or (\n            # Task controller is \"stuck\", has nothing to say\n            NO_ANSWER in self.pending_message.content\n            and self.pending_message.metadata.sender == self.controller\n        )\n        or user_quit\n    )\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.valid","title":"<code>valid(result)</code>","text":"<p>Is the result from an entity or sub-task such that we can stop searching for responses for this turn?</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def valid(self, result: Optional[ChatDocument]) -&gt; bool:\n    \"\"\"\n    Is the result from an entity or sub-task such that we can stop searching\n    for responses for this turn?\n    \"\"\"\n    # TODO caution we should ensure that no handler method (tool) returns simply\n    # an empty string (e.g when showing contents of an empty file), since that\n    # would be considered an invalid response, and other responders will wrongly\n    # be given a chance to respond.\n    return (\n        result is not None\n        and (result.content != \"\" or result.function_call is not None)\n        and (  # if NO_ANSWER is from controller, then it means\n            # controller is stuck and we are done with task loop\n            NO_ANSWER not in result.content\n            or result.metadata.sender == self.controller\n        )\n    )\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.log_message","title":"<code>log_message(resp, msg=None, mark=False)</code>","text":"<p>Log current pending message, and related state, for lineage/debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>resp</code> <code>Responder</code> <p>Responder that generated the <code>msg</code></p> required <code>msg</code> <code>ChatDocument</code> <p>Message to log. Defaults to None.</p> <code>None</code> <code>mark</code> <code>bool</code> <p>Whether to mark the message as the final result of a <code>task.step()</code> call. Defaults to False.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def log_message(\n    self,\n    resp: Responder,\n    msg: ChatDocument | None = None,\n    mark: bool = False,\n) -&gt; None:\n    \"\"\"\n    Log current pending message, and related state, for lineage/debugging purposes.\n\n    Args:\n        resp (Responder): Responder that generated the `msg`\n        msg (ChatDocument, optional): Message to log. Defaults to None.\n        mark (bool, optional): Whether to mark the message as the final result of\n            a `task.step()` call. Defaults to False.\n    \"\"\"\n    default_values = ChatDocLoggerFields().dict().values()\n    msg_str_tsv = \"\\t\".join(str(v) for v in default_values)\n    if msg is not None:\n        msg_str_tsv = msg.tsv_str()\n\n    mark_str = \"*\" if mark else \" \"\n    task_name = self.name if self.name != \"\" else \"root\"\n    resp_color = \"white\" if mark else \"red\"\n    resp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\n\n    if msg is None:\n        msg_str = f\"{mark_str}({task_name}) {resp_str}\"\n    else:\n        color = {\n            Entity.LLM: \"green\",\n            Entity.USER: \"blue\",\n            Entity.AGENT: \"red\",\n            Entity.SYSTEM: \"magenta\",\n        }[msg.metadata.sender]\n        f = msg.log_fields()\n        tool_type = f.tool_type.rjust(6)\n        tool_name = f.tool.rjust(10)\n        tool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\n        sender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\n        sender_name = f.sender_name.rjust(10)\n        recipient = \"=&gt;\" + str(f.recipient).rjust(10)\n        block = \"X \" + str(f.block or \"\").rjust(10)\n        content = f\"[{color}]{f.content}[/{color}]\"\n        msg_str = (\n            f\"{mark_str}({task_name}) \"\n            f\"{resp_str} {sender}({sender_name}) \"\n            f\"({recipient}) ({block}) {tool_str} {content}\"\n        )\n\n    if self.logger is not None:\n        self.logger.log(msg_str)\n    if self.tsv_logger is not None:\n        resp_str = str(resp)\n        self.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.set_color_log","title":"<code>set_color_log(enable=True)</code>","text":"<p>Flag to enable/disable color logging using rich.console. In some contexts, such as Colab notebooks, we may want to disable color logging using rich.console, since those logs show up in the cell output rather than in the log file. Turning off this feature will still create logs, but without the color formatting from rich.console Args:     enable (bool): value of <code>self.color_log</code> to set to,         which will enable/diable rich logging</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def set_color_log(self, enable: bool = True) -&gt; None:\n    \"\"\"\n    Flag to enable/disable color logging using rich.console.\n    In some contexts, such as Colab notebooks, we may want to disable color logging\n    using rich.console, since those logs show up in the cell output rather than\n    in the log file. Turning off this feature will still create logs, but without\n    the color formatting from rich.console\n    Args:\n        enable (bool): value of `self.color_log` to set to,\n            which will enable/diable rich logging\n\n    \"\"\"\n    self.color_log = enable\n</code></pre>"},{"location":"reference/agent/tool_message/","title":"tool_message","text":"<p>langroid/agent/tool_message.py </p> <p>Structured messages to an agent, typically from an LLM, to be handled by an agent. The messages could represent, for example: - information or data given to the agent - request for information or data from the agent - request to run a method of the agent</p>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>             Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract Class for a class that defines the structure of a \"Tool\" message from an LLM. Depending on context, \"tools\" are also referred to as \"plugins\", or \"function calls\" (in the context of OpenAI LLMs). Essentially, they are a way for the LLM to express its intent to run a special function or method. Currently these \"tools\" are handled by methods of the agent.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>str</code> <p>name of agent method to map to.</p> <code>purpose</code> <code>str</code> <p>purpose of agent method, expressed in general terms. (This is used when auto-generating the tool instruction to the LLM)</p> <code>result</code> <code>str</code> <p>example of result of agent method.</p>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Examples to use in few-shot demos with JSON formatting instructions. Returns:</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[\"ToolMessage\"]:\n    \"\"\"\n    Examples to use in few-shot demos with JSON formatting instructions.\n    Returns:\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.usage_example","title":"<code>usage_example()</code>  <code>classmethod</code>","text":"<p>Instruction to the LLM showing an example of how to use the message. Returns:     str: example of how to use the message</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef usage_example(cls) -&gt; str:\n    \"\"\"\n    Instruction to the LLM showing an example of how to use the message.\n    Returns:\n        str: example of how to use the message\n    \"\"\"\n    # pick a random example of the fields\n    if len(cls.examples()) == 0:\n        return \"\"\n    ex = choice(cls.examples())\n    return ex.json_example()\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.default_value","title":"<code>default_value(f)</code>  <code>classmethod</code>","text":"<p>Returns the default value of the given field, for the message-class Args:     f (str): field name</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>default value of the field, or None if not set or if the field does not exist.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n    \"\"\"\n    Returns the default value of the given field, for the message-class\n    Args:\n        f (str): field name\n\n    Returns:\n        Any: default value of the field, or None if not set or if the\n            field does not exist.\n    \"\"\"\n    schema = cls.schema()\n    properties = schema[\"properties\"]\n    return properties.get(f, {}).get(\"default\", None)\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.llm_function_schema","title":"<code>llm_function_schema(request=False)</code>  <code>classmethod</code>","text":"<p>Clean up the schema of the Pydantic class (which can recursively contain other Pydantic classes), to create a version compatible with OpenAI Function-call API.</p> <p>Adapted from this excellent library: https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>bool</code> <p>whether to include the \"request\" field in the schema. (we set this to True when using Langroid-native TOOLs as opposed to OpenAI Function calls)</p> <code>False</code> <p>Returns:</p> Name Type Description <code>LLMFunctionSpec</code> <code>LLMFunctionSpec</code> <p>the schema as an LLMFunctionSpec</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef llm_function_schema(cls, request: bool = False) -&gt; LLMFunctionSpec:\n    \"\"\"\n    Clean up the schema of the Pydantic class (which can recursively contain\n    other Pydantic classes), to create a version compatible with OpenAI\n    Function-call API.\n\n    Adapted from this excellent library:\n    https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py\n\n    Args:\n        request: whether to include the \"request\" field in the schema.\n            (we set this to True when using Langroid-native TOOLs as opposed to\n            OpenAI Function calls)\n\n    Returns:\n        LLMFunctionSpec: the schema as an LLMFunctionSpec\n\n    \"\"\"\n    schema = cls.schema()\n    docstring = parse(cls.__doc__ or \"\")\n    parameters = {\n        k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n    }\n    for param in docstring.params:\n        if (name := param.arg_name) in parameters[\"properties\"] and (\n            description := param.description\n        ):\n            if \"description\" not in parameters[\"properties\"][name]:\n                parameters[\"properties\"][name][\"description\"] = description\n\n    excludes = (\n        [\"result\", \"purpose\"] if request else [\"request\", \"result\", \"purpose\"]\n    )\n    # exclude 'excludes' from parameters[\"properties\"]:\n    parameters[\"properties\"] = {\n        field: details\n        for field, details in parameters[\"properties\"].items()\n        if field not in excludes\n    }\n    parameters[\"required\"] = sorted(\n        k\n        for k, v in parameters[\"properties\"].items()\n        if (\"default\" not in v and k not in excludes)\n    )\n    if request:\n        parameters[\"required\"].append(\"request\")\n\n    if \"description\" not in schema:\n        if docstring.short_description:\n            schema[\"description\"] = docstring.short_description\n        else:\n            schema[\"description\"] = (\n                f\"Correctly extracted `{cls.__name__}` with all \"\n                f\"the required parameters with correct types\"\n            )\n\n    parameters.pop(\"exclude\")\n    _recursive_purge_dict_key(parameters, \"title\")\n    _recursive_purge_dict_key(parameters, \"additionalProperties\")\n    return LLMFunctionSpec(\n        name=cls.default_value(\"request\"),\n        description=cls.default_value(\"purpose\"),\n        parameters=parameters,\n    )\n</code></pre>"},{"location":"reference/agent/special/","title":"special","text":"<p>langroid/agent/special/init.py </p>"},{"location":"reference/agent/special/doc_chat_agent/","title":"doc_chat_agent","text":"<p>langroid/agent/special/doc_chat_agent.py </p> <p>Agent that supports asking queries about a set of documents, using retrieval-augmented generation (RAG).</p> <p>Functionality includes: - summarizing a document, with a custom instruction; see <code>summarize_docs</code> - asking a question about a document; see <code>answer_from_docs</code></p> <p>Note: to use the sentence-transformer embeddings, you must install langroid with the [hf-embeddings] extra, e.g.:</p> <p>pip install \"langroid[hf-embeddings]\"</p>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgentConfig","title":"<code>DocChatAgentConfig</code>","text":"<p>             Bases: <code>ChatAgentConfig</code></p> <p>Attributes:</p> Name Type Description <code>max_context_tokens</code> <code>int</code> <p>threshold to use for various steps, e.g. if we are able to fit the current stage of doc processing into this many tokens, we skip additional compression steps, and use the current docs as-is in the context</p> <code>conversation_mode</code> <code>bool</code> <p>if True, we will accumulate message history, and pass entire history to LLM at each round. If False, each request to LLM will consist only of the initial task messages plus the current query.</p>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent","title":"<code>DocChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: DocChatAgentConfig,\n):\n    super().__init__(config)\n    self.config: DocChatAgentConfig = config\n    self.original_docs: None | List[Document] = None\n    self.original_docs_length = 0\n    self.chunked_docs: None | List[Document] = None\n    self.chunked_docs_clean: None | List[Document] = None\n    self.response: None | Document = None\n    if len(config.doc_paths) &gt; 0:\n        self.ingest()\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest","title":"<code>ingest()</code>","text":"<p>Chunk + embed + store docs specified by self.config.doc_paths</p> <p>Returns:</p> Type Description <code>None</code> <p>dict with keys: n_splits: number of splits urls: list of urls paths: list of file paths</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest(self) -&gt; None:\n    \"\"\"\n    Chunk + embed + store docs specified by self.config.doc_paths\n\n    Returns:\n        dict with keys:\n            n_splits: number of splits\n            urls: list of urls\n            paths: list of file paths\n    \"\"\"\n    if len(self.config.doc_paths) == 0:\n        # we must be using a previously defined collection\n        # But let's get all the chunked docs so we can\n        # do keyword and other non-vector searches\n        if self.vecdb is None:\n            raise ValueError(\"VecDB not set\")\n        self.chunked_docs = self.vecdb.get_all_documents()\n        # used for lexical similarity e.g. keyword search (bm25 etc)\n        self.chunked_docs_clean = [\n            Document(content=preprocess_text(d.content), metadata=d.metadata)\n            for d in self.chunked_docs\n        ]\n        return\n    urls, paths = get_urls_and_paths(self.config.doc_paths)\n    docs: List[Document] = []\n    parser = Parser(self.config.parsing)\n    if len(urls) &gt; 0:\n        loader = URLLoader(urls=urls, parser=parser)\n        docs = loader.load()\n    if len(paths) &gt; 0:\n        for p in paths:\n            path_docs = RepoLoader.get_documents(p, parser=parser)\n            docs.extend(path_docs)\n    n_docs = len(docs)\n    n_splits = self.ingest_docs(docs)\n    if n_docs == 0:\n        return\n    n_urls = len(urls)\n    n_paths = len(paths)\n    print(\n        f\"\"\"\n    [green]I have processed the following {n_urls} URLs \n    and {n_paths} paths into {n_splits} parts:\n    \"\"\".strip()\n    )\n    print(\"\\n\".join(urls))\n    print(\"\\n\".join(paths))\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest_docs","title":"<code>ingest_docs(docs)</code>","text":"<p>Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_docs(self, docs: List[Document]) -&gt; int:\n    \"\"\"\n    Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db\n    \"\"\"\n    self.original_docs = docs\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    docs = self.parser.split(docs)\n    self.chunked_docs = docs\n    self.chunked_docs_clean = [\n        Document(content=preprocess_text(d.content), metadata=d.metadata)\n        for d in self.chunked_docs\n    ]\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    # add embeddings in batches, to stay under limit of embeddings API\n    batches = list(batched(docs, self.config.embed_batch_size))\n    for batch in batches:\n        self.vecdb.add_documents(batch)\n    self.original_docs_length = self.doc_length(docs)\n    return len(docs)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_length","title":"<code>doc_length(docs)</code>","text":"<p>Calc token-length of a list of docs Args:     docs: list of Document objects Returns:     int: number of tokens</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def doc_length(self, docs: List[Document]) -&gt; int:\n    \"\"\"\n    Calc token-length of a list of docs\n    Args:\n        docs: list of Document objects\n    Returns:\n        int: number of tokens\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    return self.parser.num_tokens(self.doc_string(docs))\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.user_docs_ingest_dialog","title":"<code>user_docs_ingest_dialog()</code>","text":"<p>Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def user_docs_ingest_dialog(self) -&gt; None:\n    \"\"\"\n    Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.\n    \"\"\"\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    n_deletes = self.vecdb.clear_empty_collections()\n    collections = self.vecdb.list_collections()\n    collection_name = \"NEW\"\n    is_new_collection = False\n    replace_collection = False\n    if len(collections) &gt; 0:\n        n = len(collections)\n        delete_str = (\n            f\"(deleted {n_deletes} empty collections)\" if n_deletes &gt; 0 else \"\"\n        )\n        print(f\"Found {n} collections: {delete_str}\")\n        for i, option in enumerate(collections, start=1):\n            print(f\"{i}. {option}\")\n        while True:\n            choice = Prompt.ask(\n                f\"Enter 1-{n} to select a collection, \"\n                \"or hit ENTER to create a NEW collection, \"\n                \"or -1 to DELETE ALL COLLECTIONS\",\n                default=\"0\",\n            )\n            try:\n                if -1 &lt;= int(choice) &lt;= n:\n                    break\n            except Exception:\n                pass\n\n        if choice == \"-1\":\n            confirm = Prompt.ask(\n                \"Are you sure you want to delete all collections?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            if confirm == \"y\":\n                self.vecdb.clear_all_collections(really=True)\n                collection_name = \"NEW\"\n\n        if int(choice) &gt; 0:\n            collection_name = collections[int(choice) - 1]\n            print(f\"Using collection {collection_name}\")\n            choice = Prompt.ask(\n                \"Would you like to replace this collection?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            replace_collection = choice == \"y\"\n\n    if collection_name == \"NEW\":\n        is_new_collection = True\n        collection_name = Prompt.ask(\n            \"What would you like to name the NEW collection?\",\n            default=\"doc-chat-2\",\n        )\n\n    self.vecdb.set_collection(collection_name, replace=replace_collection)\n\n    default_urls_str = (\n        \" (or leave empty for default URLs)\" if is_new_collection else \"\"\n    )\n    print(f\"[blue]Enter some URLs or file/dir paths below {default_urls_str}\")\n    inputs = get_list_from_user()\n    if len(inputs) == 0:\n        if is_new_collection:\n            inputs = self.config.default_paths\n    self.config.doc_paths = inputs\n    self.ingest()\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_string","title":"<code>doc_string(docs)</code>  <code>staticmethod</code>","text":"<p>Generate a string representation of a list of docs. Args:     docs: list of Document objects Returns:     str: string representation</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef doc_string(docs: List[Document]) -&gt; str:\n    \"\"\"\n    Generate a string representation of a list of docs.\n    Args:\n        docs: list of Document objects\n    Returns:\n        str: string representation\n    \"\"\"\n    contents = [f\"Extract: {d.content}\" for d in docs]\n    sources = [d.metadata.source for d in docs]\n    sources = [f\"Source: {s}\" if s is not None else \"\" for s in sources]\n    return \"\\n\".join(\n        [\n            f\"\"\"\n            {content}\n            {source}\n            \"\"\"\n            for (content, source) in zip(contents, sources)\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible Args:     question: question to answer     passages: list of <code>Document</code> objects each containing a possibly relevant         snippet, and metadata Returns:     a <code>Document</code> object containing the answer,     and metadata containing source citations</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_summary_answer(self, question: str, passages: List[Document]) -&gt; Document:\n    \"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n\n    \"\"\"\n\n    passages_str = self.doc_string(passages)\n    # Substitute Q and P into the templatized prompt\n\n    final_prompt = self.config.summarize_prompt.format(\n        question=f\"Question:{question}\", extracts=passages_str\n    )\n    show_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n\n    # Generate the final verbatim extract based on the final prompt.\n    # Note this will send entire message history, plus this final_prompt\n    # to the LLM, and self.message_history will be updated to include\n    # 2 new LLMMessage objects:\n    # one for `final_prompt`, and one for the LLM response\n\n    # TODO need to \"forget\" last two messages in message_history\n    # if we are not in conversation mode\n\n    if self.config.conversation_mode:\n        # respond with temporary context\n        answer_doc = super()._llm_response_temp_context(question, final_prompt)\n    else:\n        answer_doc = super().llm_response_forget(final_prompt)\n\n    final_answer = answer_doc.content.strip()\n    show_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\n    parts = final_answer.split(\"SOURCE:\", maxsplit=1)\n    if len(parts) &gt; 1:\n        content = parts[0].strip()\n        sources = parts[1].strip()\n    else:\n        content = final_answer\n        sources = \"\"\n    return Document(\n        content=content,\n        metadata=DocMetaData(\n            source=\"SOURCE: \" + sources,\n            sender=Entity.LLM,\n            cached=getattr(answer_doc.metadata, \"cached\", False),\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.rerank_with_diversity","title":"<code>rerank_with_diversity(passages)</code>","text":"<p>Rerank a list of items in such a way that each successive item is least similar (on average) to the earlier items.</p> <p>Args: query (str): The query for which the passages are relevant. passages (List[Document]): A list of Documents to be reranked.</p> <p>Returns: List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_with_diversity(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank a list of items in such a way that each successive item is least similar\n    (on average) to the earlier items.\n\n    Args:\n    query (str): The query for which the passages are relevant.\n    passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n    List[Documents]: A reranked list of Documents.\n    \"\"\"\n\n    if self.vecdb is None:\n        logger.warning(\"No vecdb; cannot use rerank_with_diversity\")\n        return passages\n    emb_model = self.vecdb.embedding_model\n    emb_fn = emb_model.embedding_fn()\n    embs = emb_fn([p.content for p in passages])\n    embs_arr = [np.array(e) for e in embs]\n    indices = list(range(len(passages)))\n\n    # Helper function to compute average similarity to\n    # items in the current result list.\n    def avg_similarity_to_result(i: int, result: List[int]) -&gt; float:\n        return sum(  # type: ignore\n            (embs_arr[i] @ embs_arr[j])\n            / (np.linalg.norm(embs_arr[i]) * np.linalg.norm(embs_arr[j]))\n            for j in result\n        ) / len(result)\n\n    # copy passages to items\n    result = [indices.pop(0)]  # Start with the first item.\n\n    while indices:\n        # Find the item that has the least average similarity\n        # to items in the result list.\n        least_similar_item = min(\n            indices, key=lambda i: avg_similarity_to_result(i, result)\n        )\n        result.append(least_similar_item)\n        indices.remove(least_similar_item)\n\n    # return passages in order of result list\n    return [passages[i] for i in result]\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.rerank_to_periphery","title":"<code>rerank_to_periphery(passages)</code>","text":"<p>Rerank to avoid Lost In the Middle (LIM) problem, where LLMs pay more attention to items at the ends of a list, rather than the middle. So we re-rank to make the best passages appear at the periphery of the list. https://arxiv.org/abs/2307.03172</p> <p>Example reranking: 1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2</p> <p>Parameters:</p> Name Type Description Default <code>passages</code> <code>List[Document]</code> <p>A list of Documents to be reranked.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_to_periphery(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank to avoid Lost In the Middle (LIM) problem,\n    where LLMs pay more attention to items at the ends of a list,\n    rather than the middle. So we re-rank to make the best passages\n    appear at the periphery of the list.\n    https://arxiv.org/abs/2307.03172\n\n    Example reranking:\n    1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2\n\n    Args:\n        passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n        List[Documents]: A reranked list of Documents.\n\n    \"\"\"\n    # Splitting items into odds and evens based on index, not value\n    odds = passages[::2]\n    evens = passages[1::2][::-1]\n\n    # Merging them back together\n    return odds + evens\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.add_context_window","title":"<code>add_context_window(docs_scores)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. These window_ids may overlap, so we - gather connected-components of overlapping windows, - split each component into roughly equal parts, - create a new document for each part, preserving metadata,</p> <p>We may have stored a longer set of window_ids than we need. We just want <code>self.config.n_neighbor_chunks</code> on each side of the center of window_ids.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def add_context_window(\n    self,\n    docs_scores: List[Tuple[Document, float]],\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk.\n    These window_ids may overlap, so we\n    - gather connected-components of overlapping windows,\n    - split each component into roughly equal parts,\n    - create a new document for each part, preserving metadata,\n\n    We may have stored a longer set of window_ids than we need.\n    We just want `self.config.n_neighbor_chunks` on each side\n    of the center of window_ids.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    if self.vecdb is None or self.config.n_neighbor_chunks == 0:\n        return docs_scores\n    return self.vecdb.add_context_window(docs_scores, self.config.n_neighbor_chunks)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_relevant_chunks","title":"<code>get_relevant_chunks(query, query_proxies=[])</code>","text":"<p>The retrieval stage in RAG: get doc-chunks that are most \"relevant\" to the query (and possibly any proxy queries), from the document-store, which currently is the vector store, but in theory could be any document store, or even web-search. This stage does NOT involve an LLM, and the retrieved chunks could either be pre-chunked text (from the initial pre-processing stage where chunks were stored in the vector store), or they could be dynamically retrieved based on a window around a lexical match.</p> <p>These are the steps (some optional based on config): - semantic search based on vector-embedding distance, from vecdb - lexical search using bm25-ranking (keyword similarity) - fuzzy matching (keyword similarity) - re-ranking of doc-chunks by relevance to query, using cross-encoder,    and pick top k</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>original query (assumed to be in stand-alone form)</p> required <code>query_proxies</code> <code>List[str]</code> <p>possible rephrases, or hypothetical answer to query     (e.g. for HyDE-type retrieval)</p> <code>[]</code> <p>Returns:</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_relevant_chunks(\n    self, query: str, query_proxies: List[str] = []\n) -&gt; List[Document]:\n    \"\"\"\n    The retrieval stage in RAG: get doc-chunks that are most \"relevant\"\n    to the query (and possibly any proxy queries), from the document-store,\n    which currently is the vector store,\n    but in theory could be any document store, or even web-search.\n    This stage does NOT involve an LLM, and the retrieved chunks\n    could either be pre-chunked text (from the initial pre-processing stage\n    where chunks were stored in the vector store), or they could be\n    dynamically retrieved based on a window around a lexical match.\n\n    These are the steps (some optional based on config):\n    - semantic search based on vector-embedding distance, from vecdb\n    - lexical search using bm25-ranking (keyword similarity)\n    - fuzzy matching (keyword similarity)\n    - re-ranking of doc-chunks by relevance to query, using cross-encoder,\n       and pick top k\n\n    Args:\n        query: original query (assumed to be in stand-alone form)\n        query_proxies: possible rephrases, or hypothetical answer to query\n                (e.g. for HyDE-type retrieval)\n\n    Returns:\n\n    \"\"\"\n    # if we are using cross-encoder reranking, we can retrieve more docs\n    # during retrieval, and leave it to the cross-encoder re-ranking\n    # to whittle down to self.config.parsing.n_similar_docs\n    retrieval_multiple = 1 if self.config.cross_encoder_reranking_model == \"\" else 3\n\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n\n    with console.status(\"[cyan]Searching VecDB for relevant doc passages...\"):\n        docs_and_scores: List[Tuple[Document, float]] = []\n        for q in [query] + query_proxies:\n            docs_and_scores += self.vecdb.similar_texts_with_scores(\n                q,\n                k=self.config.parsing.n_similar_docs * retrieval_multiple,\n            )\n    # keep only docs with unique d.id()\n    id2doc_score = {d.id(): (d, s) for d, s in docs_and_scores}\n    docs_and_scores = list(id2doc_score.values())\n\n    passages = [\n        Document(content=d.content, metadata=d.metadata)\n        for (d, _) in docs_and_scores\n    ]\n\n    if self.config.use_bm25_search:\n        docs_scores = self.get_similar_chunks_bm25(query, retrieval_multiple)\n        passages += [d for (d, _) in docs_scores]\n\n    if self.config.use_fuzzy_match:\n        fuzzy_match_docs = self.get_fuzzy_matches(query, retrieval_multiple)\n        passages += fuzzy_match_docs\n\n    # keep unique passages\n    id2passage = {p.id(): p for p in passages}\n    passages = list(id2passage.values())\n\n    if len(passages) == 0:\n        return []\n\n    passages_scores = [(p, 0.0) for p in passages]\n    passages_scores = self.add_context_window(passages_scores)\n    passages = [p for p, _ in passages_scores]\n    # now passages can potentially have a lot of doc chunks,\n    # so we re-rank them using a cross-encoder scoring model,\n    # and pick top k where k = config.parsing.n_similar_docs\n    # https://www.sbert.net/examples/applications/retrieve_rerank\n    if self.config.cross_encoder_reranking_model != \"\":\n        passages = self.rerank_with_cross_encoder(query, passages)\n\n    if self.config.rerank_diversity:\n        # reorder to increase diversity among top docs\n        passages = self.rerank_with_diversity(passages)\n\n    if self.config.rerank_periphery:\n        # reorder so most important docs are at periphery\n        # (see Lost In the Middle issue).\n        passages = self.rerank_to_periphery(passages)\n\n    return passages\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_relevant_extracts","title":"<code>get_relevant_extracts(query)</code>","text":"<p>Get list of (verbatim) extracts from doc-chunks relevant to answering a query.</p> <p>These are the stages (some optional based on config): - use LLM to convert query to stand-alone query - optionally use LLM to rephrase query to use below - optionally use LLM to generate hypothetical answer (HyDE) to use below. - get_relevant_chunks(): get doc-chunks relevant to query and proxies - use LLM to get relevant extracts from doc-chunks</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to search for</p> required <p>Returns:</p> Name Type Description <code>query</code> <code>str</code> <p>stand-alone version of input query</p> <code>List[Document]</code> <p>List[Document]: list of relevant extracts</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@no_type_check\ndef get_relevant_extracts(self, query: str) -&gt; Tuple[str, List[Document]]:\n    \"\"\"\n    Get list of (verbatim) extracts from doc-chunks relevant to answering a query.\n\n    These are the stages (some optional based on config):\n    - use LLM to convert query to stand-alone query\n    - optionally use LLM to rephrase query to use below\n    - optionally use LLM to generate hypothetical answer (HyDE) to use below.\n    - get_relevant_chunks(): get doc-chunks relevant to query and proxies\n    - use LLM to get relevant extracts from doc-chunks\n\n    Args:\n        query (str): query to search for\n\n    Returns:\n        query (str): stand-alone version of input query\n        List[Document]: list of relevant extracts\n\n    \"\"\"\n    if len(self.dialog) &gt; 0 and not self.config.assistant_mode:\n        # Regardless of whether we are in conversation mode or not,\n        # for relevant doc/chunk extraction, we must convert the query\n        # to a standalone query to get more relevant results.\n        with console.status(\"[cyan]Converting to stand-alone query...[/cyan]\"):\n            with StreamingIfAllowed(self.llm, False):\n                query = self.llm.followup_to_standalone(self.dialog, query)\n        print(f\"[orange2]New query: {query}\")\n\n    proxies = []\n    if self.config.hypothetical_answer:\n        answer = self.llm_hypothetical_answer(query)\n        proxies = [answer]\n\n    if self.config.n_query_rephrases &gt; 0:\n        rephrases = self.llm_rephrase_query(query)\n        proxies += rephrases\n\n    passages = self.get_relevant_chunks(query, proxies)  # no LLM involved\n\n    if len(passages) == 0:\n        return query, []\n\n    with console.status(\"[cyan]LLM Extracting verbatim passages...\"):\n        with StreamingIfAllowed(self.llm, False):\n            # these are async calls, one per passage; turn off streaming\n            extracts = self.get_verbatim_extracts(query, passages)\n            extracts = [e for e in extracts if e.content != NO_ANSWER]\n\n    return query, extracts\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_verbatim_extracts","title":"<code>get_verbatim_extracts(query, passages)</code>","text":"<p>Run RelevanceExtractorAgent in async/concurrent mode on passages, to extract portions relevant to answering query, from each passage. Args:     query (str): query to answer     passages (List[Documents]): list of passages to extract from</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of Documents containing extracts and metadata.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_verbatim_extracts(\n    self,\n    query: str,\n    passages: List[Document],\n) -&gt; List[Document]:\n    \"\"\"\n    Run RelevanceExtractorAgent in async/concurrent mode on passages,\n    to extract portions relevant to answering query, from each passage.\n    Args:\n        query (str): query to answer\n        passages (List[Documents]): list of passages to extract from\n\n    Returns:\n        List[Document]: list of Documents containing extracts and metadata.\n    \"\"\"\n    agent_cfg = self.config.relevance_extractor_config\n    agent_cfg.query = query\n    agent_cfg.segment_length = 1\n    agent_cfg.llm.stream = False  # disable streaming for concurrent calls\n\n    agent = RelevanceExtractorAgent(agent_cfg)\n    task = Task(\n        agent,\n        name=\"Relevance-Extractor\",\n        default_human_response=\"\",  # eliminate human response\n        only_user_quits_root=False,  # allow agent_response to quit via \"DONE &lt;msg&gt;\"\n    )\n\n    extracts = run_batch_tasks(\n        task,\n        passages,\n        input_map=lambda msg: msg.content,\n        output_map=lambda ans: ans.content if ans is not None else NO_ANSWER,\n    )\n    metadatas = [P.metadata for P in passages]\n    # return with metadata so we can use it downstream, e.g. to cite sources\n    return [\n        Document(content=e, metadata=m)\n        for e, m in zip(extracts, metadatas)\n        if (e != NO_ANSWER and len(e) &gt; 0)\n    ]\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.answer_from_docs","title":"<code>answer_from_docs(query)</code>","text":"<p>Answer query based on relevant docs from the VecDB</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to answer</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>answer</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@no_type_check\ndef answer_from_docs(self, query: str) -&gt; Document:\n    \"\"\"\n    Answer query based on relevant docs from the VecDB\n\n    Args:\n        query (str): query to answer\n\n    Returns:\n        Document: answer\n    \"\"\"\n    response = Document(\n        content=NO_ANSWER,\n        metadata=DocMetaData(\n            source=\"None\",\n        ),\n    )\n    # query may be updated to a stand-alone version\n    query, extracts = self.get_relevant_extracts(query)\n    if len(extracts) == 0:\n        return response\n    with ExitStack() as stack:\n        # conditionally use Streaming or rich console context\n        cm = (\n            StreamingIfAllowed(self.llm)\n            if settings.stream\n            else (console.status(\"LLM Generating final answer...\"))\n        )\n        stack.enter_context(cm)\n        response = self.get_summary_answer(query, extracts)\n\n    self.update_dialog(query, response.content)\n    self.response = response  # save last response\n    return response\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.summarize_docs","title":"<code>summarize_docs(instruction='Give a concise summary of the following text:')</code>","text":"<p>Summarize all docs</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def summarize_docs(\n    self,\n    instruction: str = \"Give a concise summary of the following text:\",\n) -&gt; None | ChatDocument:\n    \"\"\"Summarize all docs\"\"\"\n    if self.llm is None:\n        raise ValueError(\"LLM not set\")\n    if self.original_docs is None:\n        logger.warning(\n            \"\"\"\n            No docs to summarize! Perhaps you are re-using a previously\n            defined collection? \n            In that case, we don't have access to the original docs.\n            To create a summary, use a new collection, and specify a list of docs. \n            \"\"\"\n        )\n        return None\n    full_text = \"\\n\\n\".join([d.content for d in self.original_docs])\n    if self.parser is None:\n        raise ValueError(\"No parser defined\")\n    tot_tokens = self.parser.num_tokens(full_text)\n    MAX_INPUT_TOKENS = (\n        self.llm.completion_context_length()\n        - self.config.llm.max_output_tokens\n        - 100\n    )\n    if tot_tokens &gt; MAX_INPUT_TOKENS:\n        # truncate\n        full_text = self.parser.tokenizer.decode(\n            self.parser.tokenizer.encode(full_text)[:MAX_INPUT_TOKENS]\n        )\n        logger.warning(\n            f\"Summarizing after truncating text to {MAX_INPUT_TOKENS} tokens\"\n        )\n    prompt = f\"\"\"\n    {instruction}\n    {full_text}\n    \"\"\".strip()\n    with StreamingIfAllowed(self.llm):\n        summary = Agent.llm_response(self, prompt)\n        return summary  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.justify_response","title":"<code>justify_response()</code>","text":"<p>Show evidence for last response</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def justify_response(self) -&gt; None:\n    \"\"\"Show evidence for last response\"\"\"\n    if self.response is None:\n        print(\"[magenta]No response yet\")\n        return\n    source = self.response.metadata.source\n    if len(source) &gt; 0:\n        print(\"[magenta]\" + source)\n    else:\n        print(\"[magenta]No source found\")\n</code></pre>"},{"location":"reference/agent/special/recipient_validator_agent/","title":"recipient_validator_agent","text":"<p>langroid/agent/special/recipient_validator_agent.py </p> <p>Agent to detect un-specified recipient and ask for clarification, and when received, modify the pending message so that it looks as if the parent task's LLM generated the right message in the first place.</p> <p>Note that this is deprecated in favor of using the <code>RecipientTool</code>, defined in <code>langroid.agent.tools.recipient_tool.py</code>. See usage examples in <code>tests/main/test_multi_agent_complex.py</code> and <code>tests/main/test_recipient_tool.py</code>.</p> <p>The advantages of using the <code>RecipientTool</code> are: - it uses the tool/function-call mechanism to specify a recipient in a JSON-structured     string, which is more consistent with the rest of the system, and does not require     inventing a new syntax like <code>TO:&lt;recipient&gt;</code> (which the RecipientValidatorAgent     uses). - it removes the need for any special parsing of the message content, since we leverage     the built-in JSON tool-matching in <code>Agent.handle_message()</code> and downstream code. - it does not require setting the <code>parent_responder</code> field in the <code>ChatDocument</code>     metadata, which is somewhat hacky. - it appears to be less brittle than requiring the LLM to use TO: syntax:   The LLM almost never forgets to use the RecipientTool as instructed. - The RecipientTool class acts as a specification of the required syntax, and also   contains mechanisms to enforce this syntax. - For a developer who needs to enforce recipient specification for an agent, they only   need to do <code>agent.enable_message(RecipientTool)</code>, and the rest is taken care of."},{"location":"reference/agent/special/recipient_validator_agent/#langroid.agent.special.recipient_validator_agent.RecipientValidator","title":"<code>RecipientValidator(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> Source code in <code>langroid/agent/special/recipient_validator_agent.py</code> <pre><code>def __init__(self, config: RecipientValidatorConfig):\n    super().__init__(config)\n    self.config: RecipientValidatorConfig = config\n    self.llm = None\n    self.vecdb = None\n</code></pre>"},{"location":"reference/agent/special/recipient_validator_agent/#langroid.agent.special.recipient_validator_agent.RecipientValidator.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Check whether the incoming message is in the expected format. Used to check whether the output of the LLM of the calling agent is in the expected format.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the incoming message (pending message of the task)</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]:</p> <code>Optional[ChatDocument]</code> <ul> <li>if msg is in expected format, return None (no objections)</li> </ul> <code>Optional[ChatDocument]</code> <ul> <li>otherwise, a ChatDocument that either contains a request to LLM to clarify/fix the msg, or a fixed version of the LLM's original message.</li> </ul> Source code in <code>langroid/agent/special/recipient_validator_agent.py</code> <pre><code>def agent_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Check whether the incoming message is in the expected format.\n    Used to check whether the output of the LLM of the calling agent is\n    in the expected format.\n\n    Args:\n        msg (str|ChatDocument): the incoming message (pending message of the task)\n\n    Returns:\n        Optional[ChatDocument]:\n        - if msg is in expected format, return None (no objections)\n        - otherwise, a ChatDocument that either contains a request to\n            LLM to clarify/fix the msg, or a fixed version of the LLM's original\n            message.\n    \"\"\"\n    if msg is None:\n        return None\n    if isinstance(msg, str):\n        msg = ChatDocument.from_str(msg)\n\n    recipient = msg.metadata.recipient\n    has_func_call = msg.function_call is not None\n    content = msg.content\n\n    if recipient != \"\":\n        # there is a clear recipient, return None (no objections)\n        return None\n\n    attachment: None | ChatDocAttachment = None\n    responder: None | Entity = None\n    sender_name = self.config.name\n    if (\n        has_func_call or \"TOOL\" in content\n    ) and self.config.tool_recipient is not None:\n        # assume it is meant for Coder, so simply set the recipient field,\n        # and the parent task loop continues as normal\n        # TODO- but what if it is not a legit function call\n        recipient = self.config.tool_recipient\n    elif content in self.config.recipients:\n        # the incoming message is a clarification response from LLM\n        recipient = content\n        if msg.attachment is not None and isinstance(\n            msg.attachment, RecipientValidatorAttachment\n        ):\n            content = msg.attachment.content\n        else:\n            logger.warning(\"ValidatorAgent: Did not find content to correct\")\n            content = \"\"\n        # we've used the attachment, don't need anymore\n        attachment = RecipientValidatorAttachment(content=\"\")\n        # we are rewriting an LLM message from parent, so\n        # pretend it is from LLM\n        responder = Entity.LLM\n        sender_name = \"\"\n    else:\n        # save the original message so when the Validator\n        # receives the LLM clarification,\n        # it can use it as the `content` field\n        attachment = RecipientValidatorAttachment(content=content)\n        recipient_str = \", \".join(self.config.recipients)\n        content = f\"\"\"\n        Who is this message for? \n        Please simply respond with one of these names:\n        {recipient_str}\n        \"\"\"\n        console.print(f\"[red]{self.indent}\", end=\"\")\n        print(f\"[red]Validator: {content}\")\n\n    return ChatDocument(\n        content=content,\n        function_call=msg.function_call if has_func_call else None,\n        attachment=attachment,\n        metadata=ChatDocMetaData(\n            source=Entity.AGENT,\n            sender=Entity.AGENT,\n            parent_responder=responder,\n            sender_name=sender_name,\n            recipient=recipient,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/","title":"relevance_extractor_agent","text":"<p>langroid/agent/special/relevance_extractor_agent.py </p> <p>Agent to retrieve relevant segments from a body of text, that are relevant to a query.</p>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent","title":"<code>RelevanceExtractorAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for extracting segments from text, that are relevant to a given query.</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def __init__(self, config: RelevanceExtractorAgentConfig):\n    super().__init__(config)\n    self.config: RelevanceExtractorAgentConfig = config\n    self.enable_message(SegmentExtractTool)\n    self.numbered_passage: Optional[str] = None\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    \"\"\"\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    return super().llm_response(prompt)\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM The LLM is expected to generate a structured msg according to the SegmentExtractTool schema, i.e. it should contain a <code>segment_list</code> field whose value is a list of segment numbers or ranges, like \"10,12,14-17\".</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    The LLM is expected to generate a structured msg according to the\n    SegmentExtractTool schema, i.e. it should contain a `segment_list` field\n    whose value is a list of segment numbers or ranges, like \"10,12,14-17\".\n    \"\"\"\n\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    return await super().llm_response_async(prompt)\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.extract_segments","title":"<code>extract_segments(msg)</code>","text":"<p>Method to handle a segmentExtractTool message from LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def extract_segments(self, msg: SegmentExtractTool) -&gt; str:\n    \"\"\"Method to handle a segmentExtractTool message from LLM\"\"\"\n    spec = msg.segment_list\n    if len(self.message_history) == 0:\n        return NO_ANSWER\n    if spec is None or spec.strip() == \"\":\n        return NO_ANSWER\n    assert self.numbered_passage is not None, \"No numbered passage\"\n    # assume this has numbered segments\n    extracts = extract_numbered_segments(self.numbered_passage, spec)\n    # this response ends the task by saying DONE\n    return \"DONE \" + extracts\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/","title":"retriever_agent","text":"<p>langroid/agent/special/retriever_agent.py </p> <p>Agent to retrieve relevant verbatim whole docs/records from a vector store, where the LLM is used to filter for \"true\" relevance after retrieval from the vector store. See test_retriever_agent.py for example usage.</p>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent","title":"<code>RetrieverAgent(config)</code>","text":"<p>             Bases: <code>DocChatAgent</code>, <code>ABC</code></p> <p>Agent for retrieving whole records/docs matching a query</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def __init__(self, config: RetrieverAgentConfig):\n    super().__init__(config)\n    self.config: RetrieverAgentConfig = config\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent.get_relevant_extracts","title":"<code>get_relevant_extracts(query)</code>","text":"<p>Given a query, get the records/docs whose contents are most relevant to the     query. First get nearest docs from vector store, then select the best     matches according to the LLM. Args:     query (str): query string</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of Document objects</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def get_relevant_extracts(self, query: str) -&gt; List[Document]:\n    \"\"\"\n    Given a query, get the records/docs whose contents are most relevant to the\n        query. First get nearest docs from vector store, then select the best\n        matches according to the LLM.\n    Args:\n        query (str): query string\n\n    Returns:\n        List[Document]: list of Document objects\n    \"\"\"\n    response = Document(\n        content=NO_ANSWER,\n        metadata=DocMetaData(\n            source=\"None\",\n        ),\n    )\n    nearest_docs = self.get_relevant_chunks(query)\n    if len(nearest_docs) == 0:\n        return [response]\n    if self.llm is None:\n        logger.warning(\"No LLM specified\")\n        return nearest_docs\n    with console.status(\"LLM selecting relevant docs from retrieved ones...\"):\n        with StreamingIfAllowed(self.llm, False):\n            doc_list = self.llm_select_relevant_docs(query, nearest_docs)\n\n    return doc_list\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent.llm_select_relevant_docs","title":"<code>llm_select_relevant_docs(query, docs)</code>","text":"<p>Given a query and a list of docs, select the docs whose contents match best,     according to the LLM. Use the doc IDs to select the docs from the vector     store. Args:     query: query string     docs: list of Document objects Returns:     list of Document objects</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def llm_select_relevant_docs(\n    self, query: str, docs: List[Document]\n) -&gt; List[Document]:\n    \"\"\"\n    Given a query and a list of docs, select the docs whose contents match best,\n        according to the LLM. Use the doc IDs to select the docs from the vector\n        store.\n    Args:\n        query: query string\n        docs: list of Document objects\n    Returns:\n        list of Document objects\n    \"\"\"\n    doc_contents = \"\\n\\n\".join(\n        [f\"DOC: ID={d.id()}, CONTENT: {d.content}\" for d in docs]\n    )\n    prompt = f\"\"\"\n    Given the following QUERY: \n    {query}\n    and the following DOCS with IDs and contents\n    {doc_contents}\n\n    Find at most {self.config.n_matches} DOCs that are most relevant to the QUERY.\n    Return your answer as a sequence of DOC IDS ONLY, for example: \n    \"id1 id2 id3...\"\n    If there are no relevant docs, simply say {NO_ANSWER}.\n    Even if there is only one relevant doc, return it as a single ID.\n    Do not give any explanations or justifications.\n    \"\"\"\n    default_response = Document(\n        content=NO_ANSWER,\n        metadata=DocMetaData(\n            source=\"None\",\n        ),\n    )\n\n    if self.llm is None:\n        logger.warning(\"No LLM specified\")\n        return [default_response]\n    response = self.llm.generate(\n        prompt, max_tokens=self.config.llm.max_output_tokens\n    )\n    if response.message == NO_ANSWER:\n        return [default_response]\n    ids = response.message.split()\n    if len(ids) == 0:\n        return [default_response]\n    if self.vecdb is None:\n        logger.warning(\"No vector store specified\")\n        return [default_response]\n    docs = self.vecdb.get_documents_by_ids(ids)\n    return [\n        Document(content=d.content, metadata=DocMetaData(source=\"LLM\"))\n        for d in docs\n    ]\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/","title":"table_chat_agent","text":"<p>langroid/agent/special/table_chat_agent.py </p> <p>Agent that supports asking queries about a tabular dataset, internally represented as a Pandas dataframe. The <code>TableChatAgent</code> is configured with a dataset, which can be a Pandas df, file or URL. The delimiter/separator is auto-detected. In response to a user query, the Agent's LLM generates Pandas code to answer the query. The code is passed via the <code>run_code</code> tool/function-call, which is handled by the Agent's <code>run_code</code> method. This method executes/evaluates the code and returns the result as a string.</p>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.RunCodeTool","title":"<code>RunCodeTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Tool/function to run code on a dataframe named <code>df</code></p>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.TableChatAgent","title":"<code>TableChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def __init__(self, config: TableChatAgentConfig):\n    if isinstance(config.data, pd.DataFrame):\n        df = config.data\n    else:\n        df = read_tabular_data(config.data, config.separator)\n\n    df.columns = df.columns.str.strip().str.replace(\" +\", \"_\", regex=True)\n\n    self.df = df\n    summary = dataframe_summary(df)\n    config.system_message = config.system_message.format(summary=summary)\n\n    super().__init__(config)\n    self.config: TableChatAgentConfig = config\n\n    logger.info(\n        f\"\"\"TableChatAgent initialized with dataframe of shape {self.df.shape}\n        and columns: \n        {self.df.columns}\n        \"\"\"\n    )\n    # enable the agent to use and handle the RunCodeTool\n    self.enable_message(RunCodeTool)\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.TableChatAgent.run_code","title":"<code>run_code(msg)</code>","text":"<p>Handle a RunCodeTool message by running the code and returning the result. Args:     msg (RunCodeTool): The tool-message to handle.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of running the code along with any print output.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def run_code(self, msg: RunCodeTool) -&gt; str:\n    \"\"\"\n    Handle a RunCodeTool message by running the code and returning the result.\n    Args:\n        msg (RunCodeTool): The tool-message to handle.\n\n    Returns:\n        str: The result of running the code along with any print output.\n    \"\"\"\n    code = msg.code\n    # Create a dictionary that maps 'df' to the actual DataFrame\n    local_vars = {\"df\": self.df}\n\n    # Create a string-based I/O stream\n    code_out = io.StringIO()\n\n    # Temporarily redirect standard output to our string-based I/O stream\n    sys.stdout = code_out\n\n    # Split the code into lines\n    lines = code.strip().split(\"\\n\")\n\n    lines = [\n        \"import pandas as pd\",\n        \"import numpy as np\",\n    ] + lines\n\n    # Run all lines as statements except for the last one\n    for line in lines[:-1]:\n        exec(line, {}, local_vars)\n\n    # Evaluate the last line and get the result\n    try:\n        eval_result = eval(lines[-1], {}, local_vars)\n    except Exception as e:\n        eval_result = f\"ERROR: {type(e)}: {e}\"\n\n    if eval_result is None:\n        eval_result = \"\"\n\n    # Always restore the original standard output\n    sys.stdout = sys.__stdout__\n\n    # If df has been modified in-place, save the changes back to self.df\n    self.df = local_vars[\"df\"]\n\n    # Get the resulting string from the I/O stream\n    print_result = code_out.getvalue() or \"\"\n    sep = \"\\n\" if print_result else \"\"\n    # Combine the print and eval results\n    result = f\"{print_result}{sep}{eval_result}\"\n    if result == \"\":\n        result = \"No result\"\n    # Return the result\n    return result\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.dataframe_summary","title":"<code>dataframe_summary(df)</code>","text":"<p>Generate a structured summary for a pandas DataFrame containing numerical and categorical values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to summarize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A nicely structured and formatted summary string.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>@no_type_check\ndef dataframe_summary(df: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Generate a structured summary for a pandas DataFrame containing numerical\n    and categorical values.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to summarize.\n\n    Returns:\n        str: A nicely structured and formatted summary string.\n    \"\"\"\n\n    # Column names display\n    col_names_str = (\n        \"COLUMN NAMES:\\n\" + \" \".join([f\"'{col}'\" for col in df.columns]) + \"\\n\\n\"\n    )\n\n    # Numerical data summary\n    num_summary = df.describe().applymap(lambda x: \"{:.2f}\".format(x))\n    num_str = \"Numerical Column Summary:\\n\" + num_summary.to_string() + \"\\n\\n\"\n\n    # Categorical data summary\n    cat_columns = df.select_dtypes(include=[np.object_]).columns\n    cat_summary_list = []\n\n    for col in cat_columns:\n        unique_values = df[col].unique()\n        if len(unique_values) &lt; 10:\n            cat_summary_list.append(f\"'{col}': {', '.join(map(str, unique_values))}\")\n        else:\n            cat_summary_list.append(f\"'{col}': {df[col].nunique()} unique values\")\n\n    cat_str = \"Categorical Column Summary:\\n\" + \"\\n\".join(cat_summary_list) + \"\\n\\n\"\n\n    # Missing values summary\n    nan_summary = df.isnull().sum().rename(\"missing_values\").to_frame()\n    nan_str = \"Missing Values Column Summary:\\n\" + nan_summary.to_string() + \"\\n\"\n\n    # Combine the summaries into one structured string\n    summary_str = col_names_str + num_str + cat_str + nan_str\n\n    return summary_str\n</code></pre>"},{"location":"reference/agent/special/sql/","title":"sql","text":"<p>langroid/agent/special/sql/init.py </p>"},{"location":"reference/agent/special/sql/sql_chat_agent/","title":"sql_chat_agent","text":"<p>langroid/agent/special/sql/sql_chat_agent.py </p> <p>Agent that allows interaction with an SQL database using SQLAlchemy library.  The agent can execute SQL queries in the database and return the result. </p> <p>Functionality includes: - adding table and column context - asking a question about a SQL schema</p>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgentConfig","title":"<code>SQLChatAgentConfig</code>","text":"<p>             Bases: <code>ChatAgentConfig</code></p>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgentConfig.use_schema_tools","title":"<code>use_schema_tools: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional, but strongly recommended, context descriptions for tables, columns,  and relationships. It should be a dictionary where each key is a table name  and its value is another dictionary. </p> <p>In this inner dictionary: - The 'description' key corresponds to a string description of the table. - The 'columns' key corresponds to another dictionary where each key is a  column name and its value is a string description of that column. - The 'relationships' key corresponds to another dictionary where each key  is another table name and the value is a description of the relationship to  that table.</p> <p>For example: {     'table1': {         'description': 'description of table1',         'columns': {             'column1': 'description of column1 in table1',             'column2': 'description of column2 in table1'         }     },     'table2': {         'description': 'description of table2',         'columns': {             'column3': 'description of column3 in table2',             'column4': 'description of column4 in table2'         }     } }</p>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent","title":"<code>SQLChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a SQL database</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If database information is not provided in the config.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def __init__(self, config: \"SQLChatAgentConfig\") -&gt; None:\n    \"\"\"Initialize the SQLChatAgent.\n\n    Raises:\n        ValueError: If database information is not provided in the config.\n    \"\"\"\n    self._validate_config(config)\n    self.config: SQLChatAgentConfig = config\n    self._init_database()\n    self._init_metadata()\n    self._init_table_metadata()\n    self._init_message_tools()\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.retry_query","title":"<code>retry_query(e, query)</code>","text":"<p>Generate an error message for a failed SQL query and return it.</p> <p>Parameters: e (Exception): The exception raised during the SQL query execution. query (str): The SQL query that failed.</p> <p>Returns: str: The error message.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def retry_query(self, e: Exception, query: str) -&gt; str:\n    \"\"\"\n    Generate an error message for a failed SQL query and return it.\n\n    Parameters:\n    e (Exception): The exception raised during the SQL query execution.\n    query (str): The SQL query that failed.\n\n    Returns:\n    str: The error message.\n    \"\"\"\n    logger.error(f\"SQL Query failed: {query}\\nException: {e}\")\n\n    # Optional part to be included based on `use_schema_tools`\n    optional_schema_description = \"\"\n    if not self.config.use_schema_tools:\n        optional_schema_description = f\"\"\"\\\n        This JSON schema maps SQL database structure. It outlines tables, each \n        with a description and columns. Each table is identified by a key, and holds\n        a description and a dictionary of columns, with column \n        names as keys and their descriptions as values.\n\n        ```json\n        {self.config.context_descriptions}\n        ```\"\"\"\n\n    # Construct the error message\n    error_message_template = f\"\"\"\\\n    {SQL_ERROR_MSG}: '{query}'\n    {str(e)}\n    Run a new query, correcting the errors.\n    {optional_schema_description}\"\"\"\n\n    return error_message_template\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.run_query","title":"<code>run_query(msg)</code>","text":"<p>Handle a RunQueryTool message by executing a SQL query and returning the result.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>RunQueryTool</code> <p>The tool-message to handle.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of executing the SQL query.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def run_query(self, msg: RunQueryTool) -&gt; str:\n    \"\"\"\n    Handle a RunQueryTool message by executing a SQL query and returning the result.\n\n    Args:\n        msg (RunQueryTool): The tool-message to handle.\n\n    Returns:\n        str: The result of executing the SQL query.\n    \"\"\"\n    query = msg.query\n    session = self.Session\n    response_message = \"\"\n\n    try:\n        logger.info(f\"Executing SQL query: {query}\")\n\n        query_result = session.execute(text(query))\n        session.commit()\n\n        rows = query_result.fetchall()\n        response_message = self._format_rows(rows)\n\n    except SQLAlchemyError as e:\n        session.rollback()\n        logger.error(f\"Failed to execute query: {query}\\n{e}\")\n        response_message = self.retry_query(e, query)\n    finally:\n        session.close()\n\n    return response_message\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.get_table_names","title":"<code>get_table_names(msg)</code>","text":"<p>Handle a GetTableNamesTool message by returning the names of all tables in the database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The names of all tables in the database.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def get_table_names(self, msg: GetTableNamesTool) -&gt; str:\n    \"\"\"\n    Handle a GetTableNamesTool message by returning the names of all tables in the\n    database.\n\n    Returns:\n        str: The names of all tables in the database.\n    \"\"\"\n    return \", \".join(self.metadata.tables.keys())\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.get_table_schema","title":"<code>get_table_schema(msg)</code>","text":"<p>Handle a GetTableSchemaTool message by returning the schema of all provided tables in the database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The schema of all provided tables in the database.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def get_table_schema(self, msg: GetTableSchemaTool) -&gt; str:\n    \"\"\"\n    Handle a GetTableSchemaTool message by returning the schema of all provided\n    tables in the database.\n\n    Returns:\n        str: The schema of all provided tables in the database.\n    \"\"\"\n    tables = msg.tables\n    result = \"\"\n    for table_name in tables:\n        table = self.table_metadata.get(table_name)\n        if table is not None:\n            result += f\"{table_name}: {table}\\n\"\n        else:\n            result += f\"{table_name} is not a valid table name.\\n\"\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.get_column_descriptions","title":"<code>get_column_descriptions(msg)</code>","text":"<p>Handle a GetColumnDescriptionsTool message by returning the descriptions of all provided columns from the database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The descriptions of all provided columns from the database.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def get_column_descriptions(self, msg: GetColumnDescriptionsTool) -&gt; str:\n    \"\"\"\n    Handle a GetColumnDescriptionsTool message by returning the descriptions of all\n    provided columns from the database.\n\n    Returns:\n        str: The descriptions of all provided columns from the database.\n    \"\"\"\n    table = msg.table\n    columns = msg.columns.split(\", \")\n    result = f\"\\nTABLE: {table}\"\n    descriptions = self.config.context_descriptions.get(table)\n\n    for col in columns:\n        result += f\"\\n{col} =&gt; {descriptions['columns'][col]}\"  # type: ignore\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/","title":"utils","text":"<p>langroid/agent/special/sql/utils/init.py </p>"},{"location":"reference/agent/special/sql/utils/description_extractors/","title":"description_extractors","text":"<p>langroid/agent/special/sql/utils/description_extractors.py </p>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_postgresql_descriptions","title":"<code>extract_postgresql_descriptions(engine)</code>","text":"<p>Extracts descriptions for tables and columns from a PostgreSQL database.</p> <p>This method retrieves the descriptions of tables and their columns from a PostgreSQL database using the provided SQLAlchemy engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine connected to a PostgreSQL database.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary mapping table names to a</p> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing the table description and a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>column descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_postgresql_descriptions(engine: Engine) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Extracts descriptions for tables and columns from a PostgreSQL database.\n\n    This method retrieves the descriptions of tables and their columns\n    from a PostgreSQL database using the provided SQLAlchemy engine.\n\n    Args:\n        engine (Engine): SQLAlchemy engine connected to a PostgreSQL database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary mapping table names to a\n        dictionary containing the table description and a dictionary of\n        column descriptions.\n    \"\"\"\n    inspector = inspect(engine)\n    table_names: List[str] = inspector.get_table_names()\n\n    result: Dict[str, Dict[str, Any]] = {}\n\n    with engine.connect() as conn:\n        for table in table_names:\n            table_comment = (\n                conn.execute(\n                    text(f\"SELECT obj_description('{table}'::regclass)\")\n                ).scalar()\n                or \"\"\n            )\n\n            columns = {}\n            col_data = inspector.get_columns(table)\n            for idx, col in enumerate(col_data, start=1):\n                col_comment = (\n                    conn.execute(\n                        text(f\"SELECT col_description('{table}'::regclass, {idx})\")\n                    ).scalar()\n                    or \"\"\n                )\n                columns[col[\"name\"]] = col_comment\n\n            result[table] = {\"description\": table_comment, \"columns\": columns}\n\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_mysql_descriptions","title":"<code>extract_mysql_descriptions(engine)</code>","text":"<p>Extracts descriptions for tables and columns from a MySQL database.</p> <p>This method retrieves the descriptions of tables and their columns from a MySQL database using the provided SQLAlchemy engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine connected to a MySQL database.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary mapping table names to a</p> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing the table description and a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>column descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_mysql_descriptions(engine: Engine) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Extracts descriptions for tables and columns from a MySQL database.\n\n    This method retrieves the descriptions of tables and their columns\n    from a MySQL database using the provided SQLAlchemy engine.\n\n    Args:\n        engine (Engine): SQLAlchemy engine connected to a MySQL database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary mapping table names to a\n        dictionary containing the table description and a dictionary of\n        column descriptions.\n    \"\"\"\n    inspector = inspect(engine)\n    table_names: List[str] = inspector.get_table_names()\n\n    result: Dict[str, Dict[str, Any]] = {}\n\n    with engine.connect() as conn:\n        for table in table_names:\n            query = text(\n                \"SELECT table_comment FROM information_schema.tables WHERE\"\n                \" table_schema = :schema AND table_name = :table\"\n            )\n            table_result = conn.execute(\n                query, {\"schema\": engine.url.database, \"table\": table}\n            )\n            table_comment = table_result.scalar() or \"\"\n\n            columns = {}\n            for col in inspector.get_columns(table):\n                columns[col[\"name\"]] = col.get(\"comment\", \"\")\n\n            result[table] = {\"description\": table_comment, \"columns\": columns}\n\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_default_descriptions","title":"<code>extract_default_descriptions(engine)</code>","text":"<p>Extracts default descriptions for tables and columns from a database.</p> <p>This method retrieves the table and column names from the given database and associates empty descriptions with them.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine connected to a database.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary mapping table names to a</p> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing an empty table description and a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>empty column descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_default_descriptions(engine: Engine) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Extracts default descriptions for tables and columns from a database.\n\n    This method retrieves the table and column names from the given database\n    and associates empty descriptions with them.\n\n    Args:\n        engine (Engine): SQLAlchemy engine connected to a database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary mapping table names to a\n        dictionary containing an empty table description and a dictionary of\n        empty column descriptions.\n    \"\"\"\n    inspector = inspect(engine)\n    table_names: List[str] = inspector.get_table_names()\n\n    result: Dict[str, Dict[str, Any]] = {}\n\n    for table in table_names:\n        columns = {}\n        for col in inspector.get_columns(table):\n            columns[col[\"name\"]] = \"\"\n\n        result[table] = {\"description\": \"\", \"columns\": columns}\n\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_schema_descriptions","title":"<code>extract_schema_descriptions(engine)</code>","text":"<p>Extracts the schema descriptions from the database connected to by the engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine instance.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary representation of table and column</p> <code>Dict[str, Dict[str, Any]]</code> <p>descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_schema_descriptions(engine: Engine) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Extracts the schema descriptions from the database connected to by the engine.\n\n    Args:\n        engine (Engine): SQLAlchemy engine instance.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary representation of table and column\n        descriptions.\n    \"\"\"\n\n    extractors = {\n        \"postgresql\": extract_postgresql_descriptions,\n        \"mysql\": extract_mysql_descriptions,\n    }\n    return extractors.get(engine.dialect.name, extract_default_descriptions)(engine)\n</code></pre>"},{"location":"reference/agent/special/sql/utils/populate_metadata/","title":"populate_metadata","text":"<p>langroid/agent/special/sql/utils/populate_metadata.py </p>"},{"location":"reference/agent/special/sql/utils/populate_metadata/#langroid.agent.special.sql.utils.populate_metadata.populate_metadata_with_schema_tools","title":"<code>populate_metadata_with_schema_tools(metadata, info)</code>","text":"<p>Extracts information from an SQLAlchemy database's metadata and combines it with another dictionary with context descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>MetaData</code> <p>SQLAlchemy metadata object of the database.</p> required <code>info</code> <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary with table and column                                  descriptions.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Union[str, Dict[str, str]]]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary with table and context information.</p> Source code in <code>langroid/agent/special/sql/utils/populate_metadata.py</code> <pre><code>def populate_metadata_with_schema_tools(\n    metadata: MetaData, info: Dict[str, Dict[str, Union[str, Dict[str, str]]]]\n) -&gt; Dict[str, Dict[str, Union[str, Dict[str, str]]]]:\n    \"\"\"\n    Extracts information from an SQLAlchemy database's metadata and combines it\n    with another dictionary with context descriptions.\n\n    Args:\n        metadata (MetaData): SQLAlchemy metadata object of the database.\n        info (Dict[str, Dict[str, Any]]): A dictionary with table and column\n                                             descriptions.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary with table and context information.\n    \"\"\"\n\n    db_info: Dict[str, Dict[str, Union[str, Dict[str, str]]]] = {}\n\n    # Create empty metadata dictionary with column datatypes\n    for table_name, table in metadata.tables.items():\n        # Populate tables with empty descriptions\n        db_info[str(table_name)] = {\n            \"description\": info[table_name][\"description\"] or \"\",\n            \"columns\": {},\n        }\n\n        for column in table.columns:\n            # Populate columns with datatype\n            db_info[str(table_name)][\"columns\"][str(column.name)] = (  # type: ignore\n                str(column.type)\n            )\n\n    return db_info\n</code></pre>"},{"location":"reference/agent/special/sql/utils/populate_metadata/#langroid.agent.special.sql.utils.populate_metadata.populate_metadata","title":"<code>populate_metadata(metadata, info)</code>","text":"<p>Populate metadata based on the provided database metadata and additional info.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>MetaData</code> <p>Metadata object from SQLAlchemy.</p> required <code>info</code> <code>Dict</code> <p>Additional information for database tables and columns.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, Dict[str, Union[str, Dict[str, str]]]]</code> <p>A dictionary containing populated metadata information.</p> Source code in <code>langroid/agent/special/sql/utils/populate_metadata.py</code> <pre><code>def populate_metadata(\n    metadata: MetaData, info: Dict[str, Dict[str, Union[str, Dict[str, str]]]]\n) -&gt; Dict[str, Dict[str, Union[str, Dict[str, str]]]]:\n    \"\"\"\n    Populate metadata based on the provided database metadata and additional info.\n\n    Args:\n        metadata (MetaData): Metadata object from SQLAlchemy.\n        info (Dict): Additional information for database tables and columns.\n\n    Returns:\n        Dict: A dictionary containing populated metadata information.\n    \"\"\"\n\n    # Fetch basic metadata info using available tools\n    db_info: Dict[\n        str, Dict[str, Union[str, Dict[str, str]]]\n    ] = populate_metadata_with_schema_tools(metadata=metadata, info=info)\n\n    # Iterate over tables to update column metadata\n    for table_name in db_info.keys():\n        # Update only if additional info for the table exists\n        if table_name in info:\n            for column_name in db_info[table_name][\"columns\"]:\n                # Merge and update column description if available\n                if column_name in info[table_name][\"columns\"]:\n                    db_info[table_name][\"columns\"][column_name] = (  # type: ignore\n                        db_info[table_name][\"columns\"][column_name]  # type: ignore\n                        + \"; \"\n                        + info[table_name][\"columns\"][column_name]  # type: ignore\n                    )\n\n    return db_info\n</code></pre>"},{"location":"reference/agent/special/sql/utils/system_message/","title":"system_message","text":"<p>langroid/agent/special/sql/utils/system_message.py </p>"},{"location":"reference/agent/special/sql/utils/tools/","title":"tools","text":"<p>langroid/agent/special/sql/utils/tools.py </p>"},{"location":"reference/agent/tools/","title":"tools","text":"<p>langroid/agent/tools/init.py </p>"},{"location":"reference/agent/tools/google_search_tool/","title":"google_search_tool","text":"<p>langroid/agent/tools/google_search_tool.py </p> <p>A tool to trigger a Google search for a given query, and return the top results with their titles, links, summaries. Since the tool is stateless (i.e. does not need access to agent state), it can be enabled for any agent, without having to define a special method inside the agent: <code>agent.enable_message(GoogleSearchTool)</code></p> <p>NOTE: Using this tool requires setting the GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables in your <code>.env</code> file, as explained in the README.</p>"},{"location":"reference/agent/tools/recipient_tool/","title":"recipient_tool","text":"<p>langroid/agent/tools/recipient_tool.py </p> <p>The <code>recipient_tool</code> is used to send a message to a specific recipient. Various methods from the RecipientTool and AddRecipientTool class are inserted into the Agent as methods (see <code>langroid/agent/base.py</code>, the method <code>_get_tool_list()</code>).</p> <p>See usage examples in <code>tests/main/test_multi_agent_complex.py</code> and <code>tests/main/test_recipient_tool.py</code>.</p> <p>Previously we were using RecipientValidatorAgent to enforce proper recipient specifiction, but the preferred method is to use the <code>RecipientTool</code> class.  This has numerous advantages: - it uses the tool/function-call mechanism to specify a recipient in a JSON-structured     string, which is more consistent with the rest of the system, and does not require     inventing a new syntax like <code>TO:&lt;recipient&gt;</code> (which the RecipientValidatorAgent     uses). - it removes the need for any special parsing of the message content, since we leverage     the built-in JSON tool-matching in <code>Agent.handle_message()</code> and downstream code. - it does not require setting the <code>parent_responder</code> field in the <code>ChatDocument</code>     metadata, which is somewhat hacky. - it appears to be less brittle than requiring the LLM to use TO: syntax:   The LLM almost never forgets to use the RecipientTool as instructed. - The RecipientTool class acts as a specification of the required syntax, and also   contains mechanisms to enforce this syntax. - For a developer who needs to enforce recipient specification for an agent, they only   need to do <code>agent.enable_message(RecipientTool)</code>, and the rest is taken care of."},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.AddRecipientTool","title":"<code>AddRecipientTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Used by LLM to add a recipient to the previous message, when it has forgotten to specify a recipient. This avoids having to re-generate the previous message (and thus saves token-cost and time).</p>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.AddRecipientTool.response","title":"<code>response(agent)</code>","text":"<p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; ChatDocument:\n    \"\"\"\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.recipient.\n    \"\"\"\n    print(f\"[red]RecipientTool: Added recipient {self.recipient} to message.\")\n    if self.__class__.saved_content == \"\":\n        recipient_request_name = RecipientTool.default_value(\"request\")\n        raise ValueError(\n            f\"\"\"\n            Recipient specified but content is empty!\n            This could be because the `{self.request}` tool/function was used \n            before using `{recipient_request_name}` tool/function.\n            Resend the message using `{recipient_request_name}` tool/function.\n            \"\"\"\n        )\n    content = self.__class__.saved_content  # use class-level attrib value\n    # erase content since we just used it.\n    self.__class__.saved_content = \"\"\n    return ChatDocument(\n        content=content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as it msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool","title":"<code>RecipientTool</code>","text":"<p>             Bases: <code>ToolMessage</code></p> <p>Used by LLM to send a message to a specific recipient.</p> <p>Useful in cases where an LLM is talking to 2 or more agents, and needs to specify which agent (task) its message is intended for. The recipient name should be the name of a task (which is normally the name of the agent that the task wraps, although the task can have its own name).</p> <p>To use this tool/function-call, LLM must generate a JSON structure with these fields: {     \"request\": \"recipient_message\", # also the function name when using fn-calling     \"intended_recipient\": ,     \"content\":  }"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.instructions","title":"<code>instructions()</code>  <code>classmethod</code>","text":"<p>Generate instructions for using this tool/function. These are intended to be appended to the system message of the LLM.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@classmethod\ndef instructions(cls) -&gt; str:\n    \"\"\"\n    Generate instructions for using this tool/function.\n    These are intended to be appended to the system message of the LLM.\n    \"\"\"\n    recipients = []\n    if has_field(cls, \"allowed_recipients\"):\n        recipients = cls.default_value(\"allowed_recipients\")\n    if len(recipients) &gt; 0:\n        recipients_str = \", \".join(recipients)\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to one of the following:\n        {recipients_str},\n        and setting the 'content' field to your message.\n        \"\"\"\n    else:\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to the name of the recipient, \n        and setting the 'content' field to your message.\n        \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.response","title":"<code>response(agent)</code>","text":"<p>When LLM has correctly used this tool, set the agent's <code>recipient_tool_used</code> field to True, and construct a ChatDocument with an explicit recipient, and make it look like it is from the LLM.</p> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; str | ChatDocument:\n    \"\"\"\n    When LLM has correctly used this tool, set the agent's `recipient_tool_used`\n    field to True, and construct a ChatDocument with an explicit recipient,\n    and make it look like it is from the LLM.\n\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.recipient.\n    \"\"\"\n\n    if self.intended_recipient == \"\":\n        # save the content as a class-variable, so that\n        # we can construct the ChatDocument once the LLM specifies a recipient.\n        # This avoids having to re-generate the entire message, saving time + cost.\n        AddRecipientTool.saved_content = self.content\n        agent.enable_message(AddRecipientTool)\n        return ChatDocument(\n            content=\"\"\"\n            Empty recipient field!\n            Please use the 'add_recipient' tool/function-call to specify who your \n            message is intended for.\n            DO NOT REPEAT your original message; ONLY specify the recipient via this\n            tool/function-call.\n            \"\"\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                recipient=Entity.LLM,\n            ),\n        )\n\n    print(\"[red]RecipientTool: Validated properly addressed message\")\n\n    return ChatDocument(\n        content=self.content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as it msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.handle_message_fallback","title":"<code>handle_message_fallback(agent, msg)</code>  <code>staticmethod</code>","text":"<p>Response of agent if this tool is not used, e.g. the LLM simply sends a message without using this tool. This method has two purposes: (a) Alert the LLM that it has forgotten to specify a recipient, and prod it     to use the <code>add_recipient</code> tool to specify just the recipient     (and not re-generate the entire message). (b) Save the content of the message in the agent's <code>content</code> field,     so the agent can construct a ChatDocument with this content once LLM     later specifies a recipient using the <code>add_recipient</code> tool.</p> <p>This method is used to set the agent's handle_message_fallback() method.</p> <p>Returns:</p> Type Description <code>str</code> <p>reminder to LLM to use the <code>add_recipient</code> tool.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@staticmethod\ndef handle_message_fallback(\n    agent: ChatAgent, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Response of agent if this tool is not used, e.g.\n    the LLM simply sends a message without using this tool.\n    This method has two purposes:\n    (a) Alert the LLM that it has forgotten to specify a recipient, and prod it\n        to use the `add_recipient` tool to specify just the recipient\n        (and not re-generate the entire message).\n    (b) Save the content of the message in the agent's `content` field,\n        so the agent can construct a ChatDocument with this content once LLM\n        later specifies a recipient using the `add_recipient` tool.\n\n    This method is used to set the agent's handle_message_fallback() method.\n\n    Returns:\n        (str): reminder to LLM to use the `add_recipient` tool.\n    \"\"\"\n    # Note: once the LLM specifies a missing recipient, the task loop\n    # mechanism will not allow any of the \"native\" responders to respond,\n    # since the recipient will differ from the task name.\n    # So if this method is called, we can be sure that the recipient has not\n    # been specified.\n    if isinstance(msg, str):\n        return None\n    if msg.metadata.sender != Entity.LLM:\n        return None\n    content = msg if isinstance(msg, str) else msg.content\n    # save the content as a class-variable, so that\n    # we can construct the ChatDocument once the LLM specifies a recipient.\n    # This avoids having to re-generate the entire message, saving time + cost.\n    AddRecipientTool.saved_content = content\n    agent.enable_message(AddRecipientTool)\n    print(\"[red]RecipientTool: Recipient not specified, asking LLM to clarify.\")\n    return ChatDocument(\n        content=\"\"\"\n        Please use the 'add_recipient' tool/function-call to specify who your \n        `intended_recipient` is.\n        DO NOT REPEAT your original message; ONLY specify the \n        `intended_recipient` via this tool/function-call.\n        \"\"\",\n        metadata=ChatDocMetaData(\n            sender=Entity.AGENT,\n            recipient=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/segment_extract_tool/","title":"segment_extract_tool","text":"<p>langroid/agent/tools/segment_extract_tool.py </p> <p>A tool to extract segment numbers from the last user message, containing numbered segments.</p> <p>The idea is that when an LLM wants to (or is asked to) simply extract portions of a message verbatim, it should use this tool/function to SPECIFY what should be extracted, rather than actually extracting it. The output will be in the form of a list of segment numbers or ranges. This will usually be much cheaper and faster than actually writing out the extracted text. The handler of this tool/function will then extract the text and send it back.</p>"},{"location":"reference/cachedb/","title":"cachedb","text":"<p>langroid/cachedb/init.py </p>"},{"location":"reference/cachedb/base/","title":"base","text":"<p>langroid/cachedb/base.py </p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB","title":"<code>CacheDB</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a cache database.</p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.store","title":"<code>store(key, value)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>dict</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef store(self, key: str, value: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Abstract method to store a value associated with a key.\n\n    Args:\n        key (str): The key under which to store the value.\n        value (dict): The value to store.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.retrieve","title":"<code>retrieve(key)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Abstract method to retrieve the value associated with a key.\n\n    Args:\n        key (str): The key to retrieve the value for.\n\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/","title":"momento_cachedb","text":"<p>langroid/cachedb/momento_cachedb.py </p>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCacheConfig","title":"<code>MomentoCacheConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration model for Momento Cache.</p>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache","title":"<code>MomentoCache(config)</code>","text":"<p>             Bases: <code>CacheDB</code></p> <p>Momento implementation of the CacheDB.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MomentoCacheConfig</code> <p>The configuration to use.</p> required Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def __init__(self, config: MomentoCacheConfig):\n    \"\"\"\n    Initialize a MomentoCache with the given config.\n\n    Args:\n        config (MomentoCacheConfig): The configuration to use.\n    \"\"\"\n    self.config = config\n    load_dotenv()\n\n    momento_token = os.getenv(\"MOMENTO_AUTH_TOKEN\")\n    if momento_token is None:\n        raise ValueError(\"\"\"MOMENTO_AUTH_TOKEN not set in .env file\"\"\")\n    else:\n        self.client = momento.CacheClient(\n            configuration=momento.Configurations.Laptop.v1(),\n            credential_provider=momento.CredentialProvider.from_environment_variable(\n                \"MOMENTO_AUTH_TOKEN\"\n            ),\n            default_ttl=timedelta(seconds=self.config.ttl),\n        )\n        self.client.create_cache(self.config.cachename)\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.clear","title":"<code>clear()</code>","text":"<p>Clear keys from current db.</p> Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear keys from current db.\"\"\"\n    self.client.flush_cache(self.config.cachename)\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.store","title":"<code>store(key, value)</code>","text":"<p>Store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def store(self, key: str, value: Any) -&gt; None:\n    \"\"\"\n    Store a value associated with a key.\n\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\n    self.client.set(self.config.cachename, key, json.dumps(value))\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.retrieve","title":"<code>retrieve(key)</code>","text":"<p>Retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the value associated with a key.\n\n    Args:\n        key (str): The key to retrieve the value for.\n\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\n    value = self.client.get(self.config.cachename, key)\n    if isinstance(value, CacheGet.Hit):\n        return json.loads(value.value_string)  # type: ignore\n    else:\n        return None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/","title":"redis_cachedb","text":"<p>langroid/cachedb/redis_cachedb.py </p>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCacheConfig","title":"<code>RedisCacheConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration model for RedisCache.</p>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache","title":"<code>RedisCache(config)</code>","text":"<p>             Bases: <code>CacheDB</code></p> <p>Redis implementation of the CacheDB.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RedisCacheConfig</code> <p>The configuration to use.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def __init__(self, config: RedisCacheConfig):\n    \"\"\"\n    Initialize a RedisCache with the given config.\n\n    Args:\n        config (RedisCacheConfig): The configuration to use.\n    \"\"\"\n    self.config = config\n    load_dotenv()\n\n    if self.config.fake:\n        self.client = fakeredis.FakeStrictRedis()  # type: ignore\n    else:\n        redis_password = os.getenv(\"REDIS_PASSWORD\")\n        redis_host = os.getenv(\"REDIS_HOST\")\n        redis_port = os.getenv(\"REDIS_PORT\")\n        if None in [redis_password, redis_host, redis_port]:\n            logger.warning(\n                \"\"\"REDIS_PASSWORD, REDIS_HOST, REDIS_PORT not set in .env file,\n                using fake redis client\"\"\"\n            )\n            self.client = fakeredis.FakeStrictRedis()  # type: ignore\n        else:\n            self.client = redis.Redis(  # type: ignore\n                host=redis_host,\n                port=redis_port,\n                password=redis_password,\n            )\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear","title":"<code>clear()</code>","text":"<p>Clear keys from current db.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear keys from current db.\"\"\"\n    self.client.flushdb()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear_all","title":"<code>clear_all()</code>","text":"<p>Clear all keys from all dbs.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear_all(self) -&gt; None:\n    \"\"\"Clear all keys from all dbs.\"\"\"\n    self.client.flushall()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.store","title":"<code>store(key, value)</code>","text":"<p>Store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def store(self, key: str, value: Any) -&gt; None:\n    \"\"\"\n    Store a value associated with a key.\n\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\n    self.client.set(key, json.dumps(value))\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.retrieve","title":"<code>retrieve(key)</code>","text":"<p>Retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the value associated with a key.\n\n    Args:\n        key (str): The key to retrieve the value for.\n\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\n    value = self.client.get(key)\n    return json.loads(value) if value else None\n</code></pre>"},{"location":"reference/embedding_models/","title":"embedding_models","text":"<p>langroid/embedding_models/init.py </p>"},{"location":"reference/embedding_models/base/","title":"base","text":"<p>langroid/embedding_models/base.py </p>"},{"location":"reference/embedding_models/base/#langroid.embedding_models.base.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for an embedding model.</p>"},{"location":"reference/embedding_models/base/#langroid.embedding_models.base.EmbeddingModel.similarity","title":"<code>similarity(text1, text2)</code>","text":"<p>Compute cosine similarity between two texts.</p> Source code in <code>langroid/embedding_models/base.py</code> <pre><code>def similarity(self, text1: str, text2: str) -&gt; float:\n    \"\"\"Compute cosine similarity between two texts.\"\"\"\n    [emb1, emb2] = self.embedding_fn()([text1, text2])\n    return float(\n        np.array(emb1)\n        @ np.array(emb2)\n        / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n    )\n</code></pre>"},{"location":"reference/embedding_models/models/","title":"models","text":"<p>langroid/embedding_models/models.py </p>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.embedding_model","title":"<code>embedding_model(embedding_fn_type='openai')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_fn_type</code> <code>str</code> <p>\"openai\" or \"sentencetransformer\" # others soon</p> <code>'openai'</code> <p>Returns:     EmbeddingModel</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def embedding_model(embedding_fn_type: str = \"openai\") -&gt; EmbeddingModel:\n    \"\"\"\n    Args:\n        embedding_fn_type: \"openai\" or \"sentencetransformer\" # others soon\n    Returns:\n        EmbeddingModel\n    \"\"\"\n    if embedding_fn_type == \"openai\":\n        return OpenAIEmbeddings  # type: ignore\n    else:  # default sentence transformer\n        return SentenceTransformerEmbeddings  # type: ignore\n</code></pre>"},{"location":"reference/language_models/","title":"language_models","text":"<p>langroid/language_models/init.py </p>"},{"location":"reference/language_models/azure_openai/","title":"azure_openai","text":"<p>langroid/language_models/azure_openai.py </p>"},{"location":"reference/language_models/azure_openai/#langroid.language_models.azure_openai.AzureConfig","title":"<code>AzureConfig</code>","text":"<p>             Bases: <code>OpenAIGPTConfig</code></p> <p>Configuration for Azure OpenAI GPT.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>should be <code>azure.</code></p> <code>api_version</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_API_VERSION.</code></p> <code>deployment_name</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_DEPLOYMENT_NAME</code> and should be based the custom name you chose for your deployment when you deployed a model.</p> <code>model_name</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_GPT_MODEL_NAME</code> and should be based on the model name chosen during setup.</p>"},{"location":"reference/language_models/azure_openai/#langroid.language_models.azure_openai.AzureGPT","title":"<code>AzureGPT(config)</code>","text":"<p>             Bases: <code>OpenAIGPT</code></p> <p>Class to access OpenAI LLMs via Azure. These env variables can be obtained from the file <code>.azure_env</code>. Azure OpenAI doesn't support <code>completion</code> Attributes:     config (AzureConfig): AzureConfig object     api_key (str): Azure API key     api_base (str): Azure API base url     api_version (str): Azure API version     model_name (str): the name of gpt model in your deployment</p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def __init__(self, config: AzureConfig):\n    super().__init__(config)\n    self.config: AzureConfig = config\n    self.api_type = config.type\n    openai.api_type = self.api_type\n    load_dotenv()\n    self.api_key = os.getenv(\"AZURE_API_KEY\", \"\")\n    if self.api_key == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_API_KEY not set in .env file,\n            please set it to your Azure API key.\"\"\"\n        )\n\n    self.api_base = os.getenv(\"AZURE_OPENAI_API_BASE\", \"\")\n    if self.api_base == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_API_BASE not set in .env file,\n            please set it to your Azure API key.\"\"\"\n        )\n    # we don't need this for ``api_key`` because it's handled inside\n    # ``openai_gpt.py`` methods before invoking chat/completion calls\n    else:\n        openai.api_base = self.api_base\n\n    self.api_version = (\n        os.getenv(\"AZURE_OPENAI_API_VERSION\", \"\") or config.api_version\n    )\n    openai.api_version = self.api_version\n\n    self.deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"\")\n    if self.deployment_name == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_OPENAI_DEPLOYMENT_NAME not set in .env file,\n            please set it to your Azure openai deployment name.\"\"\"\n        )\n\n    self.model_name = os.getenv(\"AZURE_GPT_MODEL_NAME\", \"\")\n    if self.model_name == \"\":\n        raise ValueError(\n            \"\"\"\n            AZURE_GPT_MODEL_NAME not set in .env file,\n            please set it to chat model name in you deployment model.\"\"\"\n        )\n\n    # set the chat model to be the same as the model_name\n    # This corresponds to the gpt model you chose for your deployment\n    # when you deployed a model\n    if \"35-turbo\" in self.model_name:\n        self.config.chat_model = OpenAIChatModel.GPT3_5_TURBO\n    else:\n        self.config.chat_model = OpenAIChatModel.GPT4\n</code></pre>"},{"location":"reference/language_models/base/","title":"base","text":"<p>langroid/language_models/base.py </p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionCall","title":"<code>LLMFunctionCall</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Structure of LLM response indicate it \"wants\" to call a function. Modeled after OpenAI spec for <code>function_call</code> field in ChatCompletion API.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionSpec","title":"<code>LLMFunctionSpec</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Description of a function available for the LLM to use. To be used when calling the LLM <code>chat()</code> method with the <code>functions</code> parameter. Modeled after OpenAI spec for <code>functions</code> fields in ChatCompletion API.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage","title":"<code>LLMMessage</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class representing message sent to, or received from, LLM.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage.api_dict","title":"<code>api_dict()</code>","text":"<p>Convert to dictionary for API request. Returns:     dict: dictionary representation of LLM message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def api_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert to dictionary for API request.\n    Returns:\n        dict: dictionary representation of LLM message\n    \"\"\"\n    d = self.dict()\n    # drop None values since API doesn't accept them\n    dict_no_none = {k: v for k, v in d.items() if v is not None}\n    if \"name\" in dict_no_none and dict_no_none[\"name\"] == \"\":\n        # OpenAI API does not like empty name\n        del dict_no_none[\"name\"]\n    if \"function_call\" in dict_no_none:\n        # arguments must be a string\n        if \"arguments\" in dict_no_none[\"function_call\"]:\n            dict_no_none[\"function_call\"][\"arguments\"] = json.dumps(\n                dict_no_none[\"function_call\"][\"arguments\"]\n            )\n    return dict_no_none\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse","title":"<code>LLMResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class representing response from LLM.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse.get_recipient_and_message","title":"<code>get_recipient_and_message()</code>","text":"<p>If <code>message</code> or <code>function_call</code> of an LLM response contains an explicit recipient name, return this recipient name and <code>message</code> stripped of the recipient name if specified.</p> <p>Two cases: (a) <code>message</code> contains \"TO:  \", or (b) <code>message</code> is empty and <code>function_call</code> with <code>to: &lt;name&gt;</code> <p>Returns:</p> Type Description <code>str</code> <p>name of recipient, which may be empty string if no recipient</p> <code>str</code> <p>content of message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_recipient_and_message(\n    self,\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    If `message` or `function_call` of an LLM response contains an explicit\n    recipient name, return this recipient name and `message` stripped\n    of the recipient name if specified.\n\n    Two cases:\n    (a) `message` contains \"TO: &lt;name&gt; &lt;content&gt;\", or\n    (b) `message` is empty and `function_call` with `to: &lt;name&gt;`\n\n    Returns:\n        (str): name of recipient, which may be empty string if no recipient\n        (str): content of message\n\n    \"\"\"\n\n    if self.function_call is not None:\n        # in this case we ignore message, since all information is in function_call\n        msg = \"\"\n        # recipient may either have been specified as a special field \"to\" in\n        # function_call, or as a parameter \"recipient\" in the arguments\n        # (the latter can happen when using a Tool that has a 'recipient' parameter)\n        recipient = self.function_call.to\n        if recipient == \"\":\n            args = self.function_call.arguments\n            if isinstance(args, dict):\n                recipient = args.get(\"recipient\", \"\")\n        return recipient, msg\n    else:\n        msg = self.message\n\n    # It's not a function call, so continue looking to see\n    # if a recipient is specified in the message.\n\n    # First check if message contains \"TO: &lt;recipient&gt; &lt;content&gt;\"\n    recipient_name, content = parse_message(msg) if msg is not None else (\"\", \"\")\n    # check if there is a top level json that specifies 'recipient',\n    # and retain the entire message as content.\n    if recipient_name == \"\":\n        recipient_name = top_level_json_field(msg, \"recipient\") if msg else \"\"\n        content = msg\n    return recipient_name, content\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel","title":"<code>LanguageModel(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for language models.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def __init__(self, config: LLMConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.create","title":"<code>create(config)</code>  <code>staticmethod</code>","text":"<p>Create a language model. Args:     config: configuration for language model Returns: instance of language model</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef create(config: Optional[LLMConfig]) -&gt; Optional[\"LanguageModel\"]:\n    \"\"\"\n    Create a language model.\n    Args:\n        config: configuration for language model\n    Returns: instance of language model\n    \"\"\"\n    if type(config) is LLMConfig:\n        raise ValueError(\n            \"\"\"\n            Cannot create a Language Model object from LLMConfig. \n            Please specify a specific subclass of LLMConfig e.g., \n            OpenAIGPTConfig. If you are creating a ChatAgent from \n            a ChatAgentConfig, please specify the `llm` field of this config\n            as a specific subclass of LLMConfig, e.g., OpenAIGPTConfig.\n            \"\"\"\n        )\n    from langroid.language_models.azure_openai import AzureGPT\n    from langroid.language_models.openai_gpt import OpenAIGPT\n\n    if config is None or config.type is None:\n        return None\n\n    openai: Union[Type[AzureGPT], Type[OpenAIGPT]]\n\n    if config.type == \"azure\":\n        openai = AzureGPT\n    else:\n        openai = OpenAIGPT\n    cls = dict(\n        openai=openai,\n    ).get(config.type, openai)\n    return cls(config)  # type: ignore\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.user_assistant_pairs","title":"<code>user_assistant_pairs(lst)</code>  <code>staticmethod</code>","text":"<p>Given an even-length sequence of strings, split into a sequence of pairs</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>List[str]</code> <p>sequence of strings</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>List[Tuple[str,str]]: sequence of pairs of strings</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef user_assistant_pairs(lst: List[str]) -&gt; List[Tuple[str, str]]:\n    \"\"\"\n    Given an even-length sequence of strings, split into a sequence of pairs\n\n    Args:\n        lst (List[str]): sequence of strings\n\n    Returns:\n        List[Tuple[str,str]]: sequence of pairs of strings\n    \"\"\"\n    evens = lst[::2]\n    odds = lst[1::2]\n    return list(zip(evens, odds))\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_chat_history_components","title":"<code>get_chat_history_components(messages)</code>  <code>staticmethod</code>","text":"<p>From the chat history, extract system prompt, user-assistant turns, and final user msg.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>List of messages in the chat history</p> required <p>Returns:</p> Type Description <code>Tuple[str, List[Tuple[str, str]], str]</code> <p>Tuple[str, List[Tuple[str,str]], str]: system prompt, user-assistant turns, final user msg</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef get_chat_history_components(\n    messages: List[LLMMessage],\n) -&gt; Tuple[str, List[Tuple[str, str]], str]:\n    \"\"\"\n    From the chat history, extract system prompt, user-assistant turns, and\n    final user msg.\n\n    Args:\n        messages (List[LLMMessage]): List of messages in the chat history\n\n    Returns:\n        Tuple[str, List[Tuple[str,str]], str]:\n            system prompt, user-assistant turns, final user msg\n\n    \"\"\"\n    # Handle various degenerate cases\n    messages = [m for m in messages]  # copy\n    DUMMY_SYS_PROMPT = \"You are a helpful assistant.\"\n    DUMMY_USER_PROMPT = \"Follow the instructions above.\"\n    if len(messages) == 0 or messages[0].role != Role.SYSTEM:\n        logger.warning(\"No system msg, creating dummy system prompt\")\n        messages.insert(0, LLMMessage(content=DUMMY_SYS_PROMPT, role=Role.SYSTEM))\n    system_prompt = messages[0].content\n\n    # now we have messages = [Sys,...]\n    if len(messages) == 1:\n        logger.warning(\n            \"Got only system message in chat history, creating dummy user prompt\"\n        )\n        messages.append(LLMMessage(content=DUMMY_USER_PROMPT, role=Role.USER))\n\n    # now we have messages = [Sys, msg, ...]\n\n    if messages[1].role != Role.USER:\n        messages.insert(1, LLMMessage(content=DUMMY_USER_PROMPT, role=Role.USER))\n\n    # now we have messages = [Sys, user, ...]\n    if messages[-1].role != Role.USER:\n        logger.warning(\n            \"Last message in chat history is not a user message,\"\n            \" creating dummy user prompt\"\n        )\n        messages.append(LLMMessage(content=DUMMY_USER_PROMPT, role=Role.USER))\n\n    # now we have messages = [Sys, user, ..., user]\n    # so we omit the first and last elements and make pairs of user-asst messages\n    conversation = [m.content for m in messages[1:-1]]\n    user_prompt = messages[-1].content\n    pairs = LanguageModel.user_assistant_pairs(conversation)\n    return system_prompt, pairs, user_prompt\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.set_stream","title":"<code>set_stream(stream)</code>  <code>abstractmethod</code>","text":"<p>Enable or disable streaming output from API. Return previous value of stream.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef set_stream(self, stream: bool) -&gt; bool:\n    \"\"\"Enable or disable streaming output from API.\n    Return previous value of stream.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_stream","title":"<code>get_stream()</code>  <code>abstractmethod</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef get_stream(self) -&gt; bool:\n    \"\"\"Get streaming status\"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.update_usage_cost","title":"<code>update_usage_cost(chat, prompts, completions, cost)</code>","text":"<p>Update usage cost for this LLM. Args:     chat (bool): whether to update for chat or completion model     prompts (int): number of tokens used for prompts     completions (int): number of tokens used for completions     cost (float): total token cost in USD</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def update_usage_cost(\n    self, chat: bool, prompts: int, completions: int, cost: float\n) -&gt; None:\n    \"\"\"\n    Update usage cost for this LLM.\n    Args:\n        chat (bool): whether to update for chat or completion model\n        prompts (int): number of tokens used for prompts\n        completions (int): number of tokens used for completions\n        cost (float): total token cost in USD\n    \"\"\"\n    mdl = self.config.chat_model if chat else self.config.completion_model\n    if mdl is None:\n        return\n    if mdl not in self.usage_cost_dict:\n        self.usage_cost_dict[mdl] = LLMTokenUsage()\n    counter = self.usage_cost_dict[mdl]\n    counter.prompt_tokens += prompts\n    counter.completion_tokens += completions\n    counter.cost += cost\n    counter.calls += 1\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.followup_to_standalone","title":"<code>followup_to_standalone(chat_history, question)</code>","text":"<p>Given a chat history and a question, convert it to a standalone question. Args:     chat_history: list of tuples of (question, answer)     query: follow-up question</p> <p>Returns: standalone version of the question</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def followup_to_standalone(\n    self, chat_history: List[Tuple[str, str]], question: str\n) -&gt; str:\n    \"\"\"\n    Given a chat history and a question, convert it to a standalone question.\n    Args:\n        chat_history: list of tuples of (question, answer)\n        query: follow-up question\n\n    Returns: standalone version of the question\n    \"\"\"\n    history = collate_chat_history(chat_history)\n\n    prompt = f\"\"\"\n    Given the conversationn below, and a follow-up question, rephrase the follow-up \n    question as a standalone question.\n\n    Chat history: {history}\n    Follow-up question: {question} \n    \"\"\".strip()\n    show_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-PROMPT= \")\n    standalone = self.generate(prompt=prompt, max_tokens=1024).message.strip()\n    show_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-RESPONSE= \")\n    return standalone\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_verbatim_extract_async","title":"<code>get_verbatim_extract_async(question, passage)</code>  <code>async</code>","text":"<p>Asynchronously, get verbatim extract from passage that is relevant to a question. Asynch allows parallel calls to the LLM API.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>async def get_verbatim_extract_async(self, question: str, passage: Document) -&gt; str:\n    \"\"\"\n    Asynchronously, get verbatim extract from passage\n    that is relevant to a question.\n    Asynch allows parallel calls to the LLM API.\n    \"\"\"\n    async with aiohttp.ClientSession():\n        templatized_prompt = EXTRACTION_PROMPT_GPT4\n        final_prompt = templatized_prompt.format(\n            question=question, content=passage.content\n        )\n        show_if_debug(final_prompt, \"EXTRACT-PROMPT= \")\n        final_extract = await self.agenerate(prompt=final_prompt, max_tokens=1024)\n        show_if_debug(final_extract.message.strip(), \"EXTRACT-RESPONSE= \")\n    return final_extract.message.strip()\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_verbatim_extracts","title":"<code>get_verbatim_extracts(question, passages)</code>","text":"<p>From each passage, extract verbatim text that is relevant to a question, using concurrent API calls to the LLM. Args:     question: question to be answered     passages: list of passages from which to extract relevant verbatim text     LLM: LanguageModel to use for generating the prompt and extract Returns:     list of verbatim extracts from passages that are relevant to question</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_verbatim_extracts(\n    self, question: str, passages: List[Document]\n) -&gt; List[Document]:\n    \"\"\"\n    From each passage, extract verbatim text that is relevant to a question,\n    using concurrent API calls to the LLM.\n    Args:\n        question: question to be answered\n        passages: list of passages from which to extract relevant verbatim text\n        LLM: LanguageModel to use for generating the prompt and extract\n    Returns:\n        list of verbatim extracts from passages that are relevant to question\n    \"\"\"\n    docs = asyncio.run(self._get_verbatim_extracts(question, passages))\n    return docs\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible Args:     question: question to answer     passages: list of <code>Document</code> objects each containing a possibly relevant         snippet, and metadata Returns:     a <code>Document</code> object containing the answer,     and metadata containing source citations</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_summary_answer(self, question: str, passages: List[Document]) -&gt; Document:\n    \"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n\n    \"\"\"\n\n    # Define an auxiliary function to transform the list of\n    # passages into a single string\n    def stringify_passages(passages: List[Document]) -&gt; str:\n        return \"\\n\".join(\n            [\n                f\"\"\"\n            Extract: {p.content}\n            Source: {p.metadata.source}\n            \"\"\"\n                for p in passages\n            ]\n        )\n\n    passages_str = stringify_passages(passages)\n    # Substitute Q and P into the templatized prompt\n\n    final_prompt = SUMMARY_ANSWER_PROMPT_GPT4.format(\n        question=f\"Question:{question}\", extracts=passages_str\n    )\n    show_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n    # Generate the final verbatim extract based on the final prompt\n    llm_response = self.generate(prompt=final_prompt, max_tokens=1024)\n    final_answer = llm_response.message.strip()\n    show_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\n    parts = final_answer.split(\"SOURCE:\", maxsplit=1)\n    if len(parts) &gt; 1:\n        content = parts[0].strip()\n        sources = parts[1].strip()\n    else:\n        content = final_answer\n        sources = \"\"\n    return Document(\n        content=content,\n        metadata={\n            \"source\": \"SOURCE: \" + sources,\n            \"cached\": llm_response.cached,\n        },\n    )\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.StreamingIfAllowed","title":"<code>StreamingIfAllowed(llm, stream=True)</code>","text":"<p>Context to temporarily enable or disable streaming, if allowed globally via <code>settings.stream</code></p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def __init__(self, llm: LanguageModel, stream: bool = True):\n    self.llm = llm\n    self.stream = stream\n</code></pre>"},{"location":"reference/language_models/config/","title":"config","text":"<p>langroid/language_models/config.py </p>"},{"location":"reference/language_models/openai_gpt/","title":"openai_gpt","text":"<p>langroid/language_models/openai_gpt.py </p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIChatModel","title":"<code>OpenAIChatModel</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Chat models</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAICompletionModel","title":"<code>OpenAICompletionModel</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Completion models</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig","title":"<code>OpenAIGPTConfig</code>","text":"<p>             Bases: <code>LLMConfig</code></p> <p>Class for any LLM with an OpenAI-like API: besides the OpenAI models this includes: (a) locally-served models behind an OpenAI-compatible API (b) non-local models, using a proxy adaptor lib like litellm that provides     an OpenAI-compatible API. We could rename this class to OpenAILikeConfig.</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig.create","title":"<code>create(prefix)</code>  <code>classmethod</code>","text":"<p>Create a config class whose params can be set via a desired prefix from the .env file or env vars. E.g., using <pre><code>OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\nollama_config = OllamaConfig()\n</code></pre> you can have a group of params prefixed by \"OLLAMA_\", to be used with models served via <code>ollama</code>. This way, you can maintain several setting-groups in your .env file, one per model type.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>@classmethod\ndef create(cls, prefix: str) -&gt; Type[\"OpenAIGPTConfig\"]:\n    \"\"\"Create a config class whose params can be set via a desired\n    prefix from the .env file or env vars.\n    E.g., using\n    ```python\n    OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\n    ollama_config = OllamaConfig()\n    ```\n    you can have a group of params prefixed by \"OLLAMA_\", to be used\n    with models served via `ollama`.\n    This way, you can maintain several setting-groups in your .env file,\n    one per model type.\n    \"\"\"\n\n    class DynamicConfig(OpenAIGPTConfig):\n        pass\n\n    DynamicConfig.Config.env_prefix = prefix.upper() + \"_\"\n\n    return DynamicConfig\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIResponse","title":"<code>OpenAIResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>OpenAI response model, either completion or chat.</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT","title":"<code>OpenAIGPT(config)</code>","text":"<p>             Bases: <code>LanguageModel</code></p> <p>Class for OpenAI LLMs</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, config: OpenAIGPTConfig):\n    \"\"\"\n    Args:\n        config: configuration for openai-gpt model\n    \"\"\"\n    super().__init__(config)\n    self.config: OpenAIGPTConfig = config\n    if settings.nofunc:\n        self.config.chat_model = OpenAIChatModel.GPT4_NOFUNC\n\n    # global override of chat_model,\n    # to allow quick testing with other models\n    if settings.chat_model != \"\":\n        self.config.chat_model = settings.chat_model\n\n    # if model name starts with \"litellm\",\n    # set the actual model name by stripping the \"litellm/\" prefix\n    # and set the litellm flag to True\n    if self.config.chat_model.startswith(\"litellm/\") or self.config.litellm:\n        self.config.litellm = True\n        self.api_base = self.config.api_base\n        if self.config.chat_model.startswith(\"litellm/\"):\n            # strip the \"litellm/\" prefix\n            self.config.chat_model = self.config.chat_model.split(\"/\", 1)[1]\n        # litellm/ollama/llama2 =&gt; ollama/llama2 for example\n    elif self.config.chat_model.startswith(\"local/\"):\n        # expect this to be of the form \"local/localhost:8000/v1\",\n        # depending on how the model is launched locally.\n        # In this case the model served locally behind an OpenAI-compatible API\n        # so we can just use `openai.*` methods directly,\n        # and don't need a adaptor library like litellm\n        self.config.litellm = False\n        # Extract the api_base from the model name after the \"local/\" prefix\n        self.api_base = \"http://\" + self.config.chat_model.split(\"/\", 1)[1]\n    else:\n        self.api_base = self.config.api_base\n\n    # NOTE: The api_key should be set in the .env file, or via\n    # an explicit `export OPENAI_API_KEY=xxx` or `setenv OPENAI_API_KEY xxx`\n    # Pydantic's BaseSettings will automatically pick it up from the\n    # .env file\n    self.api_key = config.api_key\n\n    self.cache: MomentoCache | RedisCache\n    if settings.cache_type == \"momento\":\n        config.cache_config = MomentoCacheConfig()\n        self.cache = MomentoCache(config.cache_config)\n    else:\n        config.cache_config = RedisCacheConfig()\n        self.cache = RedisCache(config.cache_config)\n\n    self.config._validate_litellm()\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.chat_context_length","title":"<code>chat_context_length()</code>","text":"<p>Context-length for chat-completion models/endpoints Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for chat-completion models/endpoints\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    model = (\n        self.config.completion_model\n        if self.config.use_completion_for_chat\n        else self.config.chat_model\n    )\n    return _context_length.get(model, super().chat_context_length())\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.completion_context_length","title":"<code>completion_context_length()</code>","text":"<p>Context-length for completion models/endpoints Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def completion_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for completion models/endpoints\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    model = (\n        self.config.chat_model\n        if self.config.use_chat_for_completion\n        else self.config.completion_model\n    )\n    return _context_length.get(model, super().completion_context_length())\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.chat_cost","title":"<code>chat_cost()</code>","text":"<p>(Prompt, Generation) cost per 1000 tokens, for chat-completion models/endpoints. Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_cost(self) -&gt; Tuple[float, float]:\n    \"\"\"\n    (Prompt, Generation) cost per 1000 tokens, for chat-completion\n    models/endpoints.\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    return _cost_per_1k_tokens.get(self.config.chat_model, super().chat_cost())\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.set_stream","title":"<code>set_stream(stream)</code>","text":"<p>Enable or disable streaming output from API. Args:     stream: enable streaming output from API Returns: previous value of stream</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def set_stream(self, stream: bool) -&gt; bool:\n    \"\"\"Enable or disable streaming output from API.\n    Args:\n        stream: enable streaming output from API\n    Returns: previous value of stream\n    \"\"\"\n    tmp = self.config.stream\n    self.config.stream = stream\n    return tmp\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.get_stream","title":"<code>get_stream()</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def get_stream(self) -&gt; bool:\n    \"\"\"Get streaming status\"\"\"\n    return self.config.stream\n</code></pre>"},{"location":"reference/language_models/utils/","title":"utils","text":"<p>langroid/language_models/utils.py </p>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.retry_with_exponential_backoff","title":"<code>retry_with_exponential_backoff(func, initial_delay=1, exponential_base=2, jitter=True, max_retries=10, errors=(requests.exceptions.RequestException, openai.error.Timeout, openai.error.RateLimitError, openai.error.APIError, openai.error.ServiceUnavailableError, openai.error.TryAgain, aiohttp.ServerTimeoutError, asyncio.TimeoutError))</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def retry_with_exponential_backoff(\n    func: Callable[..., Any],\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (  # type: ignore\n        requests.exceptions.RequestException,\n        openai.error.Timeout,\n        openai.error.RateLimitError,\n        openai.error.APIError,\n        openai.error.ServiceUnavailableError,\n        openai.error.TryAgain,\n        aiohttp.ServerTimeoutError,\n        asyncio.TimeoutError,\n    ),\n) -&gt; Callable[..., Any]:\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n\n    def wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n\n            except openai.error.InvalidRequestError as e:\n                # do not retry when the request itself is invalid,\n                # e.g. when context is too long\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n\n            # Retry on specified errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n\n                # Check if max retries has been reached\n                if num_retries &gt; max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n\n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n                logger.warning(\n                    f\"\"\"OpenAI API request failed with error: \n                    {e}. \n                    Retrying in {delay} seconds...\"\"\"\n                )\n                # Sleep for the delay\n                time.sleep(delay)\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n</code></pre>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.async_retry_with_exponential_backoff","title":"<code>async_retry_with_exponential_backoff(func, initial_delay=1, exponential_base=2, jitter=True, max_retries=10, errors=(openai.error.Timeout, openai.error.RateLimitError, openai.error.APIError, openai.error.ServiceUnavailableError, openai.error.TryAgain, aiohttp.ServerTimeoutError, asyncio.TimeoutError))</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def async_retry_with_exponential_backoff(\n    func: Callable[..., Any],\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (  # type: ignore\n        openai.error.Timeout,\n        openai.error.RateLimitError,\n        openai.error.APIError,\n        openai.error.ServiceUnavailableError,\n        openai.error.TryAgain,\n        aiohttp.ServerTimeoutError,\n        asyncio.TimeoutError,\n    ),\n) -&gt; Callable[..., Any]:\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n\n    async def wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or exception is raised\n        while True:\n            try:\n                result = await func(*args, **kwargs)\n                return result\n\n            except openai.error.InvalidRequestError as e:\n                # do not retry when the request itself is invalid,\n                # e.g. when context is too long\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n\n            # Retry on specified errors\n            except errors as e:\n                # Increment retries\n                num_retries += 1\n\n                # Check if max retries has been reached\n                if num_retries &gt; max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n\n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n                logger.warning(\n                    f\"\"\"OpenAI API request failed with error{e}. \n                    Retrying in {delay} seconds...\"\"\"\n                )\n                # Sleep for the delay\n                time.sleep(delay)\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/","title":"prompt_formatter","text":"<p>langroid/language_models/prompt_formatter/init.py </p>"},{"location":"reference/language_models/prompt_formatter/base/","title":"base","text":"<p>langroid/language_models/prompt_formatter/base.py </p>"},{"location":"reference/language_models/prompt_formatter/base/#langroid.language_models.prompt_formatter.base.PromptFormatter","title":"<code>PromptFormatter(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a prompt formatter</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/base/#langroid.language_models.prompt_formatter.base.PromptFormatter.format","title":"<code>format(messages)</code>  <code>abstractmethod</code>","text":"<p>Convert sequence of messages (system, user, assistant, user, assistant...user)     to a single prompt formatted according to the specific format type,     to be used in a /completions endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>chat history as a sequence of messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>formatted version of chat history</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>@abstractmethod\ndef format(self, messages: List[LLMMessage]) -&gt; str:\n    \"\"\"\n    Convert sequence of messages (system, user, assistant, user, assistant...user)\n        to a single prompt formatted according to the specific format type,\n        to be used in a /completions endpoint.\n\n    Args:\n        messages (List[LLMMessage]): chat history as a sequence of messages\n\n    Returns:\n        (str): formatted version of chat history\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/llama2_formatter/","title":"llama2_formatter","text":"<p>langroid/language_models/prompt_formatter/llama2_formatter.py </p>"},{"location":"reference/language_models/prompt_formatter/llama2_formatter/#langroid.language_models.prompt_formatter.llama2_formatter.Llama2Formatter","title":"<code>Llama2Formatter(config)</code>","text":"<p>             Bases: <code>PromptFormatter</code></p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/parsing/","title":"parsing","text":"<p>langroid/parsing/init.py </p>"},{"location":"reference/parsing/agent_chats/","title":"agent_chats","text":"<p>langroid/parsing/agent_chats.py </p>"},{"location":"reference/parsing/agent_chats/#langroid.parsing.agent_chats.parse_message","title":"<code>parse_message(msg)</code>","text":"<p>Parse the intended recipient and content of a message. Message format is assumed to be TO[]:. The TO[]: part is optional. <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to parse</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>str, str: task-name of intended recipient, and content of message (if recipient is not specified, task-name is empty string)</p> Source code in <code>langroid/parsing/agent_chats.py</code> <pre><code>@no_type_check\ndef parse_message(msg: str) -&gt; Tuple[str, str]:\n    \"\"\"\n    Parse the intended recipient and content of a message.\n    Message format is assumed to be TO[&lt;recipient&gt;]:&lt;message&gt;.\n    The TO[&lt;recipient&gt;]: part is optional.\n\n    Args:\n        msg (str): message to parse\n\n    Returns:\n        str, str: task-name of intended recipient, and content of message\n            (if recipient is not specified, task-name is empty string)\n\n    \"\"\"\n    if msg is None:\n        return \"\", \"\"\n\n    # Grammar definition\n    name = Word(alphanums)\n    to_start = Literal(\"TO[\").suppress()\n    to_end = Literal(\"]:\").suppress()\n    to_field = (to_start + name(\"name\") + to_end) | Empty().suppress()\n    message = SkipTo(StringEnd())(\"text\")\n\n    # Parser definition\n    parser = to_field + message\n\n    try:\n        parsed = parser.parseString(msg)\n        return parsed.name, parsed.text\n    except ParseException:\n        return \"\", msg\n</code></pre>"},{"location":"reference/parsing/code_parser/","title":"code_parser","text":"<p>langroid/parsing/code_parser.py </p>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser","title":"<code>CodeParser(config)</code>","text":"Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def __init__(self, config: CodeParsingConfig):\n    self.config = config\n    self.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.num_tokens","title":"<code>num_tokens(text)</code>","text":"<p>How many tokens are in the text, according to the tokenizer. This needs to be accurate, otherwise we may exceed the maximum number of tokens allowed by the model. Args:     text: string to tokenize Returns:     number of tokens in the text</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def num_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    How many tokens are in the text, according to the tokenizer.\n    This needs to be accurate, otherwise we may exceed the maximum\n    number of tokens allowed by the model.\n    Args:\n        text: string to tokenize\n    Returns:\n        number of tokens in the text\n    \"\"\"\n    tokens = self.tokenizer.encode(text)\n    return len(tokens)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.split","title":"<code>split(docs)</code>","text":"<p>Split the documents into chunks, according to the config.splitter. Only the documents with a language in the config.extensions are split.</p> <p>Note</p> <p>We assume the metadata in each document has at least a <code>language</code> field, which is used to determine how to chunk the code.</p> <p>Args:     docs: list of documents to split Returns:     list of documents, where each document is a chunk; the metadata of the     original document is duplicated for each chunk, so that when we retrieve a     chunk, we immediately know info about the original document.</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def split(self, docs: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Split the documents into chunks, according to the config.splitter.\n    Only the documents with a language in the config.extensions are split.\n    !!! note\n        We assume the metadata in each document has at least a `language` field,\n        which is used to determine how to chunk the code.\n    Args:\n        docs: list of documents to split\n    Returns:\n        list of documents, where each document is a chunk; the metadata of the\n        original document is duplicated for each chunk, so that when we retrieve a\n        chunk, we immediately know info about the original document.\n    \"\"\"\n    chunked_docs = [\n        [\n            Document(content=chunk, metadata=d.metadata)\n            for chunk in chunk_code(\n                d.content,\n                d.metadata.language,  # type: ignore\n                self.config.chunk_size,\n                self.num_tokens,\n            )\n            if chunk.strip() != \"\"\n        ]\n        for d in docs\n        if d.metadata.language in self.config.extensions  # type: ignore\n    ]\n    # collapse the list of lists into a single list\n    return reduce(lambda x, y: x + y, chunked_docs)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.chunk_code","title":"<code>chunk_code(code, language, max_tokens, len_fn)</code>","text":"<p>Chunk code into smaller pieces, so that we don't exceed the maximum number of tokens allowed by the embedding model. Args:     code: string of code     language: str as a file extension, e.g. \"py\", \"yml\"     max_tokens: max tokens per chunk     len_fn: function to get the length of a string in token units Returns:</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def chunk_code(\n    code: str, language: str, max_tokens: int, len_fn: Callable[[str], int]\n) -&gt; List[str]:\n    \"\"\"\n    Chunk code into smaller pieces, so that we don't exceed the maximum\n    number of tokens allowed by the embedding model.\n    Args:\n        code: string of code\n        language: str as a file extension, e.g. \"py\", \"yml\"\n        max_tokens: max tokens per chunk\n        len_fn: function to get the length of a string in token units\n    Returns:\n\n    \"\"\"\n    lexer = get_lexer_by_name(language)\n    tokens = list(lex(code, lexer))\n\n    chunks = []\n    current_chunk = \"\"\n    for token_type, token_value in tokens:\n        if token_type in Token.Text.Whitespace:\n            current_chunk += token_value\n        else:\n            token_tokens = len_fn(token_value)\n            if len_fn(current_chunk) + token_tokens &lt;= max_tokens:\n                current_chunk += token_value\n            else:\n                chunks.append(current_chunk)\n                current_chunk = token_value\n\n    if current_chunk:\n        chunks.append(current_chunk)\n\n    return chunks\n</code></pre>"},{"location":"reference/parsing/document_parser/","title":"document_parser","text":"<p>langroid/parsing/document_parser.py </p>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser","title":"<code>DocumentParser(source, config)</code>","text":"<p>             Bases: <code>Parser</code></p> <p>Abstract base class for extracting text from special types of docs such as PDFs or Docx.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The source, either a URL or a file path.</p> <code>doc_bytes</code> <code>BytesIO</code> <p>BytesIO object containing the doc data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the PDF, either a URL or a file path.</p> required Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str, config: ParsingConfig):\n    \"\"\"\n    Initialize the PDFParser.\n\n    Args:\n        source (str): The source of the PDF, either a URL or a file path.\n    \"\"\"\n    super().__init__(config)\n    self.source = source\n    self.config = config\n    self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.create","title":"<code>create(source, config)</code>  <code>classmethod</code>","text":"<p>Create a DocumentParser instance based on source type     and config..library specified. <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the PDF, either a URL or a file path.</p> required <code>config</code> <code>ParserConfig</code> <p>The parser configuration.</p> required <p>Returns:</p> Name Type Description <code>DocumentParser</code> <code>DocumentParser</code> <p>An instance of a DocumentParser subclass.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>@classmethod\ndef create(cls, source: str, config: ParsingConfig) -&gt; \"DocumentParser\":\n    \"\"\"\n    Create a DocumentParser instance based on source type\n        and config.&lt;source_type&gt;.library specified.\n\n    Args:\n        source (str): The source of the PDF, either a URL or a file path.\n        config (ParserConfig): The parser configuration.\n\n    Returns:\n        DocumentParser: An instance of a DocumentParser subclass.\n    \"\"\"\n    if DocumentParser._document_type(source) == DocumentType.PDF:\n        if config.pdf.library == \"fitz\":\n            return FitzPDFParser(source, config)\n        elif config.pdf.library == \"pypdf\":\n            return PyPDFParser(source, config)\n        elif config.pdf.library == \"pdfplumber\":\n            return PDFPlumberParser(source, config)\n        elif config.pdf.library == \"unstructured\":\n            return UnstructuredPDFParser(source, config)\n        elif config.pdf.library == \"haystack\":\n            return HaystackPDFParser(source, config)\n        else:\n            raise ValueError(\n                f\"Unsupported PDF library specified: {config.pdf.library}\"\n            )\n    elif DocumentParser._document_type(source) == DocumentType.DOCX:\n        if config.docx.library == \"unstructured\":\n            return UnstructuredDocxParser(source, config)\n        else:\n            raise ValueError(\n                f\"Unsupported DOCX library specified: {config.docx.library}\"\n            )\n    else:\n        raise ValueError(f\"Unsupported document type: {source}\")\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, Any], None, None]:\n    \"\"\"Yield each page in the PDF.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: Any) -&gt; str:\n    \"\"\"Extract text from a given page.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.fix_text","title":"<code>fix_text(text)</code>","text":"<p>Fix text extracted from a PDF.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The extracted text.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The fixed text.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def fix_text(self, text: str) -&gt; str:\n    \"\"\"\n    Fix text extracted from a PDF.\n\n    Args:\n        text (str): The extracted text.\n\n    Returns:\n        str: The fixed text.\n    \"\"\"\n    # Some pdf parsers introduce extra space before hyphen,\n    # so use regular expression to replace 'space-hyphen' with just 'hyphen'\n    return re.sub(r\" +\\-\", \"-\", text)\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.get_doc","title":"<code>get_doc()</code>","text":"<p>Get entire text from pdf source as a single document.</p> <p>Returns:</p> Type Description <code>Document</code> <p>a <code>Document</code> object containing the content of the pdf file, and metadata containing source name (URL or path)</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_doc(self) -&gt; Document:\n    \"\"\"\n    Get entire text from pdf source as a single document.\n\n    Returns:\n        a `Document` object containing the content of the pdf file,\n            and metadata containing source name (URL or path)\n    \"\"\"\n\n    text = \"\".join(\n        [self.extract_text_from_page(page) for _, page in self.iterate_pages()]\n    )\n    return Document(content=text, metadata=DocMetaData(source=self.source))\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.get_doc_chunks","title":"<code>get_doc_chunks()</code>","text":"<p>Get document chunks from a pdf source, with page references in the document metadata.</p> <p>Adapted from https://github.com/whitead/paper-qa/blob/main/paperqa/readers.py</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: a list of <code>Document</code> objects, each containing a chunk of text</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_doc_chunks(self) -&gt; List[Document]:\n    \"\"\"\n    Get document chunks from a pdf source,\n    with page references in the document metadata.\n\n    Adapted from\n    https://github.com/whitead/paper-qa/blob/main/paperqa/readers.py\n\n    Returns:\n        List[Document]: a list of `Document` objects,\n            each containing a chunk of text\n    \"\"\"\n\n    split = []  # tokens in curr split\n    pages: List[str] = []\n    docs: List[Document] = []\n    for i, page in self.iterate_pages():\n        page_text = self.extract_text_from_page(page)\n        split += self.tokenizer.encode(page_text)\n        pages.append(str(i + 1))\n        # split could be so long it needs to be split\n        # into multiple chunks. Or it could be so short\n        # that it needs to be combined with the next chunk.\n        while len(split) &gt; self.config.chunk_size:\n            # pretty formatting of pages (e.g. 1-3, 4, 5-7)\n            pg = \"-\".join([pages[0], pages[-1]])\n            text = self.tokenizer.decode(split[: self.config.chunk_size])\n            docs.append(\n                Document(\n                    content=text,\n                    metadata=DocMetaData(\n                        source=f\"{self.source} pages {pg}\",\n                        is_chunk=True,\n                    ),\n                )\n            )\n            split = split[self.config.chunk_size - self.config.overlap :]\n            pages = [str(i + 1)]\n    if len(split) &gt; self.config.overlap:\n        pg = \"-\".join([pages[0], pages[-1]])\n        text = self.tokenizer.decode(split[: self.config.chunk_size])\n        docs.append(\n            Document(\n                content=text,\n                metadata=DocMetaData(\n                    source=f\"{self.source} pages {pg}\",\n                    is_chunk=True,\n                ),\n            )\n        )\n    self.add_window_ids(docs)\n    return docs\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.FitzPDFParser","title":"<code>FitzPDFParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>fitz</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the PDF, either a URL or a file path.</p> required Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str, config: ParsingConfig):\n    \"\"\"\n    Initialize the PDFParser.\n\n    Args:\n        source (str): The source of the PDF, either a URL or a file path.\n    \"\"\"\n    super().__init__(config)\n    self.source = source\n    self.config = config\n    self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.FitzPDFParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>fitz</code>.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, Page], None, None]</code> <p>Generator[fitz.Page]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, fitz.Page], None, None]:\n    \"\"\"\n    Yield each page in the PDF using `fitz`.\n\n    Returns:\n        Generator[fitz.Page]: Generator yielding each page.\n    \"\"\"\n    doc = fitz.open(stream=self.doc_bytes, filetype=\"pdf\")\n    for i, page in enumerate(doc):\n        yield i, page\n    doc.close()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.FitzPDFParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>fitz</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The <code>fitz</code> page object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: fitz.Page) -&gt; str:\n    \"\"\"\n    Extract text from a given `fitz` page.\n\n    Args:\n        page (fitz.Page): The `fitz` page object.\n\n    Returns:\n        str: Extracted text from the page.\n    \"\"\"\n    return self.fix_text(page.get_text())\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyPDFParser","title":"<code>PyPDFParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>pypdf</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the PDF, either a URL or a file path.</p> required Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str, config: ParsingConfig):\n    \"\"\"\n    Initialize the PDFParser.\n\n    Args:\n        source (str): The source of the PDF, either a URL or a file path.\n    \"\"\"\n    super().__init__(config)\n    self.source = source\n    self.config = config\n    self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyPDFParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>pypdf</code>.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, PageObject], None, None]</code> <p>Generator[pypdf.pdf.PageObject]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, pypdf.PageObject], None, None]:\n    \"\"\"\n    Yield each page in the PDF using `pypdf`.\n\n    Returns:\n        Generator[pypdf.pdf.PageObject]: Generator yielding each page.\n    \"\"\"\n    reader = pypdf.PdfReader(self.doc_bytes)\n    for i, page in enumerate(reader.pages):\n        yield i, page\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyPDFParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>pypdf</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>PageObject</code> <p>The <code>pypdf</code> page object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: pypdf.PageObject) -&gt; str:\n    \"\"\"\n    Extract text from a given `pypdf` page.\n\n    Args:\n        page (pypdf.pdf.PageObject): The `pypdf` page object.\n\n    Returns:\n        str: Extracted text from the page.\n    \"\"\"\n    return self.fix_text(page.extract_text())\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PDFPlumberParser","title":"<code>PDFPlumberParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>pdfplumber</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the PDF, either a URL or a file path.</p> required Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str, config: ParsingConfig):\n    \"\"\"\n    Initialize the PDFParser.\n\n    Args:\n        source (str): The source of the PDF, either a URL or a file path.\n    \"\"\"\n    super().__init__(config)\n    self.source = source\n    self.config = config\n    self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PDFPlumberParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>pdfplumber</code>.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, Page], None, None]</code> <p>Generator[pdfplumber.Page]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(\n    self,\n) -&gt; (Generator)[Tuple[int, pdfplumber.pdf.Page], None, None]:  # type: ignore\n    \"\"\"\n    Yield each page in the PDF using `pdfplumber`.\n\n    Returns:\n        Generator[pdfplumber.Page]: Generator yielding each page.\n    \"\"\"\n    with pdfplumber.open(self.doc_bytes) as pdf:\n        for i, page in enumerate(pdf.pages):\n            yield i, page\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PDFPlumberParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>pdfplumber</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The <code>pdfplumber</code> page object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: pdfplumber.pdf.Page) -&gt; str:  # type: ignore\n    \"\"\"\n    Extract text from a given `pdfplumber` page.\n\n    Args:\n        page (pdfplumber.Page): The `pdfplumber` page object.\n\n    Returns:\n        str: Extracted text from the page.\n    \"\"\"\n    return self.fix_text(page.extract_text())\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.HaystackPDFParser","title":"<code>HaystackPDFParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>haystack</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the PDF, either a URL or a file path.</p> required Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str, config: ParsingConfig):\n    \"\"\"\n    Initialize the PDFParser.\n\n    Args:\n        source (str): The source of the PDF, either a URL or a file path.\n    \"\"\"\n    super().__init__(config)\n    self.source = source\n    self.config = config\n    self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.HaystackPDFParser.get_doc_chunks","title":"<code>get_doc_chunks()</code>","text":"<p>Overrides the base class method to use the <code>haystack</code> library. See there for more details.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_doc_chunks(self) -&gt; List[Document]:\n    \"\"\"\n    Overrides the base class method to use the `haystack` library.\n    See there for more details.\n    \"\"\"\n\n    from haystack.nodes import PDFToTextConverter, PreProcessor\n\n    converter = PDFToTextConverter(\n        remove_numeric_tables=True,\n    )\n    path = self.source\n    if path.startswith((\"http://\", \"https://\")):\n        path = url_to_tempfile(path)\n    doc = converter.convert(file_path=path, meta=None)\n    # note self.config.chunk_size is in token units,\n    # and we use an approximation of 75 words per 100 tokens\n    # to convert to word units\n    preprocessor = PreProcessor(\n        clean_empty_lines=True,\n        clean_whitespace=True,\n        clean_header_footer=False,\n        split_by=\"word\",\n        split_length=int(0.75 * self.config.chunk_size),\n        split_overlap=int(0.75 * self.config.overlap),\n        split_respect_sentence_boundary=True,\n        add_page_number=True,\n    )\n    chunks = preprocessor.process(doc)\n    return [\n        Document(\n            content=chunk.content,\n            metadata=DocMetaData(\n                source=f\"{self.source} page {chunk.meta['page']}\",\n                is_chunk=True,\n            ),\n        )\n        for chunk in chunks\n    ]\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredPDFParser","title":"<code>UnstructuredPDFParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing PDF files using the <code>unstructured</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the PDF, either a URL or a file path.</p> required Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str, config: ParsingConfig):\n    \"\"\"\n    Initialize the PDFParser.\n\n    Args:\n        source (str): The source of the PDF, either a URL or a file path.\n    \"\"\"\n    super().__init__(config)\n    self.source = source\n    self.config = config\n    self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredPDFParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>unstructured</code> element.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>unstructured element</code> <p>The <code>unstructured</code> element object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the element.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: Any) -&gt; str:\n    \"\"\"\n    Extract text from a given `unstructured` element.\n\n    Args:\n        page (unstructured element): The `unstructured` element object.\n\n    Returns:\n        str: Extracted text from the element.\n    \"\"\"\n    text = \" \".join(el.text for el in page)\n    return self.fix_text(text)\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredDocxParser","title":"<code>UnstructuredDocxParser(source, config)</code>","text":"<p>             Bases: <code>DocumentParser</code></p> <p>Parser for processing DOCX files using the <code>unstructured</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the PDF, either a URL or a file path.</p> required Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str, config: ParsingConfig):\n    \"\"\"\n    Initialize the PDFParser.\n\n    Args:\n        source (str): The source of the PDF, either a URL or a file path.\n    \"\"\"\n    super().__init__(config)\n    self.source = source\n    self.config = config\n    self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredDocxParser.extract_text_from_page","title":"<code>extract_text_from_page(page)</code>","text":"<p>Extract text from a given <code>unstructured</code> element.</p> Note <p>The concept of \"pages\" doesn't actually exist in the .docx file format in the same way it does in formats like .pdf. A .docx file is made up of a series of elements like paragraphs and tables, but the division into pages is done dynamically based on the rendering settings (like the page size, margin size, font size, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>unstructured element</code> <p>The <code>unstructured</code> element object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted text from the element.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def extract_text_from_page(self, page: Any) -&gt; str:\n    \"\"\"\n    Extract text from a given `unstructured` element.\n\n    Note:\n        The concept of \"pages\" doesn't actually exist in the .docx file format in\n        the same way it does in formats like .pdf. A .docx file is made up of a\n        series of elements like paragraphs and tables, but the division into\n        pages is done dynamically based on the rendering settings (like the page\n        size, margin size, font size, etc.).\n\n    Args:\n        page (unstructured element): The `unstructured` element object.\n\n    Returns:\n        str: Extracted text from the element.\n    \"\"\"\n    text = \" \".join(el.text for el in page)\n    return self.fix_text(text)\n</code></pre>"},{"location":"reference/parsing/json/","title":"json","text":"<p>langroid/parsing/json.py </p>"},{"location":"reference/parsing/json/#langroid.parsing.json.is_valid_json","title":"<code>is_valid_json(json_str)</code>","text":"<p>Check if the input string is a valid JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The input string to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the input string is a valid JSON, False otherwise.</p> Source code in <code>langroid/parsing/json.py</code> <pre><code>def is_valid_json(json_str: str) -&gt; bool:\n    \"\"\"Check if the input string is a valid JSON.\n\n    Args:\n        json_str (str): The input string to check.\n\n    Returns:\n        bool: True if the input string is a valid JSON, False otherwise.\n    \"\"\"\n    try:\n        json.loads(json_str)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/parsing/json/#langroid.parsing.json.extract_top_level_json","title":"<code>extract_top_level_json(s)</code>","text":"<p>Extract all top-level JSON-formatted substrings from a given string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string to search for JSON substrings.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of top-level JSON-formatted substrings.</p> Source code in <code>langroid/parsing/json.py</code> <pre><code>def extract_top_level_json(s: str) -&gt; List[str]:\n    \"\"\"Extract all top-level JSON-formatted substrings from a given string.\n\n    Args:\n        s (str): The input string to search for JSON substrings.\n\n    Returns:\n        List[str]: A list of top-level JSON-formatted substrings.\n    \"\"\"\n    # Find JSON object and array candidates using regular expressions\n    json_candidates = regex.findall(r\"(?&lt;!\\\\)(?:\\\\\\\\)*\\{(?:[^{}]|(?R))*\\}\", s)\n\n    top_level_jsons = [\n        candidate for candidate in json_candidates if is_valid_json(candidate)\n    ]\n\n    return top_level_jsons\n</code></pre>"},{"location":"reference/parsing/json/#langroid.parsing.json.top_level_json_field","title":"<code>top_level_json_field(s, f)</code>","text":"<p>Extract the value of a field f from a top-level JSON object. If there are multiple, just return the first.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string to search for JSON substrings.</p> required <code>f</code> <code>str</code> <p>The field to extract from the JSON object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Any</code> <p>The value of the field f in the top-level JSON object, if any. Otherwise, return an empty string.</p> Source code in <code>langroid/parsing/json.py</code> <pre><code>def top_level_json_field(s: str, f: str) -&gt; Any:\n    \"\"\"\n    Extract the value of a field f from a top-level JSON object.\n    If there are multiple, just return the first.\n\n    Args:\n        s (str): The input string to search for JSON substrings.\n        f (str): The field to extract from the JSON object.\n\n    Returns:\n        str: The value of the field f in the top-level JSON object, if any.\n            Otherwise, return an empty string.\n    \"\"\"\n\n    jsons = extract_top_level_json(s)\n    if len(jsons) == 0:\n        return \"\"\n    for j in jsons:\n        json_data = json.loads(j)\n        if f in json_data:\n            return json_data[f]\n\n    return \"\"\n</code></pre>"},{"location":"reference/parsing/para_sentence_split/","title":"para_sentence_split","text":"<p>langroid/parsing/para_sentence_split.py </p>"},{"location":"reference/parsing/parser/","title":"parser","text":"<p>langroid/parsing/parser.py </p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser","title":"<code>Parser(config)</code>","text":"Source code in <code>langroid/parsing/parser.py</code> <pre><code>def __init__(self, config: ParsingConfig):\n    self.config = config\n    self.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser.add_window_ids","title":"<code>add_window_ids(chunks)</code>","text":"<p>Chunks are consecutive parts of a single original document. Add window_ids in metadata</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def add_window_ids(self, chunks: List[Document]) -&gt; None:\n    \"\"\"Chunks are consecutive parts of a single original document.\n    Add window_ids in metadata\"\"\"\n\n    # The original metadata.id (if any) is ignored since it will be same for all\n    # chunks and is useless. We want a distinct id for each chunk.\n    ids = [Document.hash_id(str(c)) for c in chunks]\n\n    k = self.config.n_neighbor_ids\n    n = len(ids)\n    window_ids = [ids[max(0, i - k) : min(n, i + k + 1)] for i in range(n)]\n    for i, c in enumerate(chunks):\n        if c.content.strip() == \"\":\n            continue\n        c.metadata.window_ids = window_ids[i]\n        c.metadata.id = ids[i]\n        c.metadata.is_chunk = True\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser.chunk_tokens","title":"<code>chunk_tokens(text)</code>","text":"<p>Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries. Adapted from https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to split into chunks.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of text chunks, each of which is a string of tokens</p> <code>List[str]</code> <p>roughly self.config.chunk_size tokens long.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def chunk_tokens(\n    self,\n    text: str,\n) -&gt; List[str]:\n    \"\"\"\n    Split a text into chunks of ~CHUNK_SIZE tokens,\n    based on punctuation and newline boundaries.\n    Adapted from\n    https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py\n\n    Args:\n        text: The text to split into chunks.\n\n    Returns:\n        A list of text chunks, each of which is a string of tokens\n        roughly self.config.chunk_size tokens long.\n    \"\"\"\n    # Return an empty list if the text is empty or whitespace\n    if not text or text.isspace():\n        return []\n\n    # Tokenize the text\n    tokens = self.tokenizer.encode(text, disallowed_special=())\n\n    # Initialize an empty list of chunks\n    chunks = []\n\n    # Initialize a counter for the number of chunks\n    num_chunks = 0\n\n    # Loop until all tokens are consumed\n    while tokens and num_chunks &lt; self.config.max_chunks:\n        # Take the first chunk_size tokens as a chunk\n        chunk = tokens[: self.config.chunk_size]\n\n        # Decode the chunk into text\n        chunk_text = self.tokenizer.decode(chunk)\n\n        # Skip the chunk if it is empty or whitespace\n        if not chunk_text or chunk_text.isspace():\n            # Remove the tokens corresponding to the chunk text\n            # from remaining tokens\n            tokens = tokens[len(chunk) :]\n            # Continue to the next iteration of the loop\n            continue\n\n        # Find the last period or punctuation mark in the chunk\n        last_punctuation = max(\n            chunk_text.rfind(\".\"),\n            chunk_text.rfind(\"?\"),\n            chunk_text.rfind(\"!\"),\n            chunk_text.rfind(\"\\n\"),\n        )\n\n        # If there is a punctuation mark, and the last punctuation index is\n        # after MIN_CHUNK_SIZE_CHARS\n        if (\n            last_punctuation != -1\n            and last_punctuation &gt; self.config.min_chunk_chars\n        ):\n            # Truncate the chunk text at the punctuation mark\n            chunk_text = chunk_text[: last_punctuation + 1]\n\n        # Remove any newline characters and strip any leading or\n        # trailing whitespace\n        chunk_text_to_append = chunk_text.replace(\"\\n\", \" \").strip()\n\n        if len(chunk_text_to_append) &gt; self.config.discard_chunk_chars:\n            # Append the chunk text to the list of chunks\n            chunks.append(chunk_text_to_append)\n\n        # Remove the tokens corresponding to the chunk text\n        # from the remaining tokens\n        tokens = tokens[\n            len(self.tokenizer.encode(chunk_text, disallowed_special=())) :\n        ]\n\n        # Increment the number of chunks\n        num_chunks += 1\n\n    # There may be remaining tokens, but we discard them\n    # since we have already reached the maximum number of chunks\n\n    return chunks\n</code></pre>"},{"location":"reference/parsing/repo_loader/","title":"repo_loader","text":"<p>langroid/parsing/repo_loader.py </p>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoaderConfig","title":"<code>RepoLoaderConfig</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Configuration for RepoLoader.</p>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader","title":"<code>RepoLoader(url, config=RepoLoaderConfig())</code>","text":"<p>Class for recursively getting all file content in a repo.</p> <pre><code>config: configuration for RepoLoader\n</code></pre> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    config: RepoLoaderConfig = RepoLoaderConfig(),\n):\n    \"\"\"\n    Args:\n        url: full github url of repo, or just \"owner/repo\"\n        config: configuration for RepoLoader\n    \"\"\"\n    self.url = url\n    self.config = config\n    self.clone_path: Optional[str] = None\n    self.log_file = \".logs/repo_loader/download_log.json\"\n    os.makedirs(os.path.dirname(self.log_file), exist_ok=True)\n    if not os.path.exists(self.log_file):\n        with open(self.log_file, \"w\") as f:\n            json.dump({\"junk\": \"ignore\"}, f)\n    with open(self.log_file, \"r\") as f:\n        log = json.load(f)\n    if self.url in log and os.path.exists(log[self.url]):\n        logger.info(f\"Repo Already downloaded in {log[self.url]}\")\n        self.clone_path = log[self.url]\n\n    if \"github.com\" in self.url:\n        repo_name = self.url.split(\"github.com/\")[1]\n    else:\n        repo_name = self.url\n    load_dotenv()\n    # authenticated calls to github api have higher rate limit\n    token = os.getenv(\"GITHUB_ACCESS_TOKEN\")\n    g = Github(token)\n    self.repo = self._get_repo_with_retry(g, repo_name)\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.clone","title":"<code>clone(path=None)</code>","text":"<p>Clone a GitHub repository to a local directory specified by <code>path</code>, if it has not already been cloned.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local directory where the repository should be cloned. If not specified, a temporary directory will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The path to the local directory where the repository was cloned.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def clone(self, path: Optional[str] = None) -&gt; Optional[str]:\n    \"\"\"\n    Clone a GitHub repository to a local directory specified by `path`,\n    if it has not already been cloned.\n\n    Args:\n        path (str): The local directory where the repository should be cloned.\n            If not specified, a temporary directory will be created.\n\n    Returns:\n        str: The path to the local directory where the repository was cloned.\n    \"\"\"\n    with open(self.log_file, \"r\") as f:\n        log: Dict[str, str] = json.load(f)\n\n    if (\n        self.url in log\n        and os.path.exists(log[self.url])\n        and _has_files(log[self.url])\n    ):\n        logger.warning(f\"Repo Already downloaded in {log[self.url]}\")\n        self.clone_path = log[self.url]\n        return self.clone_path\n\n    self.clone_path = path\n    if path is None:\n        path = self.default_clone_path()\n        self.clone_path = path\n\n    try:\n        subprocess.run([\"git\", \"clone\", self.url, path], check=True)\n        log[self.url] = path\n        with open(self.log_file, \"w\") as f:\n            json.dump(log, f)\n        return self.clone_path\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Git clone failed: {e}\")\n    except Exception as e:\n        logger.error(f\"An error occurred while trying to clone the repository:{e}\")\n\n    return self.clone_path\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_tree_from_github","title":"<code>load_tree_from_github(depth, lines=0)</code>","text":"<p>Get a nested dictionary of GitHub repository file and directory names up to a certain depth, with file contents.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>The depth level.</p> required <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]:</p> <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>A dictionary containing file and directory names, with file contents.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_tree_from_github(\n    self, depth: int, lines: int = 0\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n    \"\"\"\n    Get a nested dictionary of GitHub repository file and directory names\n    up to a certain depth, with file contents.\n\n    Args:\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n\n    Returns:\n        Dict[str, Union[str, List[Dict]]]:\n        A dictionary containing file and directory names, with file contents.\n    \"\"\"\n    root_contents = self.repo.get_contents(\"\")\n    if not isinstance(root_contents, list):\n        root_contents = [root_contents]\n    repo_structure = {\n        \"type\": \"dir\",\n        \"name\": \"\",\n        \"dirs\": [],\n        \"files\": [],\n        \"path\": \"\",\n    }\n\n    # A queue of tuples (current_node, current_depth, parent_structure)\n    queue = deque([(root_contents, 0, repo_structure)])\n\n    while queue:\n        current_node, current_depth, parent_structure = queue.popleft()\n\n        for content in current_node:\n            if not self._is_allowed(content):\n                continue\n            if content.type == \"dir\" and current_depth &lt; depth:\n                # Create a new sub-dictionary for this directory\n                new_dir = {\n                    \"type\": \"dir\",\n                    \"name\": content.name,\n                    \"dirs\": [],\n                    \"files\": [],\n                    \"path\": content.path,\n                }\n                parent_structure[\"dirs\"].append(new_dir)\n                contents = self.repo.get_contents(content.path)\n                if not isinstance(contents, list):\n                    contents = [contents]\n                queue.append(\n                    (\n                        contents,\n                        current_depth + 1,\n                        new_dir,\n                    )\n                )\n            elif content.type == \"file\":\n                file_content = \"\\n\".join(\n                    _get_decoded_content(content).splitlines()[:lines]\n                )\n                file_dict = {\n                    \"type\": \"file\",\n                    \"name\": content.name,\n                    \"content\": file_content,\n                    \"path\": content.path,\n                }\n                parent_structure[\"files\"].append(file_dict)\n\n    return repo_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load","title":"<code>load(path=None, depth=3, lines=0)</code>","text":"<p>From a local folder <code>path</code> (if None, the repo clone path), get:   a nested dictionary (tree) of dicts, files and contents   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path; if none, use self.clone_path()</p> <code>None</code> <code>depth</code> <code>int</code> <p>The depth level.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents, and a list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load(\n    self,\n    path: Optional[str] = None,\n    depth: int = 3,\n    lines: int = 0,\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n    \"\"\"\n    From a local folder `path` (if None, the repo clone path), get:\n      a nested dictionary (tree) of dicts, files and contents\n      a list of Document objects for each file.\n\n    Args:\n        path (str): The local folder path; if none, use self.clone_path()\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n\n    Returns:\n        Tuple of (dict, List_of_Documents):\n            A dictionary containing file and directory names, with file\n            contents, and a list of Document objects for each file.\n    \"\"\"\n    if path is None:\n        if self.clone_path is None or not _has_files(self.clone_path):\n            self.clone()\n        path = self.clone_path\n    if path is None:\n        raise ValueError(\"Unable to clone repo\")\n    return self.load_from_folder(\n        path=path,\n        depth=depth,\n        lines=lines,\n        file_types=self.config.file_types,\n        exclude_dirs=self.config.exclude_dirs,\n        url=self.url,\n    )\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_from_folder","title":"<code>load_from_folder(path, depth=3, lines=0, file_types=None, exclude_dirs=None, url='')</code>  <code>staticmethod</code>","text":"<p>From a local folder <code>path</code> (required), get:   a nested dictionary (tree) of dicts, files and contents, restricting to     desired file_types and excluding undesired directories.   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path, required.</p> required <code>depth</code> <code>int</code> <p>The depth level. Optional, default 3.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.     Optional, default 0 (no lines =&gt; empty string).</p> <code>0</code> <code>file_types</code> <code>List[str]</code> <p>The file types to include.     Optional, default None (all).</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>The directories to exclude.     Optional, default None (no exclusions).</p> <code>None</code> <code>url</code> <code>str</code> <p>Optional url, to be stored in docs as metadata. Default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents. A list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef load_from_folder(\n    path: str,\n    depth: int = 3,\n    lines: int = 0,\n    file_types: Optional[List[str]] = None,\n    exclude_dirs: Optional[List[str]] = None,\n    url: str = \"\",\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n    \"\"\"\n    From a local folder `path` (required), get:\n      a nested dictionary (tree) of dicts, files and contents, restricting to\n        desired file_types and excluding undesired directories.\n      a list of Document objects for each file.\n\n    Args:\n        path (str): The local folder path, required.\n        depth (int): The depth level. Optional, default 3.\n        lines (int): The number of lines of file contents to include.\n                Optional, default 0 (no lines =&gt; empty string).\n        file_types (List[str]): The file types to include.\n                Optional, default None (all).\n        exclude_dirs (List[str]): The directories to exclude.\n                Optional, default None (no exclusions).\n        url (str): Optional url, to be stored in docs as metadata. Default \"\".\n\n    Returns:\n        Tuple of (dict, List_of_Documents):\n            A dictionary containing file and directory names, with file contents.\n            A list of Document objects for each file.\n    \"\"\"\n\n    folder_structure = {\n        \"type\": \"dir\",\n        \"name\": \"\",\n        \"dirs\": [],\n        \"files\": [],\n        \"path\": \"\",\n    }\n    # A queue of tuples (current_path, current_depth, parent_structure)\n    queue = deque([(path, 0, folder_structure)])\n    docs = []\n    exclude_dirs = exclude_dirs or []\n    while queue:\n        current_path, current_depth, parent_structure = queue.popleft()\n\n        for item in os.listdir(current_path):\n            item_path = os.path.join(current_path, item)\n            relative_path = os.path.relpath(item_path, path)\n            if (os.path.isdir(item_path) and item in exclude_dirs) or (\n                os.path.isfile(item_path)\n                and file_types is not None\n                and RepoLoader._file_type(item) not in file_types\n            ):\n                continue\n\n            if os.path.isdir(item_path) and current_depth &lt; depth:\n                # Create a new sub-dictionary for this directory\n                new_dir = {\n                    \"type\": \"dir\",\n                    \"name\": item,\n                    \"dirs\": [],\n                    \"files\": [],\n                    \"path\": relative_path,\n                }\n                parent_structure[\"dirs\"].append(new_dir)\n                queue.append((item_path, current_depth + 1, new_dir))\n            elif os.path.isfile(item_path):\n                # Add the file to the current dictionary\n                with open(item_path, \"r\") as f:\n                    file_lines = list(itertools.islice(f, lines))\n                file_content = \"\\n\".join(line.strip() for line in file_lines)\n                if file_content == \"\":\n                    continue\n\n                file_dict = {\n                    \"type\": \"file\",\n                    \"name\": item,\n                    \"content\": file_content,\n                    \"path\": relative_path,\n                }\n                parent_structure[\"files\"].append(file_dict)\n                docs.append(\n                    Document(\n                        content=file_content,\n                        metadata=DocMetaData(\n                            repo=url,\n                            source=relative_path,\n                            url=url,\n                            filename=item,\n                            extension=RepoLoader._file_type(item),\n                            language=RepoLoader._file_type(item),\n                        ),\n                    )\n                )\n    return folder_structure, docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.get_documents","title":"<code>get_documents(path, parser=Parser(ParsingConfig()), file_types=None, exclude_dirs=None, depth=-1, lines=None)</code>  <code>staticmethod</code>","text":"<p>Recursively get all files under a path as Document objects.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory or file.</p> required <code>parser</code> <code>Parser</code> <p>Parser to use to parse files.</p> <code>Parser(ParsingConfig())</code> <code>file_types</code> <code>List[str]</code> <p>List of file extensions OR filenames OR file_path_names to  include. Defaults to None, which includes all files.</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>List of directories to exclude. Defaults to None, which includes all directories.</p> <code>None</code> <code>depth</code> <code>int</code> <p>Max depth of recursion. Defaults to -1, which includes all depths.</p> <code>-1</code> <code>lines</code> <code>int</code> <p>Number of lines to read from each file. Defaults to None, which reads all lines.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of Document objects representing files.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef get_documents(\n    path: str,\n    parser: Parser = Parser(ParsingConfig()),\n    file_types: Optional[List[str]] = None,\n    exclude_dirs: Optional[List[str]] = None,\n    depth: int = -1,\n    lines: Optional[int] = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Recursively get all files under a path as Document objects.\n\n    Args:\n        path (str): The path to the directory or file.\n        parser (Parser): Parser to use to parse files.\n        file_types (List[str], optional): List of file extensions OR\n            filenames OR file_path_names to  include.\n            Defaults to None, which includes all files.\n        exclude_dirs (List[str], optional): List of directories to exclude.\n            Defaults to None, which includes all directories.\n        depth (int, optional): Max depth of recursion. Defaults to -1,\n            which includes all depths.\n        lines (int, optional): Number of lines to read from each file.\n            Defaults to None, which reads all lines.\n\n    Returns:\n        List[Document]: List of Document objects representing files.\n\n    \"\"\"\n    docs = []\n    file_paths = []\n    path_obj = Path(path).resolve()\n\n    if path_obj.is_file():\n        file_paths.append(str(path_obj))\n    else:\n        path_depth = len(path_obj.parts)\n        for root, dirs, files in os.walk(path):\n            # Exclude directories if needed\n            if exclude_dirs:\n                dirs[:] = [d for d in dirs if d not in exclude_dirs]\n\n            current_depth = len(Path(root).resolve().parts) - path_depth\n            if depth == -1 or current_depth &lt;= depth:\n                for file in files:\n                    file_path = str(Path(root) / file)\n                    if (\n                        file_types is None\n                        or RepoLoader._file_type(file_path) in file_types\n                        or os.path.basename(file_path) in file_types\n                        or file_path in file_types\n                    ):\n                        file_paths.append(file_path)\n\n    for file_path in file_paths:\n        _, file_extension = os.path.splitext(file_path)\n        if file_extension.lower() in [\".pdf\", \".docx\"]:\n            doc_parser = DocumentParser.create(\n                file_path,\n                parser.config,\n            )\n            docs.extend(doc_parser.get_doc_chunks())\n        else:\n            with open(file_path, \"r\") as f:\n                if lines is not None:\n                    file_lines = list(itertools.islice(f, lines))\n                    content = \"\\n\".join(line.strip() for line in file_lines)\n                else:\n                    content = f.read()\n            soup = BeautifulSoup(content, \"html.parser\")\n            text = soup.get_text()\n            docs.append(\n                Document(\n                    content=text,\n                    metadata=DocMetaData(source=str(file_path)),\n                )\n            )\n\n    return docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_docs_from_github","title":"<code>load_docs_from_github(k=None, depth=None, lines=None)</code>","text":"<p>Directly from GitHub, recursively get all files in a repo that have one of the extensions, possibly up to a max number of files, max depth, and max number of lines per file (if any of these are specified).</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>max number of files to load, or None for all files</p> <code>None</code> <code>depth</code> <code>int</code> <p>max depth to recurse, or None for infinite depth</p> <code>None</code> <code>lines</code> <code>int</code> <p>max number of lines to get, from a file, or None for all lines</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of Document objects, each has fields <code>content</code> and <code>metadata</code>,</p> <code>List[Document]</code> <p>and <code>metadata</code> has fields <code>url</code>, <code>filename</code>, <code>extension</code>, <code>language</code></p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_docs_from_github(\n    self,\n    k: Optional[int] = None,\n    depth: Optional[int] = None,\n    lines: Optional[int] = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Directly from GitHub, recursively get all files in a repo that have one of the\n    extensions, possibly up to a max number of files, max depth, and max number\n    of lines per file (if any of these are specified).\n\n    Args:\n        k (int): max number of files to load, or None for all files\n        depth (int): max depth to recurse, or None for infinite depth\n        lines (int): max number of lines to get, from a file, or None for all lines\n\n    Returns:\n        list of Document objects, each has fields `content` and `metadata`,\n        and `metadata` has fields `url`, `filename`, `extension`, `language`\n    \"\"\"\n    contents = self.repo.get_contents(\"\")\n    if not isinstance(contents, list):\n        contents = [contents]\n    stack = list(zip(contents, [0] * len(contents)))  # stack of (content, depth)\n    # recursively get all files in repo that have one of the extensions\n    docs = []\n    i = 0\n\n    while stack:\n        if k is not None and i == k:\n            break\n        file_content, d = stack.pop()\n        if not self._is_allowed(file_content):\n            continue\n        if file_content.type == \"dir\":\n            if depth is None or d &lt;= depth:\n                items = self.repo.get_contents(file_content.path)\n                if not isinstance(items, list):\n                    items = [items]\n                stack.extend(list(zip(items, [d + 1] * len(items))))\n        else:\n            if depth is None or d &lt;= depth:\n                # need to decode the file content, which is in bytes\n                contents = self.repo.get_contents(file_content.path)\n                if isinstance(contents, list):\n                    contents = contents[0]\n                text = _get_decoded_content(contents)\n                if lines is not None:\n                    text = \"\\n\".join(text.split(\"\\n\")[:lines])\n                i += 1\n\n                # Note `source` is important, it may be used to cite\n                # evidence for an answer.\n                # See  URLLoader\n                # TODO we should use Pydantic to enforce/standardize this\n\n                docs.append(\n                    Document(\n                        content=text,\n                        metadata=DocMetaData(\n                            repo=self.url,\n                            source=file_content.html_url,\n                            url=file_content.html_url,\n                            filename=file_content.name,\n                            extension=self._file_type(file_content.name),\n                            language=self._file_type(file_content.name),\n                        ),\n                    )\n                )\n    return docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.select","title":"<code>select(structure, includes, excludes=[])</code>  <code>staticmethod</code>","text":"<p>Filter a structure dictionary for certain directories and files.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>includes</code> <code>List[str]</code> <p>A list of desired directories and files. For files, either full file names or \"file type\" can be specified. E.g.  \"toml\" will include all files with the \".toml\" extension, or \"Makefile\" will include all files named \"Makefile\".</p> required <code>excludes</code> <code>List[str]</code> <p>A list of directories and files to exclude. Similar to <code>includes</code>, full file/dir names or \"file type\" can be specified. Optional, defaults to empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef select(\n    structure: Dict[str, Union[str, List[Dict[str, Any]]]],\n    includes: List[str],\n    excludes: List[str] = [],\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n    \"\"\"\n    Filter a structure dictionary for certain directories and files.\n\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        includes (List[str]): A list of desired directories and files.\n            For files, either full file names or \"file type\" can be specified.\n            E.g.  \"toml\" will include all files with the \".toml\" extension,\n            or \"Makefile\" will include all files named \"Makefile\".\n        excludes (List[str]): A list of directories and files to exclude.\n            Similar to `includes`, full file/dir names or \"file type\" can be\n            specified. Optional, defaults to empty list.\n\n\n    Returns:\n        Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.\n    \"\"\"\n    filtered_structure = {\n        \"type\": structure[\"type\"],\n        \"name\": structure[\"name\"],\n        \"dirs\": [],\n        \"files\": [],\n        \"path\": structure[\"path\"],\n    }\n\n    for dir in structure[\"dirs\"]:\n        if (\n            dir[\"name\"] in includes\n            or RepoLoader._file_type(dir[\"name\"]) in includes\n        ) and (\n            dir[\"name\"] not in excludes\n            and RepoLoader._file_type(dir[\"name\"]) not in excludes\n        ):\n            # If the directory is in the select list, include the whole subtree\n            filtered_structure[\"dirs\"].append(dir)\n        else:\n            # Otherwise, filter the directory's contents\n            filtered_dir = RepoLoader.select(dir, includes)\n            if (\n                filtered_dir[\"dirs\"] or filtered_dir[\"files\"]\n            ):  # only add if not empty\n                filtered_structure[\"dirs\"].append(filtered_dir)\n\n    for file in structure[\"files\"]:\n        if (\n            file[\"name\"] in includes\n            or RepoLoader._file_type(file[\"name\"]) in includes\n        ) and (\n            file[\"name\"] not in excludes\n            and RepoLoader._file_type(file[\"name\"]) not in excludes\n        ):\n            filtered_structure[\"files\"].append(file)\n\n    return filtered_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.ls","title":"<code>ls(structure, depth=0)</code>  <code>staticmethod</code>","text":"<p>Get a list of names of files or directories up to a certain depth from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of names of files or directories.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef ls(structure: Dict[str, Union[str, List[Dict]]], depth: int = 0) -&gt; List[str]:\n    \"\"\"\n    Get a list of names of files or directories up to a certain depth from a\n    structure dictionary.\n\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        depth (int, optional): The depth level. Defaults to 0.\n\n    Returns:\n        List[str]: A list of names of files or directories.\n    \"\"\"\n    names = []\n\n    # A queue of tuples (current_structure, current_depth)\n    queue = deque([(structure, 0)])\n\n    while queue:\n        current_structure, current_depth = queue.popleft()\n\n        if current_depth &lt;= depth:\n            names.append(current_structure[\"name\"])\n\n            for dir in current_structure[\"dirs\"]:\n                queue.append((dir, current_depth + 1))\n\n            for file in current_structure[\"files\"]:\n                # add file names only if depth is less than the limit\n                if current_depth &lt; depth:\n                    names.append(file[\"name\"])\n    names = [n for n in names if n not in [\"\", None]]\n    return names\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.list_files","title":"<code>list_files(dir, depth=1, include_types=[], exclude_types=[])</code>  <code>staticmethod</code>","text":"<p>Recursively list all files in a directory, up to a certain depth.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>The directory path, relative to root.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 1.</p> <code>1</code> <code>include_types</code> <code>List[str]</code> <p>A list of file types to include. Defaults to empty list.</p> <code>[]</code> <code>exclude_types</code> <code>List[str]</code> <p>A list of file types to exclude. Defaults to empty list.</p> <code>[]</code> <p>Returns:     List[str]: A list of file names.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef list_files(\n    dir: str,\n    depth: int = 1,\n    include_types: List[str] = [],\n    exclude_types: List[str] = [],\n) -&gt; List[str]:\n    \"\"\"\n    Recursively list all files in a directory, up to a certain depth.\n\n    Args:\n        dir (str): The directory path, relative to root.\n        depth (int, optional): The depth level. Defaults to 1.\n        include_types (List[str], optional): A list of file types to include.\n            Defaults to empty list.\n        exclude_types (List[str], optional): A list of file types to exclude.\n            Defaults to empty list.\n    Returns:\n        List[str]: A list of file names.\n    \"\"\"\n    depth = depth if depth &gt;= 0 else 200\n    output = []\n\n    for root, dirs, files in os.walk(dir):\n        if root.count(os.sep) - dir.count(os.sep) &lt; depth:\n            level = root.count(os.sep) - dir.count(os.sep)\n            sub_indent = \" \" * 4 * (level + 1)\n            for d in dirs:\n                output.append(\"{}{}/\".format(sub_indent, d))\n            for f in files:\n                if include_types and RepoLoader._file_type(f) not in include_types:\n                    continue\n                if exclude_types and RepoLoader._file_type(f) in exclude_types:\n                    continue\n                output.append(\"{}{}\".format(sub_indent, f))\n    return output\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.show_file_contents","title":"<code>show_file_contents(tree)</code>  <code>staticmethod</code>","text":"<p>Print the contents of all files from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef show_file_contents(tree: Dict[str, Union[str, List[Dict[str, Any]]]]) -&gt; str:\n    \"\"\"\n    Print the contents of all files from a structure dictionary.\n\n    Args:\n        tree (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n    \"\"\"\n    contents = \"\"\n    for dir in tree[\"dirs\"]:\n        contents += RepoLoader.show_file_contents(dir)\n    for file in tree[\"files\"]:\n        path = file[\"path\"]\n        contents += f\"\"\"\n        {path}:\n        --------------------\n        {file[\"content\"]}\n\n        \"\"\"\n\n    return contents\n</code></pre>"},{"location":"reference/parsing/search/","title":"search","text":"<p>langroid/parsing/search.py </p> <p>Utils to search for close matches in (a list of) strings. Useful for retrieval of docs/chunks relevant to a query, in the context of Retrieval-Augmented Generation (RAG), and SQLChat (e.g., to pull relevant parts of a large schema). See tests for examples: tests/main/test_string_search.py</p>"},{"location":"reference/parsing/search/#langroid.parsing.search.find_fuzzy_matches_in_docs","title":"<code>find_fuzzy_matches_in_docs(query, docs, docs_clean, k, words_before=None, words_after=None)</code>","text":"<p>Find approximate matches of the query in the docs and return surrounding characters.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search string.</p> required <code>docs</code> <code>List[Document]</code> <p>List of Document objects to search through.</p> required <code>k</code> <code>int</code> <p>Number of best matches to return.</p> required <code>words_before</code> <code>int | None</code> <p>Number of words to include before each match. Default None =&gt; return max</p> <code>None</code> <code>words_after</code> <code>int | None</code> <p>Number of words to include after each match. Default None =&gt; return max</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of Documents containing the matches, including the given number of words around the match.</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def find_fuzzy_matches_in_docs(\n    query: str,\n    docs: List[Document],\n    docs_clean: List[Document],\n    k: int,\n    words_before: int | None = None,\n    words_after: int | None = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Find approximate matches of the query in the docs and return surrounding\n    characters.\n\n    Args:\n        query (str): The search string.\n        docs (List[Document]): List of Document objects to search through.\n        k (int): Number of best matches to return.\n        words_before (int|None): Number of words to include before each match.\n            Default None =&gt; return max\n        words_after (int|None): Number of words to include after each match.\n            Default None =&gt; return max\n\n    Returns:\n        List[Document]: List of Documents containing the matches,\n            including the given number of words around the match.\n    \"\"\"\n    if len(docs) == 0:\n        return []\n    best_matches = process.extract(\n        query,\n        [d.content for d in docs_clean],\n        limit=k,\n        scorer=fuzz.partial_ratio,\n    )\n\n    real_matches = [m for m, score in best_matches if score &gt; 50]\n    # find the original docs that corresponding to the matches\n    orig_doc_matches = []\n    for i, m in enumerate(real_matches):\n        for j, doc_clean in enumerate(docs_clean):\n            if m in doc_clean.content:\n                orig_doc_matches.append(docs[j])\n                break\n    if words_after is None and words_before is None:\n        return orig_doc_matches\n\n    contextual_matches = []\n    for match in orig_doc_matches:\n        choice_text = match.content\n        contexts = []\n        while choice_text != \"\":\n            context, start_pos, end_pos = get_context(\n                query, choice_text, words_before, words_after\n            )\n            if context == \"\" or end_pos == 0:\n                break\n            contexts.append(context)\n            words = choice_text.split()\n            end_pos = min(end_pos, len(words))\n            choice_text = \" \".join(words[end_pos:])\n        if len(contexts) &gt; 0:\n            contextual_matches.append(\n                Document(\n                    content=\" ... \".join(contexts),\n                    metadata=match.metadata,\n                )\n            )\n\n    return contextual_matches\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.preprocess_text","title":"<code>preprocess_text(text)</code>","text":"<p>Preprocesses the given text by: 1. Lowercasing all words. 2. Tokenizing (splitting the text into words). 3. Removing punctuation. 4. Removing stopwords. 5. Lemmatizing words.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The preprocessed text.</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def preprocess_text(text: str) -&gt; str:\n    \"\"\"\n    Preprocesses the given text by:\n    1. Lowercasing all words.\n    2. Tokenizing (splitting the text into words).\n    3. Removing punctuation.\n    4. Removing stopwords.\n    5. Lemmatizing words.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        str: The preprocessed text.\n    \"\"\"\n    # Ensure the NLTK resources are available\n    for resource in [\"punkt\", \"wordnet\", \"stopwords\"]:\n        download_nltk_resource(resource)\n\n    # Lowercase the text\n    text = text.lower()\n\n    # Tokenize the text and remove punctuation\n    tokenizer = RegexpTokenizer(r\"\\w+\")\n    tokens = tokenizer.tokenize(text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words(\"english\"))\n    tokens = [t for t in tokens if t not in stop_words]\n\n    # Lemmatize words\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n\n    # Join the words back into a string\n    text = \" \".join(tokens)\n\n    return text\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.find_closest_matches_with_bm25","title":"<code>find_closest_matches_with_bm25(docs, docs_clean, query, k=5)</code>","text":"<p>Finds the k closest approximate matches using the BM25 algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Documents to search through.</p> required <code>docs_clean</code> <code>List[Document]</code> <p>List of cleaned Documents</p> required <code>query</code> <code>str</code> <p>The search query.</p> required <code>k</code> <code>int</code> <p>Number of matches to retrieve. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document,float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def find_closest_matches_with_bm25(\n    docs: List[Document],\n    docs_clean: List[Document],\n    query: str,\n    k: int = 5,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Finds the k closest approximate matches using the BM25 algorithm.\n\n    Args:\n        docs (List[Document]): List of Documents to search through.\n        docs_clean (List[Document]): List of cleaned Documents\n        query (str): The search query.\n        k (int, optional): Number of matches to retrieve. Defaults to 5.\n\n    Returns:\n        List[Tuple[Document,float]]: List of (Document, score) tuples.\n    \"\"\"\n    if len(docs) == 0:\n        return []\n    texts = [doc.content for doc in docs_clean]\n    query = preprocess_text(query)\n\n    text_words = [text.split() for text in texts]\n\n    bm25 = BM25Okapi(text_words)\n    query_words = query.split()\n    doc_scores = bm25.get_scores(query_words)\n\n    # Get indices of top k scores\n    top_indices = sorted(range(len(doc_scores)), key=lambda i: -doc_scores[i])[:k]\n\n    # return the original docs, based on the scores from cleaned docs\n    return [(docs[i], doc_scores[i]) for i in top_indices]\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.get_context","title":"<code>get_context(query, text, words_before=100, words_after=100)</code>","text":"<p>Returns a portion of text containing the best approximate match of the query, including b words before and a words after the match.</p> <p>Args: query (str): The string to search for. text (str): The body of text in which to search. b (int): The number of words before the query to return. a (int): The number of words after the query to return.</p> <p>str: A string containing b words before, the match, and a words after     the best approximate match position of the query in the text. If no     match is found, returns empty string. int: The start position of the match in the text. int: The end position of the match in the text.</p> <p>Example:</p> <p>get_context(\"apple\", \"The quick brown fox jumps over the apple.\", 3, 2)</p>"},{"location":"reference/parsing/search/#langroid.parsing.search.get_context--fox-jumps-over-the-apple","title":"'fox jumps over the apple.'","text":"Source code in <code>langroid/parsing/search.py</code> <pre><code>def get_context(\n    query: str,\n    text: str,\n    words_before: int | None = 100,\n    words_after: int | None = 100,\n) -&gt; Tuple[str, int, int]:\n    \"\"\"\n    Returns a portion of text containing the best approximate match of the query,\n    including b words before and a words after the match.\n\n    Args:\n    query (str): The string to search for.\n    text (str): The body of text in which to search.\n    b (int): The number of words before the query to return.\n    a (int): The number of words after the query to return.\n\n    Returns:\n    str: A string containing b words before, the match, and a words after\n        the best approximate match position of the query in the text. If no\n        match is found, returns empty string.\n    int: The start position of the match in the text.\n    int: The end position of the match in the text.\n\n    Example:\n    &gt;&gt;&gt; get_context(\"apple\", \"The quick brown fox jumps over the apple.\", 3, 2)\n    # 'fox jumps over the apple.'\n    \"\"\"\n    if words_after is None and words_before is None:\n        # return entire text since we're not asked to return a bounded context\n        return text, 0, 0\n\n    # make sure there is a good enough match to the query\n    if fuzz.partial_ratio(query, text) &lt; 40:\n        return \"\", 0, 0\n\n    sequence_matcher = difflib.SequenceMatcher(None, text, query)\n    match = sequence_matcher.find_longest_match(0, len(text), 0, len(query))\n\n    if match.size == 0:\n        return \"\", 0, 0\n\n    segments = text.split()\n    n_segs = len(segments)\n\n    start_segment_pos = len(text[: match.a].split())\n\n    words_before = words_before or n_segs\n    words_after = words_after or n_segs\n    start_pos = max(0, start_segment_pos - words_before)\n    end_pos = min(len(segments), start_segment_pos + words_after + len(query.split()))\n\n    return \" \".join(segments[start_pos:end_pos]), start_pos, end_pos\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.eliminate_near_duplicates","title":"<code>eliminate_near_duplicates(passages, threshold=0.8)</code>","text":"<p>Eliminate near duplicate text passages from a given list using MinHash and LSH. TODO: this has not been tested and the datasketch lib is not a dependency. Args:     passages (List[str]): A list of text passages.     threshold (float, optional): Jaccard similarity threshold to consider two                                  passages as near-duplicates. Default is 0.8.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of passages after eliminating near duplicates.</p> Example <p>passages = [\"Hello world\", \"Hello, world!\", \"Hi there\", \"Hello world!\"] print(eliminate_near_duplicates(passages))</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def eliminate_near_duplicates(passages: List[str], threshold: float = 0.8) -&gt; List[str]:\n    \"\"\"\n    Eliminate near duplicate text passages from a given list using MinHash and LSH.\n    TODO: this has not been tested and the datasketch lib is not a dependency.\n    Args:\n        passages (List[str]): A list of text passages.\n        threshold (float, optional): Jaccard similarity threshold to consider two\n                                     passages as near-duplicates. Default is 0.8.\n\n    Returns:\n        List[str]: A list of passages after eliminating near duplicates.\n\n    Example:\n        passages = [\"Hello world\", \"Hello, world!\", \"Hi there\", \"Hello world!\"]\n        print(eliminate_near_duplicates(passages))\n        # ['Hello world', 'Hi there']\n    \"\"\"\n\n    from datasketch import MinHash, MinHashLSH\n\n    # Create LSH index\n    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n\n    # Create MinHash objects for each passage and insert to LSH\n    minhashes = {}\n    for idx, passage in enumerate(passages):\n        m = MinHash(num_perm=128)\n        for word in passage.split():\n            m.update(word.encode(\"utf-8\"))\n        lsh.insert(idx, m)\n        minhashes[idx] = m\n\n    unique_idxs = set()\n    for idx in minhashes.keys():\n        # Query for similar passages (including itself)\n        result = lsh.query(minhashes[idx])\n\n        # If only the passage itself is returned, it's unique\n        if len(result) == 1 and idx in result:\n            unique_idxs.add(idx)\n\n    return [passages[idx] for idx in unique_idxs]\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.eliminate_near_duplicates--hello-world-hi-there","title":"['Hello world', 'Hi there']","text":""},{"location":"reference/parsing/spider/","title":"spider","text":"<p>langroid/parsing/spider.py </p>"},{"location":"reference/parsing/spider/#langroid.parsing.spider.DomainSpecificSpider","title":"<code>DomainSpecificSpider(start_url, k=20, *args, **kwargs)</code>","text":"<p>             Bases: <code>CrawlSpider</code></p> <p>Parameters:</p> Name Type Description Default <code>start_url</code> <code>str</code> <p>The starting URL.</p> required <code>k</code> <code>int</code> <p>The max desired final URLs. Defaults to 20.</p> <code>20</code> Source code in <code>langroid/parsing/spider.py</code> <pre><code>def __init__(self, start_url: str, k: int = 20, *args, **kwargs):  # type: ignore\n    \"\"\"Initialize the spider with start_url and k.\n\n    Args:\n        start_url (str): The starting URL.\n        k (int, optional): The max desired final URLs. Defaults to 20.\n    \"\"\"\n    super(DomainSpecificSpider, self).__init__(*args, **kwargs)\n    self.start_urls = [start_url]\n    self.allowed_domains = [urlparse(start_url).netloc]\n    self.k = k\n    self.visited_urls: Set[str] = set()\n</code></pre>"},{"location":"reference/parsing/spider/#langroid.parsing.spider.DomainSpecificSpider.parse_item","title":"<code>parse_item(response)</code>","text":"<p>Extracts URLs that are within the same domain.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>The scrapy response object.</p> required Source code in <code>langroid/parsing/spider.py</code> <pre><code>def parse_item(self, response: Response):  # type: ignore\n    \"\"\"Extracts URLs that are within the same domain.\n\n    Args:\n        response: The scrapy response object.\n    \"\"\"\n    for link in LinkExtractor(allow_domains=self.allowed_domains).extract_links(\n        response\n    ):\n        if len(self.visited_urls) &lt; self.k:\n            self.visited_urls.add(link.url)\n            yield {\"url\": link.url}\n</code></pre>"},{"location":"reference/parsing/spider/#langroid.parsing.spider.scrapy_fetch_urls","title":"<code>scrapy_fetch_urls(url, k=20)</code>","text":"<p>Fetches up to k URLs reachable from the input URL using Scrapy.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The starting URL.</p> required <code>k</code> <code>int</code> <p>The max desired final URLs. Defaults to 20.</p> <code>20</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of URLs within the same domain as the input URL.</p> Source code in <code>langroid/parsing/spider.py</code> <pre><code>@no_type_check\ndef scrapy_fetch_urls(url: str, k: int = 20) -&gt; List[str]:\n    \"\"\"Fetches up to k URLs reachable from the input URL using Scrapy.\n\n    Args:\n        url (str): The starting URL.\n        k (int, optional): The max desired final URLs. Defaults to 20.\n\n    Returns:\n        List[str]: List of URLs within the same domain as the input URL.\n    \"\"\"\n    urls = []\n\n    def _collect_urls(spider):\n        \"\"\"Handler for the spider_closed signal. Collects the visited URLs.\"\"\"\n        nonlocal urls\n        urls.extend(list(spider.visited_urls))\n\n    # Connect the spider_closed signal with our handler\n    dispatcher.connect(_collect_urls, signal=signals.spider_closed)\n\n    runner = CrawlerRunner(\n        {\n            \"USER_AGENT\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\n        }\n    )\n\n    d = runner.crawl(DomainSpecificSpider, start_url=url, k=k)\n\n    # Block until crawling is done and then stop the reactor\n    crawl_deferred = defer.Deferred()\n\n    def _crawl_done(_):\n        reactor.stop()\n        crawl_deferred.callback(urls)\n\n    d.addBoth(_crawl_done)\n\n    # Start the reactor, it will stop once the crawl is done\n    reactor.run(installSignalHandlers=0)\n\n    # This will block until the deferred gets a result\n    return crawl_deferred.result\n</code></pre>"},{"location":"reference/parsing/table_loader/","title":"table_loader","text":"<p>langroid/parsing/table_loader.py </p>"},{"location":"reference/parsing/table_loader/#langroid.parsing.table_loader.read_tabular_data","title":"<code>read_tabular_data(path_or_url, sep=None)</code>","text":"<p>Reads tabular data from a file or URL and returns a pandas DataFrame. The separator is auto-detected if not specified.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_url</code> <code>str</code> <p>Path or URL to the file to be read.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from file or URL as a pandas DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data cannot be read or is misformatted.</p> Source code in <code>langroid/parsing/table_loader.py</code> <pre><code>def read_tabular_data(path_or_url: str, sep: None | str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads tabular data from a file or URL and returns a pandas DataFrame.\n    The separator is auto-detected if not specified.\n\n    Args:\n        path_or_url (str): Path or URL to the file to be read.\n\n    Returns:\n        pd.DataFrame: Data from file or URL as a pandas DataFrame.\n\n    Raises:\n        ValueError: If the data cannot be read or is misformatted.\n    \"\"\"\n    try:\n        if sep is None:\n            # Read the first few lines to guess the separator\n            with pd.io.common.get_handle(path_or_url, \"r\") as file_handler:\n                first_lines = \"\".join(file_handler.handle.readlines(5))\n                sep = Sniffer().sniff(first_lines).delimiter\n                # If it's a local file, reset to the beginning\n                if hasattr(file_handler.handle, \"seek\"):\n                    file_handler.handle.seek(0)\n\n        # Read the data\n\n        # get non-blank column names\n        with pd.io.common.get_handle(path_or_url, \"r\") as f:\n            header_line = f.handle.readline().strip()\n            valid_cols = [col for col in header_line.split(sep) if col]\n            valid_cols = [c.replace('\"', \"\").replace(\"'\", \"\") for c in valid_cols]\n            if hasattr(f.handle, \"seek\"):\n                f.handle.seek(0)\n\n        # use only those columns\n        data = pd.read_csv(path_or_url, sep=sep, usecols=valid_cols)\n        data.columns = data.columns.str.strip()  # e.g. \"  column 1  \" -&gt; \"column 1\"\n\n        return data\n\n    except Exception as e:\n        raise ValueError(\n            \"Unable to read data. \"\n            \"Please ensure it is correctly formatted. Error: \" + str(e)\n        )\n</code></pre>"},{"location":"reference/parsing/url_loader/","title":"url_loader","text":"<p>langroid/parsing/url_loader.py </p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.URLLoader","title":"<code>URLLoader(urls, parser=Parser(ParsingConfig()))</code>","text":"<p>Load a list of URLs and extract the text content. Alternative approaches could use <code>bs4</code> or <code>scrapy</code>.</p> <p>TODO - this currently does not handle cookie dialogs,  i.e. if there is a cookie pop-up, most/all of the extracted  content could be cookie policy text.  We could use <code>playwright</code> to simulate a user clicking  the \"accept\" button on the cookie dialog.</p> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def __init__(self, urls: List[str], parser: Parser = Parser(ParsingConfig())):\n    self.urls = urls\n    self.parser = parser\n</code></pre>"},{"location":"reference/parsing/urls/","title":"urls","text":"<p>langroid/parsing/urls.py </p>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.url_to_tempfile","title":"<code>url_to_tempfile(url)</code>","text":"<p>Fetch content from the given URL and save it to a temporary local file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the content to fetch.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the temporary file where the content is saved.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If there's any issue fetching the content.</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def url_to_tempfile(url: str) -&gt; str:\n    \"\"\"\n    Fetch content from the given URL and save it to a temporary local file.\n\n    Args:\n        url (str): The URL of the content to fetch.\n\n    Returns:\n        str: The path to the temporary file where the content is saved.\n\n    Raises:\n        HTTPError: If there's any issue fetching the content.\n    \"\"\"\n\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n\n    # Create a temporary file and write the content\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".tmp\") as temp_file:\n        temp_file.write(response.content)\n        return temp_file.name\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_user_input","title":"<code>get_user_input(msg, color='blue')</code>","text":"<p>Prompt the user for input. Args:     msg: printed prompt     color: color of the prompt Returns:     user input</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_user_input(msg: str, color: str = \"blue\") -&gt; str:\n    \"\"\"\n    Prompt the user for input.\n    Args:\n        msg: printed prompt\n        color: color of the prompt\n    Returns:\n        user input\n    \"\"\"\n    color_str = f\"[{color}]{msg} \" if color else msg + \" \"\n    print(color_str, end=\"\")\n    return input(\"\")\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_list_from_user","title":"<code>get_list_from_user(prompt=\"Enter input (type 'done' or hit return to finish)\", n=None)</code>","text":"<p>Prompt the user for inputs. Args:     prompt: printed prompt     n: how many inputs to prompt for. If None, then prompt until done, otherwise         quit after n inputs. Returns:     list of input strings</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_list_from_user(\n    prompt: str = \"Enter input (type 'done' or hit return to finish)\",\n    n: int | None = None,\n) -&gt; List[str]:\n    \"\"\"\n    Prompt the user for inputs.\n    Args:\n        prompt: printed prompt\n        n: how many inputs to prompt for. If None, then prompt until done, otherwise\n            quit after n inputs.\n    Returns:\n        list of input strings\n    \"\"\"\n    # Create an empty set to store the URLs.\n    input_set = set()\n\n    # Use a while loop to continuously ask the user for URLs.\n    for _ in range(n or 1000):\n        # Prompt the user for input.\n        input_str = Prompt.ask(f\"[blue]{prompt}\")\n\n        # Check if the user wants to exit the loop.\n        if input_str.lower() == \"done\" or input_str == \"\":\n            break\n\n        # if it is a URL, ask how many to crawl\n        if is_url(input_str):\n            url = input_str\n            input_str = Prompt.ask(\"[blue] How many new URLs to crawl?\", default=\"0\")\n            max_urls = int(input_str) + 1\n            tot_urls = list(find_urls(url, max_links=max_urls, max_depth=2))\n            tot_urls_str = \"\\n\".join(tot_urls)\n            print(\n                f\"\"\"\n                Found these {len(tot_urls)} links upto depth 2:\n                {tot_urls_str}\n                \"\"\"\n            )\n\n            input_set.update(tot_urls)\n        else:\n            input_set.add(input_str.strip())\n\n    return list(input_set)\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_urls_and_paths","title":"<code>get_urls_and_paths(inputs)</code>","text":"<p>Given a list of inputs, return a list of URLs and a list of paths. Args:     inputs: list of strings Returns:     list of URLs, list of paths</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_urls_and_paths(inputs: List[str]) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"\n    Given a list of inputs, return a list of URLs and a list of paths.\n    Args:\n        inputs: list of strings\n    Returns:\n        list of URLs, list of paths\n    \"\"\"\n    urls = []\n    paths = []\n    for item in inputs:\n        try:\n            m = Url(url=parse_obj_as(HttpUrl, item))\n            urls.append(str(m.url))\n        except ValidationError:\n            if os.path.exists(item):\n                paths.append(item)\n            else:\n                logger.warning(f\"{item} is neither a URL nor a path.\")\n    return urls, paths\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.crawl_url","title":"<code>crawl_url(url, max_urls=1)</code>","text":"<p>Crawl starting at the url and return a list of URLs to be parsed, up to a maximum of <code>max_urls</code>. This has not been tested to work as intended. Ignore.</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def crawl_url(url: str, max_urls: int = 1) -&gt; List[str]:\n    \"\"\"\n    Crawl starting at the url and return a list of URLs to be parsed,\n    up to a maximum of `max_urls`.\n    This has not been tested to work as intended. Ignore.\n    \"\"\"\n    if max_urls == 1:\n        # no need to crawl, just return the original list\n        return [url]\n\n    to_visit = None\n    known_urls = None\n\n    # Create a RobotFileParser object\n    robots = urllib.robotparser.RobotFileParser()\n    while True:\n        if known_urls is not None and len(known_urls) &gt;= max_urls:\n            break\n        # Set the RobotFileParser object to the website's robots.txt file\n        robots.set_url(url + \"/robots.txt\")\n        robots.read()\n\n        if robots.can_fetch(\"*\", url):\n            # Start or resume the crawl\n            to_visit, known_urls = focused_crawler(\n                url,\n                max_seen_urls=max_urls,\n                max_known_urls=max_urls,\n                todo=to_visit,\n                known_links=known_urls,\n                rules=robots,\n            )\n        if to_visit is None:\n            break\n\n    if known_urls is None:\n        return [url]\n    final_urls = [s.strip() for s in known_urls]\n    return list(final_urls)[:max_urls]\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.find_urls","title":"<code>find_urls(url='https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', max_links=20, visited=None, depth=0, max_depth=2, match_domain=True)</code>","text":"<p>Recursively find all URLs on a given page.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to start from.</p> <code>'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer'</code> <code>max_links</code> <code>int</code> <p>The maximum number of links to find.</p> <code>20</code> <code>visited</code> <code>set</code> <p>A set of URLs that have already been visited.</p> <code>None</code> <code>depth</code> <code>int</code> <p>The current depth of the recursion.</p> <code>0</code> <code>max_depth</code> <code>int</code> <p>The maximum depth of the recursion.</p> <code>2</code> <code>match_domain</code> <code>bool</code> <p>Whether to only return URLs that are on the same domain.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>set</code> <code>Set[str]</code> <p>A set of URLs found on the page.</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def find_urls(\n    url: str = \"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\",\n    max_links: int = 20,\n    visited: Optional[Set[str]] = None,\n    depth: int = 0,\n    max_depth: int = 2,\n    match_domain: bool = True,\n) -&gt; Set[str]:\n    \"\"\"\n    Recursively find all URLs on a given page.\n\n    Args:\n        url (str): The URL to start from.\n        max_links (int): The maximum number of links to find.\n        visited (set): A set of URLs that have already been visited.\n        depth (int): The current depth of the recursion.\n        max_depth (int): The maximum depth of the recursion.\n        match_domain (bool): Whether to only return URLs that are on the same domain.\n\n    Returns:\n        set: A set of URLs found on the page.\n    \"\"\"\n\n    if visited is None:\n        visited = set()\n\n    if url in visited or depth &gt; max_depth:\n        return visited\n\n    visited.add(url)\n    base_domain = urlparse(url).netloc\n\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        links = [urljoin(url, a[\"href\"]) for a in soup.find_all(\"a\", href=True)]\n\n        # Defrag links: discard links that are to portions of same page\n        defragged_links = list(set(urldefrag(link).url for link in links))\n\n        # Filter links based on domain matching requirement\n        domain_matching_links = [\n            link for link in defragged_links if urlparse(link).netloc == base_domain\n        ]\n\n        # If found links exceed max_links, return immediately\n        if len(domain_matching_links) &gt;= max_links:\n            return set(domain_matching_links[:max_links])\n\n        for link in domain_matching_links:\n            if len(visited) &gt;= max_links:\n                break\n\n            if link not in visited:\n                visited.update(\n                    find_urls(\n                        link,\n                        max_links,\n                        visited,\n                        depth + 1,\n                        max_depth,\n                        match_domain,\n                    )\n                )\n\n    except (requests.RequestException, Exception) as e:\n        print(f\"Error fetching {url}. Error: {e}\")\n\n    return set(list(visited)[:max_links])\n</code></pre>"},{"location":"reference/parsing/utils/","title":"utils","text":"<p>langroid/parsing/utils.py </p>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.batched","title":"<code>batched(iterable, n)</code>","text":"<p>Batch data into tuples of length n. The last batch may be shorter.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def batched(iterable: Iterable[Any], n: int) -&gt; Iterable[Any]:\n    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n    # batched('ABCDEFG', 3) --&gt; ABC DEF G\n    if n &lt; 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while batch := tuple(islice(it, n)):\n        yield batch\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.closest_string","title":"<code>closest_string(query, string_list)</code>","text":"<p>Find the closest match to the query in a list of strings.</p> <p>This function is case-insensitive and ignores leading and trailing whitespace. If no match is found, it returns 'No match found'.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The string to match.</p> required <code>string_list</code> <code>List[str]</code> <p>The list of strings to search.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The closest match to the query from the list, or 'No match found'  if no match is found.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def closest_string(query: str, string_list: List[str]) -&gt; str:\n    \"\"\"Find the closest match to the query in a list of strings.\n\n    This function is case-insensitive and ignores leading and trailing whitespace.\n    If no match is found, it returns 'No match found'.\n\n    Args:\n        query (str): The string to match.\n        string_list (List[str]): The list of strings to search.\n\n    Returns:\n        str: The closest match to the query from the list, or 'No match found'\n             if no match is found.\n    \"\"\"\n    # Create a dictionary where the keys are the standardized strings and\n    # the values are the original strings.\n    str_dict = {s.lower().strip(): s for s in string_list}\n\n    # Standardize the query and find the closest match in the list of keys.\n    closest_match = difflib.get_close_matches(\n        query.lower().strip(), str_dict.keys(), n=1\n    )\n\n    # Retrieve the original string from the value in the dictionary.\n    original_closest_match = (\n        str_dict[closest_match[0]] if closest_match else \"No match found\"\n    )\n\n    return original_closest_match\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.split_paragraphs","title":"<code>split_paragraphs(text)</code>","text":"<pre><code>Split the input text into paragraphs using \"\n</code></pre> <p>\" as the delimiter.</p> <pre><code>Args:\n    text (str): The input text.\n\nReturns:\n    list: A list of paragraphs.\n</code></pre> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def split_paragraphs(text: str) -&gt; List[str]:\n    \"\"\"\n    Split the input text into paragraphs using \"\\n\\n\" as the delimiter.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        list: A list of paragraphs.\n    \"\"\"\n    # Split based on a newline, followed by spaces/tabs, then another newline.\n    paras = re.split(r\"\\n[ \\t]*\\n\", text)\n    return [para.strip() for para in paras if para.strip()]\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.number_segments","title":"<code>number_segments(s, len=1)</code>","text":"<p>Number the segments in a given text, preserving paragraph structure. A segment is a sequence of <code>len</code> consecutive sentences.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text.</p> required <code>len</code> <code>int</code> <p>The number of sentences in a segment.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with segments numbered in the style &lt;#1#&gt;, &lt;#2#&gt; etc.</p> Example <p>number_segments(\"Hello world! How are you? Have a good day.\") '&lt;#1#&gt; Hello world! &lt;#2#&gt; How are you? &lt;#3#&gt; Have a good day.'</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def number_segments(s: str, len: int = 1) -&gt; str:\n    \"\"\"\n    Number the segments in a given text, preserving paragraph structure.\n    A segment is a sequence of `len` consecutive sentences.\n\n    Args:\n        s (str): The input text.\n        len (int): The number of sentences in a segment.\n\n    Returns:\n        str: The text with segments numbered in the style &lt;#1#&gt;, &lt;#2#&gt; etc.\n\n    Example:\n        &gt;&gt;&gt; number_segments(\"Hello world! How are you? Have a good day.\")\n        '&lt;#1#&gt; Hello world! &lt;#2#&gt; How are you? &lt;#3#&gt; Have a good day.'\n    \"\"\"\n    numbered_text = []\n    count = 0\n\n    paragraphs = split_paragraphs(s)\n    for paragraph in paragraphs:\n        sentences = nltk.sent_tokenize(paragraph)\n        for i, sentence in enumerate(sentences):\n            num = count // len + 1\n            number_prefix = f\"&lt;#{num}#&gt;\" if count % len == 0 else \"\"\n            sentence = f\"{number_prefix} {sentence}\"\n            count += 1\n            sentences[i] = sentence\n        numbered_paragraph = \" \".join(sentences)\n        numbered_text.append(numbered_paragraph)\n\n    return \"  \\n\\n  \".join(numbered_text)\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.parse_number_range_list","title":"<code>parse_number_range_list(specs)</code>","text":"<p>Parse a specs string like \"3,5,7-10\" into a list of integers.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>str</code> <p>A string containing segment numbers and/or ranges          (e.g., \"3,5,7-10\").</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: List of segment numbers.</p> Example <p>parse_number_range_list(\"3,5,7-10\") [3, 5, 7, 8, 9, 10]</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def parse_number_range_list(specs: str) -&gt; List[int]:\n    \"\"\"\n    Parse a specs string like \"3,5,7-10\" into a list of integers.\n\n    Args:\n        specs (str): A string containing segment numbers and/or ranges\n                     (e.g., \"3,5,7-10\").\n\n    Returns:\n        List[int]: List of segment numbers.\n\n    Example:\n        &gt;&gt;&gt; parse_number_range_list(\"3,5,7-10\")\n        [3, 5, 7, 8, 9, 10]\n    \"\"\"\n    spec_indices = set()  # type: ignore\n    for part in specs.split(\",\"):\n        if \"-\" in part:\n            start, end = map(int, part.split(\"-\"))\n            spec_indices.update(range(start, end + 1))\n        else:\n            spec_indices.add(int(part))\n\n    return sorted(list(spec_indices))\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.strip_k","title":"<code>strip_k(s, k=2)</code>","text":"<p>Strip any leading and trailing whitespaces from the input text beyond length k. This is useful for removing leading/trailing whitespaces from a text while preserving paragraph structure.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text.</p> required <code>k</code> <code>int</code> <p>The number of leading and trailing whitespaces to retain.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with leading and trailing whitespaces removed beyond length k.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def strip_k(s: str, k: int = 2) -&gt; str:\n    \"\"\"\n    Strip any leading and trailing whitespaces from the input text beyond length k.\n    This is useful for removing leading/trailing whitespaces from a text while\n    preserving paragraph structure.\n\n    Args:\n        s (str): The input text.\n        k (int): The number of leading and trailing whitespaces to retain.\n\n    Returns:\n        str: The text with leading and trailing whitespaces removed beyond length k.\n    \"\"\"\n\n    # Count leading and trailing whitespaces\n    leading_count = len(s) - len(s.lstrip())\n    trailing_count = len(s) - len(s.rstrip())\n\n    # Determine how many whitespaces to retain\n    leading_keep = min(leading_count, k)\n    trailing_keep = min(trailing_count, k)\n\n    # Use slicing to get the desired output\n    return s[leading_count - leading_keep : len(s) - (trailing_count - trailing_keep)]\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.clean_whitespace","title":"<code>clean_whitespace(text)</code>","text":"<p>Remove extra whitespace from the input text, while preserving paragraph structure.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def clean_whitespace(text: str) -&gt; str:\n    \"\"\"Remove extra whitespace from the input text, while preserving\n    paragraph structure.\n    \"\"\"\n    paragraphs = split_paragraphs(text)\n    cleaned_paragraphs = [\" \".join(p.split()) for p in paragraphs if p]\n    return \"\\n\\n\".join(cleaned_paragraphs)  # Join the cleaned paragraphs.\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.extract_numbered_segments","title":"<code>extract_numbered_segments(s, specs)</code>","text":"<p>Extract specified segments from a numbered text, preserving paragraph structure.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text containing numbered segments.</p> required <code>specs</code> <code>str</code> <p>A string containing segment numbers and/or ranges          (e.g., \"3,5,7-10\").</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted segments, keeping original paragraph structures.</p> Example <p>text = \"(1) Hello world! (2) How are you? (3) Have a good day.\" extract_numbered_segments(text, \"1,3\") 'Hello world! Have a good day.'</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def extract_numbered_segments(s: str, specs: str) -&gt; str:\n    \"\"\"\n    Extract specified segments from a numbered text, preserving paragraph structure.\n\n    Args:\n        s (str): The input text containing numbered segments.\n        specs (str): A string containing segment numbers and/or ranges\n                     (e.g., \"3,5,7-10\").\n\n    Returns:\n        str: Extracted segments, keeping original paragraph structures.\n\n    Example:\n        &gt;&gt;&gt; text = \"(1) Hello world! (2) How are you? (3) Have a good day.\"\n        &gt;&gt;&gt; extract_numbered_segments(text, \"1,3\")\n        'Hello world! Have a good day.'\n    \"\"\"\n    # Use the helper function to get the list of indices from specs\n    if specs.strip() == \"\":\n        return \"\"\n    spec_indices = parse_number_range_list(specs)\n\n    # Regular expression to identify numbered segments like\n    # &lt;#1#&gt; Hello world! This is me. &lt;#2#&gt; How are you? &lt;#3#&gt; Have a good day.\n    segment_pattern = re.compile(r\"&lt;#(\\d+)#&gt; ((?:(?!&lt;#).)+)\")\n\n    # Split the text into paragraphs while preserving their boundaries\n    paragraphs = split_paragraphs(s)\n\n    extracted_paragraphs = []\n\n    for paragraph in paragraphs:\n        segments_with_numbers = segment_pattern.findall(paragraph)\n\n        # Extract the desired segments from this paragraph\n        extracted_segments = [\n            segment\n            for num, segment in segments_with_numbers\n            if int(num) in spec_indices\n        ]\n\n        # If we extracted any segments from this paragraph,\n        # join them and append to results\n        if extracted_segments:\n            extracted_paragraphs.append(\" \".join(extracted_segments))\n\n    return \"\\n\\n\".join(extracted_paragraphs)\n</code></pre>"},{"location":"reference/parsing/web_search/","title":"web_search","text":"<p>langroid/parsing/web_search.py </p> <p>Utilities for web search.</p> <p>NOTE: Using Google Search requires setting the GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables in your <code>.env</code> file, as explained in the README.</p>"},{"location":"reference/parsing/web_search/#langroid.parsing.web_search.WebSearchResult","title":"<code>WebSearchResult(title, link, max_content_length=3500, max_summary_length=300)</code>","text":"<p>Class representing a Web Search result, containing the title, link, summary and full content of the result.</p> <pre><code>link (str): The link to the search result.\nmax_content_length (int): The maximum length of the full content.\nmax_summary_length (int): The maximum length of the summary.\n</code></pre> Source code in <code>langroid/parsing/web_search.py</code> <pre><code>def __init__(\n    self,\n    title: str,\n    link: str,\n    max_content_length: int = 3500,\n    max_summary_length: int = 300,\n):\n    \"\"\"\n    Args:\n        title (str): The title of the search result.\n        link (str): The link to the search result.\n        max_content_length (int): The maximum length of the full content.\n        max_summary_length (int): The maximum length of the summary.\n    \"\"\"\n    self.title = title\n    self.link = link\n    self.max_content_length = max_content_length\n    self.max_summary_length = max_summary_length\n    self.full_content = self.get_full_content()\n    self.summary = self.get_summary()\n</code></pre>"},{"location":"reference/prompts/","title":"prompts","text":"<p>langroid/prompts/init.py </p>"},{"location":"reference/prompts/dialog/","title":"dialog","text":"<p>langroid/prompts/dialog.py </p>"},{"location":"reference/prompts/dialog/#langroid.prompts.dialog.collate_chat_history","title":"<code>collate_chat_history(inputs)</code>","text":"<p>Collate (human, ai) pairs into a single, string Args:     inputs: Returns:</p> Source code in <code>langroid/prompts/dialog.py</code> <pre><code>def collate_chat_history(inputs: List[tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Collate (human, ai) pairs into a single, string\n    Args:\n        inputs:\n    Returns:\n    \"\"\"\n    pairs = [\n        f\"\"\"Human:{human}\n        AI:{ai}\n        \"\"\"\n        for human, ai in inputs\n    ]\n    return \"\\n\".join(pairs)\n</code></pre>"},{"location":"reference/prompts/prompts_config/","title":"prompts_config","text":"<p>langroid/prompts/prompts_config.py </p>"},{"location":"reference/prompts/templates/","title":"templates","text":"<p>langroid/prompts/templates.py </p>"},{"location":"reference/prompts/transforms/","title":"transforms","text":"<p>langroid/prompts/transforms.py </p>"},{"location":"reference/prompts/transforms/#langroid.prompts.transforms.get_verbatim_extract_async","title":"<code>get_verbatim_extract_async(question, passage, LLM)</code>  <code>async</code>","text":"<p>Asynchronously, get verbatim extract from passage that is relevant to a question.</p> Source code in <code>langroid/prompts/transforms.py</code> <pre><code>async def get_verbatim_extract_async(\n    question: str,\n    passage: Document,\n    LLM: LanguageModel,\n) -&gt; str:\n    \"\"\"\n    Asynchronously, get verbatim extract from passage that is relevant to a question.\n    \"\"\"\n    async with aiohttp.ClientSession():\n        templatized_prompt = EXTRACTION_PROMPT\n        final_prompt = templatized_prompt.format(question=question, content=passage)\n        final_extract = await LLM.agenerate(prompt=final_prompt, max_tokens=1024)\n\n    return final_extract.message.strip()\n</code></pre>"},{"location":"reference/prompts/transforms/#langroid.prompts.transforms.get_verbatim_extracts","title":"<code>get_verbatim_extracts(question, passages, LLM)</code>","text":"<p>From each passage, extract verbatim text that is relevant to a question, using concurrent API calls to the LLM. Args:     question: question to be answered     passages: list of passages from which to extract relevant verbatim text     LLM: LanguageModel to use for generating the prompt and extract Returns:     list of verbatim extracts (Documents) from passages that are relevant to     question</p> Source code in <code>langroid/prompts/transforms.py</code> <pre><code>def get_verbatim_extracts(\n    question: str,\n    passages: List[Document],\n    LLM: LanguageModel,\n) -&gt; List[Document]:\n    \"\"\"\n    From each passage, extract verbatim text that is relevant to a question,\n    using concurrent API calls to the LLM.\n    Args:\n        question: question to be answered\n        passages: list of passages from which to extract relevant verbatim text\n        LLM: LanguageModel to use for generating the prompt and extract\n    Returns:\n        list of verbatim extracts (Documents) from passages that are relevant to\n        question\n    \"\"\"\n    return asyncio.run(_get_verbatim_extracts(question, passages, LLM))\n</code></pre>"},{"location":"reference/prompts/transforms/#langroid.prompts.transforms.followup_to_standalone","title":"<code>followup_to_standalone(LLM, chat_history, question)</code>","text":"<p>Given a chat history and a question, convert it to a standalone question. Args:     chat_history: list of tuples of (question, answer)     query: follow-up question</p> <p>Returns: standalone version of the question</p> Source code in <code>langroid/prompts/transforms.py</code> <pre><code>def followup_to_standalone(\n    LLM: LanguageModel, chat_history: List[Tuple[str, str]], question: str\n) -&gt; str:\n    \"\"\"\n    Given a chat history and a question, convert it to a standalone question.\n    Args:\n        chat_history: list of tuples of (question, answer)\n        query: follow-up question\n\n    Returns: standalone version of the question\n    \"\"\"\n    history = collate_chat_history(chat_history)\n\n    prompt = f\"\"\"\n    Given the conversationn below, and a follow-up question, rephrase the follow-up \n    question as a standalone question.\n\n    Chat history: {history}\n    Follow-up question: {question} \n    \"\"\".strip()\n    standalone = LLM.generate(prompt=prompt, max_tokens=1024).message.strip()\n    return standalone\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":"<p>langroid/utils/init.py </p>"},{"location":"reference/utils/configuration/","title":"configuration","text":"<p>langroid/utils/configuration.py </p>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.update_global_settings","title":"<code>update_global_settings(cfg, keys)</code>","text":"<p>Update global settings so modules can access them via (as an example): <pre><code>from langroid.utils.configuration import settings\nif settings.debug...\n</code></pre> Caution we do not want to have too many such global settings! Args:     cfg: pydantic config, typically from a main script     keys: which keys from cfg to use, to update the global settings object</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>def update_global_settings(cfg: BaseSettings, keys: List[str]) -&gt; None:\n    \"\"\"\n    Update global settings so modules can access them via (as an example):\n    ```\n    from langroid.utils.configuration import settings\n    if settings.debug...\n    ```\n    Caution we do not want to have too many such global settings!\n    Args:\n        cfg: pydantic config, typically from a main script\n        keys: which keys from cfg to use, to update the global settings object\n    \"\"\"\n    config_dict = cfg.dict()\n\n    # Filter the config_dict based on the keys\n    filtered_config = {key: config_dict[key] for key in keys if key in config_dict}\n\n    # create a new Settings() object to let pydantic validate it\n    new_settings = Settings(**filtered_config)\n\n    # Update the unique global settings object\n    settings.__dict__.update(new_settings.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.set_global","title":"<code>set_global(key_vals)</code>","text":"<p>Update the unique global settings object</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>def set_global(key_vals: Settings) -&gt; None:\n    \"\"\"Update the unique global settings object\"\"\"\n    settings.__dict__.update(key_vals.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.temporary_settings","title":"<code>temporary_settings(temp_settings)</code>","text":"<p>Temporarily update the global settings and restore them afterward.</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>@contextmanager\ndef temporary_settings(temp_settings: Settings) -&gt; Iterator[None]:\n    \"\"\"Temporarily update the global settings and restore them afterward.\"\"\"\n    original_settings = copy.deepcopy(settings)\n\n    set_global(temp_settings)\n\n    try:\n        yield\n    finally:\n        settings.__dict__.update(original_settings.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.quiet_mode","title":"<code>quiet_mode(quiet=True)</code>","text":"<p>Temporarily set quiet=True in global settings and restore afterward.</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>@contextmanager\ndef quiet_mode(quiet: bool = True) -&gt; Iterator[None]:\n    \"\"\"Temporarily set quiet=True in global settings and restore afterward.\"\"\"\n    original_settings = copy.deepcopy(settings)\n    if quiet:\n        temp_settings = original_settings.copy(update={\"quiet\": True})\n        set_global(temp_settings)\n\n    try:\n        yield\n    finally:\n        if quiet:\n            settings.__dict__.update(original_settings.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.set_env","title":"<code>set_env(settings)</code>","text":"<p>Set environment variables from a BaseSettings instance Args:     settings (BaseSettings): desired settings Returns:</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>def set_env(settings: BaseSettings) -&gt; None:\n    \"\"\"\n    Set environment variables from a BaseSettings instance\n    Args:\n        settings (BaseSettings): desired settings\n    Returns:\n    \"\"\"\n    for field_name, field in settings.__class__.__fields__.items():\n        env_var_name = field.field_info.extra.get(\"env\", field_name).upper()\n        os.environ[env_var_name] = str(settings.dict()[field_name])\n</code></pre>"},{"location":"reference/utils/constants/","title":"constants","text":"<p>langroid/utils/constants.py </p>"},{"location":"reference/utils/globals/","title":"globals","text":"<p>langroid/utils/globals.py </p>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState","title":"<code>GlobalState</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A base Pydantic model for global states.</p>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState.get_instance","title":"<code>get_instance()</code>  <code>classmethod</code>","text":"<p>Get the global instance of the specific subclass.</p> <p>Returns:</p> Type Description <code>GlobalState</code> <p>The global instance of the subclass.</p> Source code in <code>langroid/utils/globals.py</code> <pre><code>@classmethod\ndef get_instance(cls: Type[\"GlobalState\"]) -&gt; \"GlobalState\":\n    \"\"\"\n    Get the global instance of the specific subclass.\n\n    Returns:\n        The global instance of the subclass.\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = cls()\n    return cls._instance\n</code></pre>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState.set_values","title":"<code>set_values(**kwargs)</code>  <code>classmethod</code>","text":"<p>Set values on the global instance of the specific subclass.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Dict[str, Any]</code> <p>The fields and their values to set.</p> <code>{}</code> Source code in <code>langroid/utils/globals.py</code> <pre><code>@classmethod\ndef set_values(cls: Type[T], **kwargs: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Set values on the global instance of the specific subclass.\n\n    Args:\n        **kwargs: The fields and their values to set.\n    \"\"\"\n    instance = cls.get_instance()\n    for key, value in kwargs.items():\n        setattr(instance, key, value)\n</code></pre>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState.get_value","title":"<code>get_value(name)</code>  <code>classmethod</code>","text":"<p>Retrieve the value of a specific field from the global instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field to retrieve.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Any</code> <p>The value of the specified field.</p> Source code in <code>langroid/utils/globals.py</code> <pre><code>@classmethod\ndef get_value(cls: Type[T], name: str) -&gt; Any:\n    \"\"\"\n    Retrieve the value of a specific field from the global instance.\n\n    Args:\n        name (str): The name of the field to retrieve.\n\n    Returns:\n        str: The value of the specified field.\n    \"\"\"\n    instance = cls.get_instance()\n    return getattr(instance, name)\n</code></pre>"},{"location":"reference/utils/logging/","title":"logging","text":"<p>langroid/utils/logging.py </p>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_logger","title":"<code>setup_logger(name, level=logging.INFO)</code>","text":"<p>Set up a logger of module <code>name</code> at a desired level. Args:     name: module name     level: desired logging level Returns:     logger</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_logger(name: str, level: int = logging.INFO) -&gt; logging.Logger:\n    \"\"\"\n    Set up a logger of module `name` at a desired level.\n    Args:\n        name: module name\n        level: desired logging level\n    Returns:\n        logger\n    \"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    if not logger.hasHandlers():\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n    return logger\n</code></pre>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_loggers_for_package","title":"<code>setup_loggers_for_package(package_name, level)</code>","text":"<p>Set up loggers for all modules in a package. This ensures that log-levels of modules outside the package are not affected. Args:     package_name: main package name     level: desired logging level Returns:</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_loggers_for_package(package_name: str, level: int) -&gt; None:\n    \"\"\"\n    Set up loggers for all modules in a package.\n    This ensures that log-levels of modules outside the package are not affected.\n    Args:\n        package_name: main package name\n        level: desired logging level\n    Returns:\n    \"\"\"\n    import importlib\n    import pkgutil\n\n    package = importlib.import_module(package_name)\n    for _, module_name, _ in pkgutil.walk_packages(\n        package.__path__, package.__name__ + \".\"\n    ):\n        module = importlib.import_module(module_name)\n        setup_logger(module.__name__, level)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/","title":"pydantic_utils","text":"<p>langroid/utils/pydantic_utils.py </p>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.has_field","title":"<code>has_field(model_class, field_name)</code>","text":"<p>Check if a Pydantic model class has a field with the given name.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def has_field(model_class: Type[BaseModel], field_name: str) -&gt; bool:\n    \"\"\"Check if a Pydantic model class has a field with the given name.\"\"\"\n    return field_name in model_class.__fields__\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.flatten_pydantic_model","title":"<code>flatten_pydantic_model(model, base_model=BaseModel)</code>","text":"<p>Given a possibly nested Pydantic class, return a flattened version of it, by constructing top-level fields, whose names are formed from the path through the nested structure, separated by double underscores.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model to flatten.</p> required <code>base_model</code> <code>Type[BaseModel]</code> <p>The base model to use for the flattened model. Defaults to BaseModel.</p> <code>BaseModel</code> <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>Type[BaseModel]: The flattened Pydantic model.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def flatten_pydantic_model(\n    model: Type[BaseModel],\n    base_model: Type[BaseModel] = BaseModel,\n) -&gt; Type[BaseModel]:\n    \"\"\"\n    Given a possibly nested Pydantic class, return a flattened version of it,\n    by constructing top-level fields, whose names are formed from the path\n    through the nested structure, separated by double underscores.\n\n    Args:\n        model (Type[BaseModel]): The Pydantic model to flatten.\n        base_model (Type[BaseModel], optional): The base model to use for the\n            flattened model. Defaults to BaseModel.\n\n    Returns:\n        Type[BaseModel]: The flattened Pydantic model.\n    \"\"\"\n\n    flattened_fields: Dict[str, Any] = {}\n    models_to_process = [(model, \"\")]\n\n    while models_to_process:\n        current_model, current_prefix = models_to_process.pop()\n\n        for name, field in current_model.__fields__.items():\n            if isinstance(field.outer_type_, type) and issubclass(\n                field.outer_type_, BaseModel\n            ):\n                new_prefix = (\n                    f\"{current_prefix}{name}__\" if current_prefix else f\"{name}__\"\n                )\n                models_to_process.append((field.outer_type_, new_prefix))\n            else:\n                flattened_name = f\"{current_prefix}{name}\"\n\n                if field.default_factory is not field.default_factory:\n                    flattened_fields[flattened_name] = (\n                        field.outer_type_,\n                        field.default_factory,\n                    )\n                elif field.default is not field.default:\n                    flattened_fields[flattened_name] = (\n                        field.outer_type_,\n                        field.default,\n                    )\n                else:\n                    flattened_fields[flattened_name] = (field.outer_type_, ...)\n\n    return create_model(\"FlatModel\", __base__=base_model, **flattened_fields)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.flatten_pydantic_instance","title":"<code>flatten_pydantic_instance(instance, prefix='', force_str=False)</code>","text":"<p>Given a possibly nested Pydantic instance, return a flattened version of it, as a dict where nested traversal paths are translated to keys a__b__c.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>BaseModel</code> <p>The Pydantic instance to flatten.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use for the top-level fields.</p> <code>''</code> <code>force_str</code> <code>bool</code> <p>Whether to force all values to be strings.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The flattened dict.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def flatten_pydantic_instance(\n    instance: BaseModel,\n    prefix: str = \"\",\n    force_str: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Given a possibly nested Pydantic instance, return a flattened version of it,\n    as a dict where nested traversal paths are translated to keys a__b__c.\n\n    Args:\n        instance (BaseModel): The Pydantic instance to flatten.\n        prefix (str, optional): The prefix to use for the top-level fields.\n        force_str (bool, optional): Whether to force all values to be strings.\n\n    Returns:\n        Dict[str, Any]: The flattened dict.\n\n    \"\"\"\n    flat_data: Dict[str, Any] = {}\n    for name, value in instance.dict().items():\n        # Assuming nested pydantic model will be a dict here\n        if isinstance(value, dict):\n            nested_flat_data = flatten_pydantic_instance(\n                instance.__fields__[name].type_(**value),\n                prefix=f\"{prefix}{name}__\",\n                force_str=force_str,\n            )\n            flat_data.update(nested_flat_data)\n        else:\n            flat_data[f\"{prefix}{name}\"] = str(value) if force_str else value\n    return flat_data\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.nested_dict_from_flat","title":"<code>nested_dict_from_flat(flat_data, sub_dict='')</code>","text":"<p>Given a flattened version of a nested dict, reconstruct the nested dict. Field names in the flattened dict are assumed to be of the form \"field1__field2__field3\", going from top level down.</p> <p>Parameters:</p> Name Type Description Default <code>flat_data</code> <code>Dict[str, Any]</code> <p>The flattened dict.</p> required <code>sub_dict</code> <code>str</code> <p>The name of the sub-dict to extract from the flattened dict. Defaults to \"\" (extract the whole dict).</p> <code>''</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The nested dict.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def nested_dict_from_flat(\n    flat_data: Dict[str, Any],\n    sub_dict: str = \"\",\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Given a flattened version of a nested dict, reconstruct the nested dict.\n    Field names in the flattened dict are assumed to be of the form\n    \"field1__field2__field3\", going from top level down.\n\n    Args:\n        flat_data (Dict[str, Any]): The flattened dict.\n        sub_dict (str, optional): The name of the sub-dict to extract from the\n            flattened dict. Defaults to \"\" (extract the whole dict).\n\n    Returns:\n        Dict[str, Any]: The nested dict.\n\n    \"\"\"\n    nested_data: Dict[str, Any] = {}\n    for key, value in flat_data.items():\n        if sub_dict != \"\" and not key.startswith(sub_dict + \"__\"):\n            continue\n        keys = key.split(\"__\")\n        d = nested_data\n        for k in keys[:-1]:\n            d = d.setdefault(k, {})\n        d[keys[-1]] = value\n    if sub_dict != \"\":  # e.g. \"payload\"\n        nested_data = nested_data[sub_dict]\n    return nested_data\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.pydantic_obj_from_flat_dict","title":"<code>pydantic_obj_from_flat_dict(flat_data, model, sub_dict='')</code>","text":"<p>flatened dict with a__b__c style keys -&gt; nested dict -&gt; pydantic object</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def pydantic_obj_from_flat_dict(\n    flat_data: Dict[str, Any],\n    model: Type[BaseModel],\n    sub_dict: str = \"\",\n) -&gt; BaseModel:\n    \"\"\"flatened dict with a__b__c style keys -&gt; nested dict -&gt; pydantic object\"\"\"\n    nested_data = nested_dict_from_flat(flat_data, sub_dict)\n    return model(**nested_data)\n</code></pre>"},{"location":"reference/utils/system/","title":"system","text":"<p>langroid/utils/system.py </p>"},{"location":"reference/utils/system/#langroid.utils.system.rmdir","title":"<code>rmdir(path)</code>","text":"<p>Remove a directory recursively. Args:     path (str): path to directory to remove Returns:     True if a dir was removed, false otherwise. Raises error if failed to remove.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def rmdir(path: str) -&gt; bool:\n    \"\"\"\n    Remove a directory recursively.\n    Args:\n        path (str): path to directory to remove\n    Returns:\n        True if a dir was removed, false otherwise. Raises error if failed to remove.\n    \"\"\"\n    if not any([path.startswith(p) for p in DELETION_ALLOWED_PATHS]):\n        raise ValueError(\n            f\"\"\"\n        Removing Dir '{path}' not allowed. \n        Must start with one of {DELETION_ALLOWED_PATHS}\n        This is a safety measure to prevent accidental deletion of files.\n        If you are sure you want to delete this directory, please add it \n        to the `DELETION_ALLOWED_PATHS` list in langroid/utils/system.py and \n        re-run the command.\n        \"\"\"\n        )\n\n    try:\n        shutil.rmtree(path)\n    except FileNotFoundError:\n        logger.warning(f\"Directory '{path}' does not exist. No action taken.\")\n        return False\n    except Exception as e:\n        logger.error(f\"Error while removing directory '{path}': {e}\")\n    return True\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.caller_name","title":"<code>caller_name()</code>","text":"<p>Who called the function?</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def caller_name() -&gt; str:\n    \"\"\"\n    Who called the function?\n    \"\"\"\n    frame = inspect.currentframe()\n    if frame is None:\n        return \"\"\n\n    caller_frame = frame.f_back\n\n    # If there's no caller frame, the function was called from the global scope\n    if caller_frame is None:\n        return \"\"\n\n    return caller_frame.f_code.co_name\n</code></pre>"},{"location":"reference/utils/algorithms/","title":"algorithms","text":"<p>langroid/utils/algorithms/init.py </p>"},{"location":"reference/utils/algorithms/graph/","title":"graph","text":"<p>langroid/utils/algorithms/graph.py </p> <p>Graph algos.</p>"},{"location":"reference/utils/algorithms/graph/#langroid.utils.algorithms.graph.topological_sort","title":"<code>topological_sort(order)</code>","text":"<p>Given a directed adjacency matrix, return a topological sort of the nodes. order[i,j] = -1 means there is an edge from i to j. order[i,j] = 0 means there is no edge from i to j. order[i,j] = 1 means there is an edge from j to i.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>array</code> <p>The adjacency matrix.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: The topological sort of the nodes.</p> Source code in <code>langroid/utils/algorithms/graph.py</code> <pre><code>@no_type_check\ndef topological_sort(order: np.array) -&gt; List[int]:\n    \"\"\"\n    Given a directed adjacency matrix, return a topological sort of the nodes.\n    order[i,j] = -1 means there is an edge from i to j.\n    order[i,j] = 0 means there is no edge from i to j.\n    order[i,j] = 1 means there is an edge from j to i.\n\n    Args:\n        order (np.array): The adjacency matrix.\n\n    Returns:\n        List[int]: The topological sort of the nodes.\n\n    \"\"\"\n    n = order.shape[0]\n\n    # Calculate the in-degrees\n    in_degree = [0] * n\n    for i in range(n):\n        for j in range(n):\n            if order[i, j] == -1:\n                in_degree[j] += 1\n\n    # Initialize the queue with nodes of in-degree 0\n    queue = [i for i in range(n) if in_degree[i] == 0]\n    result = []\n\n    while queue:\n        node = queue.pop(0)\n        result.append(node)\n\n        for i in range(n):\n            if order[node, i] == -1:\n                in_degree[i] -= 1\n                if in_degree[i] == 0:\n                    queue.append(i)\n\n    assert len(result) == n, \"Cycle detected\"\n    return result\n</code></pre>"},{"location":"reference/utils/output/","title":"output","text":"<p>langroid/utils/output/init.py </p>"},{"location":"reference/utils/output/printing/","title":"printing","text":"<p>langroid/utils/output/printing.py </p>"},{"location":"reference/utils/output/printing/#langroid.utils.output.printing.PrintColored","title":"<code>PrintColored(color)</code>","text":"<p>Context to temporarily print in a desired color</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>def __init__(self, color: str):\n    self.color = color\n</code></pre>"},{"location":"reference/utils/output/printing/#langroid.utils.output.printing.silence_stdout","title":"<code>silence_stdout()</code>","text":"<p>Temporarily silence all output to stdout and from rich.print.</p> <p>This context manager redirects all output written to stdout (which includes outputs from the built-in print function and rich.print) to /dev/null on UNIX-like systems or NUL on Windows. Once the context block exits, stdout is restored to its original state.</p> Example <p>with silence_stdout_and_rich():     print(\"This won't be printed\")     rich.print(\"This also won't be printed\")</p> Note <p>This suppresses both standard print functions and the rich library outputs.</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>@contextmanager\ndef silence_stdout() -&gt; Iterator[None]:\n    \"\"\"\n    Temporarily silence all output to stdout and from rich.print.\n\n    This context manager redirects all output written to stdout (which includes\n    outputs from the built-in print function and rich.print) to /dev/null on\n    UNIX-like systems or NUL on Windows. Once the context block exits, stdout is\n    restored to its original state.\n\n    Example:\n        with silence_stdout_and_rich():\n            print(\"This won't be printed\")\n            rich.print(\"This also won't be printed\")\n\n    Note:\n        This suppresses both standard print functions and the rich library outputs.\n    \"\"\"\n    platform_null = \"/dev/null\" if sys.platform != \"win32\" else \"NUL\"\n    original_stdout = sys.stdout\n    fnull = open(platform_null, \"w\")\n    sys.stdout = fnull\n    try:\n        yield\n    finally:\n        sys.stdout = original_stdout\n        fnull.close()\n</code></pre>"},{"location":"reference/vector_store/","title":"vector_store","text":"<p>langroid/vector_store/init.py </p>"},{"location":"reference/vector_store/base/","title":"base","text":"<p>langroid/vector_store/base.py </p>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore","title":"<code>VectorStore(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a vector store.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def __init__(self, config: VectorStoreConfig):\n    self.config = config\n    self.embedding_model = EmbeddingModel.create(config.embedding)\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.clear_empty_collections","title":"<code>clear_empty_collections()</code>  <code>abstractmethod</code>","text":"<p>Clear all empty collections in the vector store. Returns the number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_empty_collections(self) -&gt; int:\n    \"\"\"Clear all empty collections in the vector store.\n    Returns the number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>  <code>abstractmethod</code>","text":"<p>Clear all collections in the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>really</code> <code>bool</code> <p>Whether to really clear all collections. Defaults to False.</p> <code>False</code> <code>prefix</code> <code>str</code> <p>Prefix of collections to clear.</p> <code>''</code> <p>Returns:     int: Number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"\n    Clear all collections in the vector store.\n\n    Args:\n        really (bool, optional): Whether to really clear all collections.\n            Defaults to False.\n        prefix (str, optional): Prefix of collections to clear.\n    Returns:\n        int: Number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.list_collections","title":"<code>list_collections(empty=False)</code>  <code>abstractmethod</code>","text":"<p>List all collections in the vector store (only non empty collections if empty=False).</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"List all collections in the vector store\n    (only non empty collections if empty=False).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.set_collection","title":"<code>set_collection(collection_name, replace=False)</code>","text":"<p>Set the current collection to the given collection name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the collection if it         already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def set_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Set the current collection to the given collection name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the collection if it\n            already exists. Defaults to False.\n    \"\"\"\n\n    self.config.collection_name = collection_name\n    if collection_name not in self.list_collections() or replace:\n        self.create_collection(collection_name, replace=replace)\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>  <code>abstractmethod</code>","text":"<p>Create a collection with the given name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the         collection if it already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"Create a collection with the given name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the\n            collection if it already exists. Defaults to False.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.similar_texts_with_scores","title":"<code>similar_texts_with_scores(text, k=1, where=None)</code>  <code>abstractmethod</code>","text":"<p>Find k most similar texts to the given text, in terms of vector distance metric (e.g., cosine similarity).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to find similar texts for.</p> required <code>k</code> <code>int</code> <p>Number of similar texts to retrieve. Defaults to 1.</p> <code>1</code> <code>where</code> <code>Optional[str]</code> <p>Where clause to filter the search.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document,float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef similar_texts_with_scores(\n    self,\n    text: str,\n    k: int = 1,\n    where: Optional[str] = None,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Find k most similar texts to the given text, in terms of vector distance metric\n    (e.g., cosine similarity).\n\n    Args:\n        text (str): The text to find similar texts for.\n        k (int, optional): Number of similar texts to retrieve. Defaults to 1.\n        where (Optional[str], optional): Where clause to filter the search.\n\n    Returns:\n        List[Tuple[Document,float]]: List of (Document, score) tuples.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.add_context_window","title":"<code>add_context_window(docs_scores, neighbors=0)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. These window_ids may overlap, so we - gather connected-components of overlapping windows, - split each component into roughly equal parts, - create a new document for each part, preserving metadata,</p> <p>We may have stored a longer set of window_ids than we need during chunking. Now, we just want <code>neighbors</code> on each side of the center of the window_ids list.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <code>neighbors</code> <code>int</code> <p>Number of neighbors on \"each side\" of match to retrieve. Defaults to 0. \"Each side\" here means before and after the match, in the original text.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def add_context_window(\n    self, docs_scores: List[Tuple[Document, float]], neighbors: int = 0\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk.\n    These window_ids may overlap, so we\n    - gather connected-components of overlapping windows,\n    - split each component into roughly equal parts,\n    - create a new document for each part, preserving metadata,\n\n    We may have stored a longer set of window_ids than we need during chunking.\n    Now, we just want `neighbors` on each side of the center of the window_ids list.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n        neighbors (int, optional): Number of neighbors on \"each side\" of match to\n            retrieve. Defaults to 0.\n            \"Each side\" here means before and after the match,\n            in the original text.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    # We return a larger context around each match, i.e.\n    # a window of `neighbors` on each side of the match.\n    docs = [d for d, s in docs_scores]\n    scores = [s for d, s in docs_scores]\n    if neighbors == 0:\n        return docs_scores\n    doc_chunks = [d for d in docs if d.metadata.is_chunk]\n    if len(doc_chunks) == 0:\n        return docs_scores\n    window_ids_list = []\n    id2metadata = {}\n    # id -&gt; highest score of a doc it appears in\n    id2max_score: Dict[int | str, float] = {}\n    for i, d in enumerate(docs):\n        window_ids = d.metadata.window_ids\n        if len(window_ids) == 0:\n            window_ids = [d.id()]\n        id2metadata.update({id: d.metadata for id in window_ids})\n\n        id2max_score.update(\n            {id: max(id2max_score.get(id, 0), scores[i]) for id in window_ids}\n        )\n        n = len(window_ids)\n        chunk_idx = window_ids.index(d.id())\n        neighbor_ids = window_ids[\n            max(0, chunk_idx - neighbors) : min(n, chunk_idx + neighbors + 1)\n        ]\n        window_ids_list += [neighbor_ids]\n\n    # window_ids could be from different docs,\n    # and they may overlap, so we first remove overlaps\n    window_ids_list = self.remove_overlaps(window_ids_list)\n    final_docs = []\n    final_scores = []\n    for w in window_ids_list:\n        metadata = copy.deepcopy(id2metadata[w[0]])\n        metadata.window_ids = w\n        document = Document(\n            content=\" \".join([d.content for d in self.get_documents_by_ids(w)]),\n            metadata=metadata,\n        )\n        # make a fresh id since content is in general different\n        document.metadata.id = document.hash_id(document.content)\n        final_docs += [document]\n        final_scores += [max(id2max_score[id] for id in w)]\n    return list(zip(final_docs, final_scores))\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.remove_overlaps","title":"<code>remove_overlaps(windows)</code>  <code>staticmethod</code>","text":"<p>Given a collection of windows, where each window is a sequence of ids, identify groups of overlapping windows, and for each overlapping k-group, split the ids into k roughly equal sequences.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>List[int | str]</code> <p>List of windows, where each window is a sequence of ids.</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List[int|str]: List of windows, where each window is a sequence of ids, and no two windows overlap.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@staticmethod\ndef remove_overlaps(windows: List[List[str]]) -&gt; List[List[str]]:\n    \"\"\"\n    Given a collection of windows, where each window is a sequence of ids,\n    identify groups of overlapping windows, and for each overlapping k-group,\n    split the ids into k roughly equal sequences.\n\n    Args:\n        windows (List[int|str]): List of windows, where each window is a\n            sequence of ids.\n\n    Returns:\n        List[int|str]: List of windows, where each window is a sequence of ids,\n            and no two windows overlap.\n    \"\"\"\n    ids = set(id for w in windows for id in w)\n    # id -&gt; {win -&gt; # pos}\n    id2win2pos: Dict[str, Dict[int, int]] = {id: {} for id in ids}\n\n    for i, w in enumerate(windows):\n        for j, id in enumerate(w):\n            id2win2pos[id][i] = j\n\n    n = len(windows)\n    # relation between windows:\n    order = np.zeros((n, n), dtype=np.int8)\n    for i, w in enumerate(windows):\n        for j, x in enumerate(windows):\n            if i == j:\n                continue\n            if len(set(w).intersection(x)) == 0:\n                continue\n            id = list(set(w).intersection(x))[0]  # any common id\n            if id2win2pos[id][i] &gt; id2win2pos[id][j]:\n                order[i, j] = -1  # win i is before win j\n            else:\n                order[i, j] = 1  # win i is after win j\n\n    # find groups of windows that overlap, like connected components in a graph\n    groups = [[0]]\n    for i in range(1, n):\n        found = False\n        for g in groups:\n            if any(order[i, j] != 0 for j in g):\n                g.append(i)\n                found = True\n                break\n        if not found:\n            groups.append([i])\n\n    # split each group into roughly equal parts\n    new_windows = []\n    max_window_len = max(len(w) for w in windows)\n    for g in groups:\n        # find total ordering among windows in group based on order matrix\n        # (this is a topological sort)\n        _g = np.array(g)\n        order_matrix = order[_g][:, _g]\n        ordered_window_indices = topological_sort(order_matrix)\n        ordered_window_ids = [windows[i] for i in _g[ordered_window_indices]]\n        flattened = [id for w in ordered_window_ids for id in w]\n        flattened_deduped = list(dict.fromkeys(flattened))\n        # split into k parts where k is the smallest integer such that\n        # each part has length &lt;= max_window_len\n        k = max(1, int(ceil(len(flattened_deduped) / max_window_len)))\n        new_windows += np.array_split(flattened_deduped, k)\n\n    return [w.tolist() for w in new_windows]\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.get_all_documents","title":"<code>get_all_documents()</code>  <code>abstractmethod</code>","text":"<p>Get all documents in the current collection.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_all_documents(self) -&gt; List[Document]:\n    \"\"\"\n    Get all documents in the current collection.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.get_documents_by_ids","title":"<code>get_documents_by_ids(ids)</code>  <code>abstractmethod</code>","text":"<p>Get documents by their ids. Args:     ids (List[str]): List of document ids.</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\n    \"\"\"\n    Get documents by their ids.\n    Args:\n        ids (List[str]): List of document ids.\n\n    Returns:\n        List[Document]: List of documents\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/chromadb/","title":"chromadb","text":"<p>langroid/vector_store/chromadb.py </p>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB","title":"<code>ChromaDB(config)</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def __init__(self, config: ChromaDBConfig):\n    super().__init__(config)\n    self.config = config\n    emb_model = EmbeddingModel.create(config.embedding)\n    self.embedding_fn = emb_model.embedding_fn()\n    self.client = chromadb.Client(\n        chromadb.config.Settings(\n            # chroma_db_impl=\"duckdb+parquet\",\n            persist_directory=config.storage_path,\n        )\n    )\n    if self.config.collection_name is not None:\n        self.create_collection(\n            self.config.collection_name,\n            replace=self.config.replace_collection,\n        )\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections in the vector store with the given prefix.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections in the vector store with the given prefix.\"\"\"\n\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll = [c for c in self.client.list_collections() if c.name.startswith(prefix)]\n    if len(coll) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for c in coll:\n        n_empty_deletes += c.count() == 0\n        n_non_empty_deletes += c.count() &gt; 0\n        self.client.delete_collection(name=c.name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and \n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>List non-empty collections in the vector store. Args:     empty (bool, optional): Whether to list empty collections. Returns:     List[str]: List of non-empty collection names.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    List non-empty collections in the vector store.\n    Args:\n        empty (bool, optional): Whether to list empty collections.\n    Returns:\n        List[str]: List of non-empty collection names.\n    \"\"\"\n    colls = self.client.list_collections()\n    if empty:\n        return [coll.name for coll in colls]\n    return [coll.name for coll in colls if coll.count() &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection in the vector store, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create or replace.     replace (bool, optional): Whether to replace an existing collection.         Defaults to False.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection in the vector store, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create or replace.\n        replace (bool, optional): Whether to replace an existing collection.\n            Defaults to False.\n\n    \"\"\"\n    self.config.collection_name = collection_name\n    self.collection = self.client.create_collection(\n        name=self.config.collection_name,\n        embedding_function=self.embedding_fn,\n        get_or_create=not replace,\n    )\n</code></pre>"},{"location":"reference/vector_store/lancedb/","title":"lancedb","text":"<p>langroid/vector_store/lancedb.py </p>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB","title":"<code>LanceDB(config)</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def __init__(self, config: LanceDBConfig):\n    super().__init__(config)\n    self.config: LanceDBConfig = config\n    emb_model = EmbeddingModel.create(config.embedding)\n    self.embedding_fn: EmbeddingFunction = emb_model.embedding_fn()\n    self.embedding_dim = emb_model.embedding_dims\n    self.host = config.host\n    self.port = config.port\n    self.schema = self._create_lance_schema(self.config.document_class)\n    self.flat_schema = self._create_flat_lance_schema(self.config.document_class)\n    load_dotenv()\n    if self.config.cloud:\n        logger.warning(\n            \"LanceDB Cloud is not available yet. Switching to local storage.\"\n        )\n        config.cloud = False\n    else:\n        try:\n            self.client = lancedb.connect(\n                uri=config.storage_path,\n            )\n        except Exception as e:\n            new_storage_path = config.storage_path + \".new\"\n            logger.warning(\n                f\"\"\"\n                Error connecting to local LanceDB at {config.storage_path}:\n                {e}\n                Switching to {new_storage_path}\n                \"\"\"\n            )\n            self.client = lancedb.connect(\n                uri=new_storage_path,\n            )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of vecdb until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [\n        c for c in self.list_collections(empty=True) if c.startswith(prefix)\n    ]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for name in coll_names:\n        nr = self.client.open_table(name).head(1).shape[0]\n        n_empty_deletes += nr == 0\n        n_non_empty_deletes += nr &gt; 0\n        self.client.drop_table(name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and \n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n    colls = self.client.table_names()\n    if len(colls) == 0:\n        return []\n    if empty:  # include empty tbls\n        return colls  # type: ignore\n    counts = [self.client.open_table(coll).head(1).shape[0] for coll in colls]\n    return [coll for coll, count in zip(colls, counts) if count &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    self.config.collection_name = collection_name\n    collections = self.list_collections()\n    if collection_name in collections:\n        coll = self.client.open_table(collection_name)\n        if coll.head().shape[0] &gt; 0:\n            logger.warning(f\"Non-empty Collection {collection_name} already exists\")\n            if not replace:\n                logger.warning(\"Not replacing collection\")\n                return\n            else:\n                logger.warning(\"Recreating fresh collection\")\n    tbl = self.client.create_table(\n        collection_name, schema=self.flat_schema, mode=\"overwrite\"\n    )\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(tbl.schema)\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/meilisearch/","title":"meilisearch","text":"<p>langroid/vector_store/meilisearch.py </p> <p>MeiliSearch as a pure document store, without its (experimental) vector-store functionality. We aim to use MeiliSearch for fast lexical search. Note that what we call \"Collection\" in Langroid is referred to as \"Index\" in MeiliSearch. Each data-store has its own terminology, but for uniformity we use the Langroid terminology here.</p>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch","title":"<code>MeiliSearch(config)</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def __init__(self, config: MeiliSearchConfig):\n    super().__init__(config)\n    self.config: MeiliSearchConfig = config\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    self.key = os.getenv(\"MEILISEARCH_API_KEY\") or \"masterKey\"\n    self.url = os.getenv(\"MEILISEARCH_API_URL\") or f\"http://{self.host}:{self.port}\"\n    if config.cloud and None in [self.key, self.url]:\n        logger.warning(\n            f\"\"\"MEILISEARCH_API_KEY, MEILISEARCH_API_URL env variable must be set \n            to use MeiliSearch in cloud mode. Please set these values \n            in your .env file. Switching to local MeiliSearch at \n            {self.url} \n            \"\"\"\n        )\n        config.cloud = False\n\n    self.client: Callable[[], meilisearch.AsyncClient] = lambda: (\n        meilisearch.AsyncClient(url=self.url, api_key=self.key)\n    )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of db until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.clear_empty_collections","title":"<code>clear_empty_collections()</code>","text":"<p>All collections are treated as non-empty in MeiliSearch, so this is a no-op</p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def clear_empty_collections(self) -&gt; int:\n    \"\"\"All collections are treated as non-empty in MeiliSearch, so this is a\n    no-op\"\"\"\n    return 0\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Delete all indices whose names start with <code>prefix</code></p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Delete all indices whose names start with `prefix`\"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [c for c in self.list_collections() if c.startswith(prefix)]\n    deletes = asyncio.run(self._async_delete_indices(coll_names))\n    n_deletes = sum(deletes)\n    logger.warning(f\"Deleted {n_deletes} indices in MeiliSearch\")\n    return n_deletes\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of index names stored. We treat any existing index as non-empty.</p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of index names stored. We treat any existing index as non-empty.\n    \"\"\"\n    indexes = asyncio.run(self._async_get_indexes())\n    if len(indexes) == 0:\n        return []\n    else:\n        return [ind.uid for ind in indexes]\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    self.config.collection_name = collection_name\n    collections = self.list_collections()\n    if collection_name in collections:\n        logger.warning(\n            f\"MeiliSearch Non-empty Index {collection_name} already exists\"\n        )\n        if not replace:\n            logger.warning(\"Not replacing collection\")\n            return\n        else:\n            logger.warning(\"Recreating fresh collection\")\n            asyncio.run(self._async_delete_index(collection_name))\n    asyncio.run(self._async_create_index(collection_name))\n    collection_info = asyncio.run(self._async_get_index(collection_name))\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(collection_info)\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/momento/","title":"momento","text":"<p>langroid/vector_store/momento.py </p> <p>Momento Vector Index. https://docs.momentohq.com/vector-index/develop/api-reference</p>"},{"location":"reference/vector_store/momento/#langroid.vector_store.momento.MomentoVI","title":"<code>MomentoVI(config)</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/momento.py</code> <pre><code>def __init__(self, config: MomentoVIConfig):\n    super().__init__(config)\n    self.config: MomentoVIConfig = config\n    emb_model = EmbeddingModel.create(config.embedding)\n    self.embedding_fn: EmbeddingFunction = emb_model.embedding_fn()\n    self.embedding_dim = emb_model.embedding_dims\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    api_key = os.getenv(\"MOMENTO_API_KEY\")\n    if config.cloud:\n        if api_key is None:\n            raise ValueError(\n                \"\"\"MOMENTO_API_KEY env variable must be set to \n                MomentoVI hosted service. Please set this in your .env file. \n                \"\"\"\n            )\n        self.client = PreviewVectorIndexClient(\n            configuration=VectorIndexConfigurations.Default.latest(),\n            credential_provider=CredentialProvider.from_string(api_key),\n        )\n    else:\n        raise NotImplementedError(\"MomentoVI local not available yet\")\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of vecdb until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/momento/#langroid.vector_store.momento.MomentoVI.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/momento.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = self.list_collections(empty=False)\n    coll_names = [name for name in coll_names if name.startswith(prefix)]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    for name in coll_names:\n        self.delete_collection(name)\n    logger.warning(\n        f\"\"\"\n        Deleted {len(coll_names)} indices from Momento VI\n        \"\"\"\n    )\n    return len(coll_names)\n</code></pre>"},{"location":"reference/vector_store/momento/#langroid.vector_store.momento.MomentoVI.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/momento.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n    response = self.client.list_indexes()\n    if isinstance(response, mvi_response.ListIndexes.Success):\n        return response.index_names\n    elif isinstance(response, mvi_response.ListIndexes.Error):\n        raise ValueError(f\"Error listing collections: {response.message}\")\n    else:\n        raise ValueError(f\"Unexpected response: {response}\")\n</code></pre>"},{"location":"reference/vector_store/momento/#langroid.vector_store.momento.MomentoVI.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/momento.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    self.config.collection_name = collection_name\n    response = self.client.create_index(\n        index_name=collection_name,\n        num_dimensions=self.embedding_dim,\n        similarity_metric=self.config.distance,\n    )\n    if isinstance(response, mvi_response.CreateIndex.Success):\n        logger.info(f\"Created collection {collection_name}\")\n    elif isinstance(response, mvi_response.CreateIndex.IndexAlreadyExists):\n        logger.warning(f\"Collection {collection_name} already exists\")\n    elif isinstance(response, mvi_response.CreateIndex.Error):\n        raise ValueError(\n            f\"Error creating collection {collection_name}: {response.message}\"\n        )\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(f\"Collection {collection_name} created\")\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/","title":"qdrantdb","text":"<p>langroid/vector_store/qdrantdb.py </p>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB","title":"<code>QdrantDB(config)</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def __init__(self, config: QdrantDBConfig):\n    super().__init__(config)\n    self.config = config\n    emb_model = EmbeddingModel.create(config.embedding)\n    self.embedding_fn: EmbeddingFunction = emb_model.embedding_fn()\n    self.embedding_dim = emb_model.embedding_dims\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    key = os.getenv(\"QDRANT_API_KEY\")\n    url = os.getenv(\"QDRANT_API_URL\")\n    if config.cloud and None in [key, url]:\n        logger.warning(\n            f\"\"\"QDRANT_API_KEY, QDRANT_API_URL env variable must be set to use \n            QdrantDB in cloud mode. Please set these values \n            in your .env file. \n            Switching to local storage at {config.storage_path} \n            \"\"\"\n        )\n        config.cloud = False\n    if config.cloud:\n        self.client = QdrantClient(\n            url=url,\n            api_key=key,\n            timeout=config.timeout,\n        )\n    else:\n        try:\n            self.client = QdrantClient(\n                path=config.storage_path,\n            )\n        except Exception as e:\n            new_storage_path = config.storage_path + \".new\"\n            logger.warning(\n                f\"\"\"\n                Error connecting to local QdrantDB at {config.storage_path}:\n                {e}\n                Switching to {new_storage_path}\n                \"\"\"\n            )\n            self.client = QdrantClient(\n                path=new_storage_path,\n            )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of vecdb until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [\n        c for c in self.list_collections(empty=True) if c.startswith(prefix)\n    ]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for name in coll_names:\n        info = self.client.get_collection(collection_name=name)\n        n_empty_deletes += info.points_count == 0\n        n_non_empty_deletes += info.points_count &gt; 0\n        self.client.delete_collection(collection_name=name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and \n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n    colls = list(self.client.get_collections())[0][1]\n    if empty:\n        return [coll.name for coll in colls]\n    counts = []\n    for coll in colls:\n        try:\n            counts.append(\n                self.client.get_collection(collection_name=coll.name).points_count\n            )\n        except Exception:\n            logger.warning(f\"Error getting collection {coll.name}\")\n            counts.append(0)\n    return [coll.name for coll, count in zip(colls, counts) if count &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    self.config.collection_name = collection_name\n    collections = self.list_collections()\n    if collection_name in collections:\n        coll = self.client.get_collection(collection_name=collection_name)\n        if coll.status == CollectionStatus.GREEN and coll.points_count &gt; 0:\n            logger.warning(f\"Non-empty Collection {collection_name} already exists\")\n            if not replace:\n                logger.warning(\"Not replacing collection\")\n                return\n            else:\n                logger.warning(\"Recreating fresh collection\")\n    self.client.recreate_collection(\n        collection_name=collection_name,\n        vectors_config=VectorParams(\n            size=self.embedding_dim,\n            distance=Distance.COSINE,\n        ),\n    )\n    collection_info = self.client.get_collection(collection_name=collection_name)\n    assert collection_info.status == CollectionStatus.GREEN\n    assert collection_info.vectors_count == 0\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(collection_info)\n        logger.setLevel(level)\n</code></pre>"},{"location":"tutorials/non-openai-llms/","title":"Using Langroid with Non-OpenAI LLMs","text":"<p>Langroid was initially written to work with OpenAI models via their API. This may sound limiting, but fortunately there are tools that provide an OpenAI-like API  for hundreds of LLM providers.  We show below how you can define an <code>OpenAIGPTConfig</code> object for these scenarios. This config object then be used to create a Langroid  LLM object to interact with directly, or you can wrap it into an Agent and a Task to create a chat loop or a more complex multi-agent setup where different agents may be using different LLMs.</p>"},{"location":"tutorials/non-openai-llms/#litellm-openai-proxy-server","title":"LiteLLM OpenAI Proxy Server","text":"<p>LiteLLM is an excellent library which, among other things (see below), offers a proxy server that allows you  spin up a server acting as a proxy for a variety of LLM models (over a 100 providers!) at an  OpenAI-like endpoint. This means you can continue to use the <code>openai</code> python client,  except that you will need to change the <code>openai.api_base</code> to point to the proxy server's URL (this is done behind the scenes in Langroid via the <code>chat_model</code> name as shown below). Here are the specifics steps to use this proxy server with Langroid:</p> <p>First in a separate terminal window, spin up the proxy server <code>litellm</code>. For example to use the <code>anthropic.claude-instant-v1</code> model, you can do: <pre><code>export ANTHROPIC_API_KEY=my-api-key\nlitellm --model claude-instant-1\n</code></pre> Or if you want to use the proxy server for a local model running with <code>ollama</code>, you can first run <code>ollama pull mistral</code> for example and then  run <code>litellm --model ollama/mistral</code> to spin up the proxy server for this model. <pre><code>This will show a message indicating the URL where the server is listening, e.g.,\n```bash\nUvicorn running on http://0.0.0.0:8000\n</code></pre></p> <p>This URL is equivalent to <code>http://localhost:8000</code>, which is the URL you will use in your Langroid code below. To use this model in your Langroid code, first create config object for this model and instantiate it.</p> <pre><code>from langroid.language_models.openai_gpt import OpenAIGPTConfig, OpenAIGPT\n\n# create the (Pydantic-derived) config class: Allows setting params via MYLLM_XXX env vars\nMyLLMConfig = OpenAIGPTConfig.create(prefix=\"myllm\") #(1)!\n\n# instantiate the class, with the model name and context length\nmy_llm_config = MyLLMConfig(\n    chat_model=\"local/localhost:8000\",\n    chat_context_length=2048,  # adjust based on model\n)\n</code></pre> <ol> <li>The prefix ensures you can specify the settings in the instantiated object    using environment variables (or in the <code>.env</code> file), using the <code>MYLLM_</code> prefix.    This helps when you want to have different agents use    different models, each with their own environment settings. If you create    subclasses of <code>OpenAIGPTConfig</code> with different prefixes, you can set the    environment variables for each of these models separately, and have all of these    in the <code>.env</code> file without any conflicts.</li> </ol>"},{"location":"tutorials/non-openai-llms/#other-local-llm-servers","title":"Other local LLM servers","text":"<p>There are other ways to spin up a local server running an LLM behind an OpenAI-compatible API,</p> <ul> <li><code>oobabooga/text-generation-webui</code>, <code>ollama</code>, and <code>llama-cpp-python</code>.</li> <li><code>ollama</code></li> <li><code>llama-cpp-python</code></li> </ul> <p>For all of these, the process is the same as in the above example, i.e., you will set the <code>chat_model</code> to a string that looks like <code>local/localhost:&lt;port&gt;</code> or  <code>local/localhost:&lt;port&gt;/v1</code> (depending on the model). </p>"},{"location":"tutorials/non-openai-llms/#using-the-litellm-library","title":"Using the LiteLLM library","text":"<p>LiteLLM also has a python library that  provides functions that mimic the OpenAI API for a variety of LLMs. This means that instead of using <code>openai.ChatCompletion.create</code>, you can use liteLLM's corresponding <code>completion</code> function, and the rest of your code can remain the same (of course this is handled behind the scenes in Langroid, as you see below). Also, there is no need to spin up a local server, which is useful in some scenarios, especially when you want to have multiple agents using different LLMs. Using the LiteLLM library with Langroid is very simple:  simply set the <code>chat_agent</code> in the <code>OpenAIGPTConfig</code> to a string like  <code>litellm/bedrock/anthropic.claude-instant-v1</code>:</p> <pre><code>from langroid.language_models.openai_gpt import OpenAIGPTConfig\nLiteLLMBedrockConfig = OpenAIGPTConfig.create(prefix=\"BEDROCK\") \nlitellm_bedrock_config = LiteLLMBedrockConfig(\n    chat_model=\"litellm/bedrock/anthropic.claude-instant-v1\", #(1)!\n    chat_context_length=4096, # adjust according to model\n)\n</code></pre> <ol> <li>This three-part model name denotes that we are using the <code>litellm</code> adapter library,      the LLM provider is <code>bedrock</code> and the actual model is <code>anthropic.claude-instant-v1</code>.</li> </ol> <p>The general rule for the <code>chat_model</code> parameter is to prepend <code>litellm/</code> to the model name specified in the LiteLLM docs.  For non-local models you will also need to specify one or more API Keys and related values.  There is an internal validation function that will check if the keys for the model have been specified in the environment variables. If not, it will raise an exception telling  you which keys to specify. </p> <p>If you are using environment variables or a <code>.env</code> file, you can specify these  variables using the upper-case version of the <code>prefix</code> argument to the <code>OpenAIGPTConfig.create</code>, e.g. in the above case, you would set the following environment variables like <code>BEDROCK_API_KEY=&lt;your-api-key&gt;</code>.</p> <p>The <code>LiteLLM</code> library can also be used when you have a locally-served model, but you are not using the <code>LiteLLM</code> proxy server. In this case you would set the  <code>chat_model</code> parameter in the <code>OpenAIGPTConfig</code> to a string like <code>litellm/ollama/mistral</code>, again following the pattern of prepending <code>litellm/</code> to the model name specified in the LiteLLM docs.</p>"},{"location":"tutorials/non-openai-llms/#working-with-the-created-openaigptconfig-object","title":"Working with the created <code>OpenAIGPTConfig</code> object","text":"<p>Once you create an <code>OpenAIGPTConfig</code> object using any of the above methods,  you can use it to create an object of class <code>OpenAIGPT</code> (which represents any LLM with an OpenAI-compatible API) and interact with it directly: <pre><code>from langroid.language_models.base import LLMMessage, Role\n\nllm = OpenAIGPT(my_llm_config)\nmessages = [\n    LLMMessage(content=\"You are a helpful assistant\",  role=Role.SYSTEM),\n    LLMMessage(content=\"What is the capital of Ontario?\",  role=Role.USER),\n],\nresponse = mdl.chat(messages, max_tokens=50)\n</code></pre></p> <p>When you interact directly with the LLM, you are responsible for keeping dialog history. Also you would often want an LLM to have access to tools/functions and external data/documents (e.g. vector DB or traditional DB). An Agent class simplifies managing all of these. For example, you can create an Agent powered by the above LLM, wrap it in a Task and have it run as an interactive chat app:</p> <pre><code>from langroid.agent.base import ChatAgent, ChatAgentConfig\nfrom langroid.agent.task import Task\n\nagent_config = ChatAgentConfig(llm=my_llm_config, name=\"my-llm-agent\")\nagent = ChatAgent(agent_config)\n\ntask = Task(agent, name=\"my-llm-task\")\ntask.run()\n</code></pre>"},{"location":"tutorials/non-openai-llms/#working-example-simple-chat-script-with-a-localremote-model","title":"Working example: Simple Chat script with a local/remote model","text":"<p>For a working example, see the basic chat script in the <code>langroid-examples</code> repo,  which you can run a few different ways, to interact with a non-OpenAI model in an interactive chat loop. (If you omit the <code>-m</code> option, it will use the default OpenAI GPT-4 model.) </p> <ol> <li> <p>Using the <code>liteLLM</code> proxy server, with <code>ollama</code>: First run <code>ollama</code> to download and serve a local model, say <code>mistral</code>:  <pre><code>ollama run mistral # download and run the mistral model\n</code></pre> Then in a separate terminal window, run the liteLLM proxy server: <pre><code>litellm --model ollama/mistral # run the proxy server, listening at localhost:8000\n</code></pre> In a third terminal window, run the chat script: <pre><code>python3 examples/basic/chat.py -m local/localhost:8000\n</code></pre></p> </li> <li> <p>Using the <code>liteLLM</code> library, with a remote model: <pre><code>python3 examples/basic/chat.py -m litellm/bedrock/anthropic.claude-instant-v1\n</code></pre></p> </li> </ol>"},{"location":"tutorials/non-openai-llms/#quick-testing-with-non-openai-models","title":"Quick testing with non-OpenAI models","text":"<p>There are numerous tests in the main Langroid repo that involve LLMs, and once you setup the dev environment as described in the README of the repo,  you can run any of those tests (which run against the default GPT4 model) against local/remote models that are proxied by <code>liteLLM</code> (or served locally via the options mentioned above, such as <code>oobabooga</code>, <code>ollama</code> or <code>llama-cpp-python</code>), using the <code>--m &lt;model-name&gt;</code> option, where <code>model-name</code> takes one of the forms above. Some examples of tests are:</p> <p><pre><code>pytest -s tests/test_llm.py --m local/localhost:8000\npytest -s tests/test_llm.py --m litellm/bedrock/anthropic.claude-instant-v1\npytest -s tests/test_llm.py --m litellm/ollama/mistral\n</code></pre> When the <code>--m</code> option is omitted, the default OpenAI GPT4 model is used.</p> <p><code>chat_context_length</code> is not affected by <code>--m</code></p> <p>Be aware that the <code>--m</code> only switches the model, but does not affect the <code>chat_context_length</code>    parameter in the <code>OpenAIGPTConfig</code> object. which you may need to adjust for different models.   So this option is only meant for quickly testing against different models, and not meant as   a way to switch between models in a production environment.</p>"},{"location":"tutorials/postgresql-agent/","title":"Chat with a PostgreSQL DB using SQLChatAgent","text":"<p>The <code>SQLChatAgent</code> is designed to facilitate interactions with an SQL database using natural language. A ready-to-use script based on the <code>SQLChatAgent</code> is available in the <code>langroid-examples</code>  repo at <code>examples/data-qa/sql-chat/sql_chat.py</code> (and also in a similar location in the main <code>langroid</code> repo). This tutorial walks you through how you might use the <code>SQLChatAgent</code> if you were to write your own script from scratch. We also show some of the internal workings of this Agent.</p> <p>The agent uses the schema context to generate SQL queries based on a user's input. Here is a tutorial on how to set up an agent with your PostgreSQL database. The steps for other databases are similar. Since the agent implementation relies  on SqlAlchemy, it should work with any SQL DB that supports SqlAlchemy. It offers enhanced functionality for MySQL and PostgreSQL by  automatically extracting schemas from the database. </p>"},{"location":"tutorials/postgresql-agent/#before-you-begin","title":"Before you begin","text":"<p>Data Privacy Considerations</p> <p>Since the SQLChatAgent uses the OpenAI GPT-4 as the underlying language model, users should be aware that database information processed by the agent may be sent to OpenAI's API and should therefore be comfortable with this.</p> <ol> <li> <p>Install PostgreSQL dev libraries for your platform, e.g.</p> <ul> <li><code>sudo apt-get install libpq-dev</code> on Ubuntu,</li> <li><code>brew install postgresql</code> on Mac, etc.</li> </ul> </li> <li> <p>Follow the general setup guide to get started with Langroid (mainly, install <code>langroid</code> into your virtual env, and set up suitable values in  the <code>.env</code> file). Note that to use the SQLChatAgent with a PostgreSQL database, you need to install the <code>langroid[postgres]</code> extra, e.g.:</p> <ul> <li><code>pip install langroid[postgres]</code> or </li> <li><code>poetry add langroid[postgres]</code> or </li> <li><code>poetry install -E postgres</code>.</li> </ul> <p>If this gives you an error, try <code>pip install psycopg2-binary</code> in your virtualenv.</p> </li> </ol>"},{"location":"tutorials/postgresql-agent/#initialize-the-agent","title":"Initialize the agent","text":"<pre><code>from langroid.agent.special.sql.sql_chat_agent import (\n    SQLChatAgent,\n    SQLChatAgentConfig,\n)\n\nagent = SQLChatAgent(\n    config=SQLChatAgentConfig(\n        database_uri=\"postgresql://example.db\",\n    )\n)\n</code></pre>"},{"location":"tutorials/postgresql-agent/#configuration","title":"Configuration","text":"<p>The following components of <code>SQLChatAgentConfig</code> are optional but strongly recommended for improved results:</p> <ul> <li><code>context_descriptions</code>: A nested dictionary that specifies the schema context for   the agent to use when generating queries, for example:</li> </ul> <pre><code>{\n  \"table1\": {\n    \"description\": \"description of table1\",\n    \"columns\": {\n      \"column1\": \"description of column1 in table1\",\n      \"column2\": \"description of column2 in table1\"\n    }\n  },\n  \"employees\": {\n    \"description\": \"The 'employees' table contains information about the employees. It relates to the 'departments' and 'sales' tables via foreign keys.\",\n    \"columns\": {\n      \"id\": \"A unique identifier for an employee. This ID is used as a foreign key in the 'sales' table.\",\n      \"name\": \"The name of the employee.\",\n      \"department_id\": \"The ID of the department the employee belongs to. This is a foreign key referencing the 'id' in the 'departments' table.\"\n    }\n  }\n}\n</code></pre> <p>By default, if no context description json file is provided in the config, the  agent will automatically generate the file using the built-in Postgres table/column comments.</p> <ul> <li> <p><code>schema_tools</code>: When set to <code>True</code>, activates a retrieval mode where the agent   systematically requests only the parts of the schemas relevant to the current query.    When this option is enabled, the agent performs the following steps:</p> <ol> <li>Asks for table names.</li> <li>Asks for table descriptions and column names from possibly relevant table    names.</li> <li>Asks for column descriptions from possibly relevant columns.</li> <li>Writes the SQL query.</li> </ol> </li> </ul> <p>Setting <code>schema_tools=True</code> is especially useful for large schemas where it is costly or impossible    to include the entire schema in a query context.    By selectively using only the relevant parts of the context descriptions, this mode   reduces token usage, though it may result in 1-3 additional OpenAI API calls before   the final SQL query is generated.</p>"},{"location":"tutorials/postgresql-agent/#putting-it-all-together","title":"Putting it all together","text":"<p>In the code below, we will allow the agent to generate the context descriptions from table comments by excluding the <code>context_descriptions</code> config option. We set <code>schema_tools</code> to <code>True</code> to enable the retrieval mode.</p> <pre><code>from langroid.agent.special.sql.sql_chat_agent import (\n    SQLChatAgent,\n    SQLChatAgentConfig,\n)\n\n# Initialize SQLChatAgent with a PostgreSQL database URI and enable schema_tools\nagent = SQLChatAgent(gi\nconfig = SQLChatAgentConfig(\n    database_uri=\"postgresql://example.db\",\n    schema_tools=True,\n)\n)\n\n# Run the task to interact with the SQLChatAgent\ntask = Task(agent)\ntask.run()\n</code></pre> <p>By following these steps, you should now be able to set up an <code>SQLChatAgent</code> that interacts with a PostgreSQL database, making querying a seamless experience.</p> <p>In the <code>langroid</code> repo we have provided a ready-to-use script <code>sql_chat.py</code> based on the above, that you can use right away to interact with your PostgreSQL database:</p> <pre><code>python3 examples/data-qa/sql-chat/sql_chat.py\n</code></pre> <p>This script will prompt you for the database URI, and then start the agent.</p>"}]}